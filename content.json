[{"title":"利用python爬取网易云音乐评论，并把数据存入mysql","date":"2018-08-07T00:43:42.113Z","path":"2018/08/07/python3.x爬取网易云音乐评论/","text":"在简单学习了python爬虫后，又想继续折腾，进而找到了这个网易云音乐，因为本人平时就是用它听的歌，也喜欢看歌里的评论，所以就爬网易云音乐评论吧，那么开始吧！正式进入主题首先还是去找目标网页并开始分析网页结构，如下上面的三个箭头都是所要找的数据，分别是评论用户，评论和点赞数，都可以用正则表达式找出来，接下来继续找怎样找到下一页的数据，还是用开发者工具，但是当点击下一页的时候，网页的url没有变，说明网页是动态加载，所以就不能在当前网页找数据了，应该在他的xhr文件里找，所以点入network看看，然后也点击下一页一看，果然有想要的看到这里，就兴奋地去敲代码了一点击运行，结果什么东西都没有，但是他的状态码是200，明显请求成功啊，却没有东西返回，再去network仔细看看这个网页，看到他是个post请求，也看到了需要post两个参数params和ensSecKey一看到这个，密密麻麻的数字和字母，就猜应该是被加密了，不过可以复制下来看看有没有用。接下来看下他的Response，咦，这是个json，不是html结构的，所以需要用到Json库来进行解析现在开始敲代码吧，先把上面的两个参数复制过来看看。现在把每条评论的评论用户和点赞数和评论获取出来可以看到，利用json.loads()方法把数据转成python格式里的字典后就可以把想要的数据取出来了，但是，下一页怎样取？总不能每次都复制粘贴那两个参数吧？那唯一的方法就是不爬了。。怎么可能？我的继续，那我就要进行破解这两个参数了，那好继续看network，因为要加密，肯定要用js进行加密的看到刚才那个网站的发起者core.js,,然后把它文件下载下来慢慢研究保存后在经过美化，然后进行查找那个encSecKey参数（ps:JSj’e’tong’yang’de美化网址为http://www.css88.com/tool/js_beautify/），然后找到这个看到window.asrsea()方法有四个参数，先不去管这个函数，先看看他的四个参数是什什，这里没必要去研究那四个参数怎样来的，只需要知道他是什么，那么我们可以加点代码上去让他显示出来，从而利用fiddler来进行调试加入代码如下可以分别获取上面的每一个参数，也把那个params获取看看，然后在fiddler上操作如下完成上面的设置后刷新网页就可以在console上面找到参数信息，如果没有的话这是因为你之前浏览该网页的时候它被缓存了下来，所以要清除缓存文件（在清除浏览器记录里面有）那个rid有本歌曲的id，明显是与评论有关的，我试着连翻几页后，发现那个offset就是评论偏移数，offset就是(页数-1)20，total在第一页是true，在其他页是false同样的方法也得到第二个参数为：010001第三个参数为：00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7第四个参数为：0CoJUm6Qyw8W8jud接下来就要看window.asrsea()方法是什么操作的了，还是通过查找js文件可以看到这个通过研究i是随机获取十六个字符而b函数是AES加密，其中偏移量为0102030405060708，模式为CBC，看回d函数，其中params连续两次加密，第一次加密时，文本为第一个参数。密钥为第四个参数，第二次加密时文本为第一次加密的值，密钥为随机数a。而encSeckey是一个RSA加密，他的公钥是第二个参数，模式是第三个参数，文本为那个随机字符串a终于分析完了，接着开始敲代码先来个获取第一页评论的代码这是获取两个参数的类这是解析网易云音乐和获取评论的类然而一点击运行，直接给我报了个错：TypeError: can’t concat str to bytes原来是因为在第二次加密的时候，那个params是个byte类型，所以把他转成字符串类型就可以了再次点击运行，结果还是报错了：json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)这个报错因为我的json解析错了，回头调试一看，网页返回的东西是空的，但他的状态码是200，这是什么鬼？接着我再试了把那两个参数的值直接复制和前面一样看看，结果运行成功，这就说明我的是加密过程错了，然后我就回去看了几篇，看不出什么错误，上网百度找到了这个知乎文章https://zhuanlan.zhihu.com/p/32069543，我把她的代码复制过来运行下，结果是可以的，我就继续看看我和她的区别，原来我在用那个16个随机字符的时候用错了，我在两个参数给了两个不同的，而是需要给共同一个的，看到这里，我就直接回去改了下，果然运行成功，代码我就不贴出来了，效果如下接着是获取每一页的评论，而每一页与第一个参数的offset有关，其中的公式为offse=(页数-1)20，total在第一页是true，在其他页是false而写入数据库我用的是我这篇文章的操作&nbsp;http://mp.weixin.qq.com/s/6sQ_ER39P2NtXaPOnGdQNA，由于篇幅过长，就不贴出来了，感兴趣的可以去看看接下来点运行就可以了，但是运行到第八页的时候出现了这个异常raise errorclass(errno, errval)pymysql.err.InternalError: (1366, “Incorrect string value: ‘\\xF0\\x9F\\x92\\x94’ for column ‘content’ at row 1”)原因是这条评论有个识别不了的表情，之后百度参考这篇文章http://blog.csdn.net/HHTNAN/article/details/76769264?locationNum=9&amp;fps=1修改了数据库的编码方式，注意还要自己修改下创建数据库时的编码方式才可！这是首页数据库效果获取完成（家驹的歌评论这么少吗？不解）终于完成了，虽然辛苦，但是值得，在这个过程中也学会了很多东西，在写这篇文章时参考了两篇文章，一个是知乎首个回答https://www.zhihu.com/question/36081767/answer/140287795，另一个就是解密过程https://github.com/cosven/cosven.github.io/issues/30大家有什么问题的话欢迎去我的公众号日常学python的后台那里问我，我知道的我都一一为你解答，最后，若你也在这篇文章学到了，可以帮我点个赞，转发下吗？谢谢支持哈！ps:若需要完整代码可以在我的公众号日常学python后台回复评论即可获取，还有其他福利以后会一一分析我的公众号的二维码","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"hexo搭建的博客提交文章本地预览成功但github没有显示解决方法","date":"2018-08-07T00:43:42.059Z","path":"2018/08/07/hexo搭建的博客提交文章本地预览成功但github没有显示解决方法/","text":"记录自己博客的一个坑！前几天我写了文章，想发布在博客上，谁知道在提交上去，只可以在本地预览，却不可以在github上看到，而且github上也没有提交文章的记录。这个坑我百度了很久也没有找到，直到问到人，说需要升级git，然后升级了。果然，成功提交了！！！升级git是直接在重新下载个git把原来的覆盖掉就可以了。升级完成之后记得再提交一次，比如：hexo cleanhexo ghexo d这样就可以了，如果还是不可以，可以选择清除下cookie及历史记录。我完成了上面的之后，文章算是提交上去了，但是，我的主题没有了，很悲催，没想到一坑还比一坑高，所以还是接着百度，看到了可能是我的github的仓库名字和我的hexo博客里的配置_config.yml​文件中的url名字不一样导致的试了一下，果真是可以了。这个坑是前面那个坑乱改东西留下的，不过解决了，就好！！生活不止眼前的bug，还有乐趣和斗志！","tags":[{"name":"坑","slug":"坑","permalink":"http://sergiojune.com/tags/坑/"}]},{"title":"学习用python操作mysql","date":"2018-08-07T00:43:42.012Z","path":"2018/08/07/学习用python操作mysql/","text":"首先祝大家新年快乐哈！学生的估计明天也要上课了，工作的估计早就去上班了，我也快要上课了，哈哈，新年这段时间一直没有写过文章，一直忙于跑亲戚和学习，感觉有点对不起关注我的粉丝。所以，今天决定抽空写一篇技术文章来给大家看看，继上篇写了入门mysql之后，还没有学习如何用python来操作数据库，那我今天就带大家来学习如何用python操操作数据库。还有文末有福利，这算是给大家的新年礼物(记得点赞哦)进入正题工欲善其事，必先利其器。所以第一步，我们先下载第三方库。在这里，我用到的是pymysql库。下载库：在命令行输入1pip install pymysql下载后可检验一下是否成功下载。直接在命令行进入python然后导库即可1C:\\Users\\June&gt;python2Python 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win323Type “help”, “copyright”, “credits” or “license” for more information.4&gt;&gt;&gt; import pymysql5&gt;&gt;&gt;看到这个画面就说明下载成功了，接下来学习如何操作数据库了！！！1连接数据库1import pymysql2# 连接数据库3db = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘your password’,db=‘news’,port=3306,charset=‘utf8’)以上的参数是必填的，host: 这个是ip地址，因为我这里是本地的，所以填127.0.0.1，也可以填localhost。user：用户名，如果你也是本地的，就填root好了passwd：这个是密码，填上你自己设的密码就可以了db：这个是数据库名，我这里选的是news数据库port：这个是端口，本地的一般都是3306charset：这个是编码方式，要和你数据库的编码方式一致，要不会连接失败连接上了，怎么验证呢？这里我们可以选择查一条数据 1try: 2 db = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘your password’,db=‘news’,port=3306,charset=‘utf8’) 3 # 检验数据库是否连接成功 4 cursor = db.cursor() 5 # 这个是执行sql语句，返回的是影响的条数 6 data = cursor.execute(‘SELECT FROM new‘) 7 # 得到一条数据 8 one = cursor.fetchone() 9 print(data)10 print(one)11except pymysql.Error as e:12 print(e)13 print(‘操作数据库失败’)14finally:15 # 如果连接成功就要关闭数据库16 if db:17 db.close()代码解读：因为在连接数据库中，有时会发生连接失败等异常，所以这里就进行捕捉异常，这里的异常都是在 pymsql.Error 里面。上面的代码看不懂也没关系，因为我接下来会说，如果运行后有结果证明连接成功。在用完后，一定要记得关闭数据库连接，防止资源泄露问题。2对数据进行查询 1import pymysql 2try: 3 conn = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘password’,db=‘news’,charset=‘utf8’,port=3306) 4 # 这个是光标，用来操作数据库语句 5 cursor = conn.cursor() 6 # 执行sql语句 7 cursor.execute(‘SELECT FROM new‘) 8 print(cursor.fetchone()) 9 # 关闭光标10 cursor.close()11except pymysql.Error as e:12 print(e)13 print(‘操作数据库失败’)14finally:15 if conn:16 conn.close()代码解读：cursor():这个是光标，用来执行mysql语句的，用完后也是需要关闭的excute()：这个是执行语句，执行参数的mysql语句fetchone()：这个是查看执行语句后的一条数据fetchall()：这个是查看所有数据在查询数据后，返回的是一整条数据，有没有可以按字典形式来查询的呢？来试试！1print(cursor.fetchone()[‘name’])23Traceback (most recent call last):4 File “E:/anaconda/python_project/mysql_test/test2.py”, line 8, in &lt;module&gt;5 print(cursor.fetchone()[‘name’])6TypeError: tuple indices must be integers or slices, not str查了之后，编译器想都不想就给了我这个错误，说这是个元组，不能这样操作。虽然python没有提供，但是我们可以手动转成字典来查询啊cursor这里有个属性：description。获取的是数据库每个栏位情况，如下：1print(cursor.description)2# 下面是结果3((‘id’, 3, None, 11, 11, 0, False), (‘type’, 253, None, 5, 5, 0, False), (‘title’, 253, None, 50, 50, 0, False), (‘content’, 253, None, 2000, 2000, 0, False), (‘view_count’, 3, None, 11, 11, 0, False), (‘release_time’, 12, None, 19, 19, 0, False), (‘author’, 253, None, 20, 20, 0, True), (‘from’, 253, None, 20, 20, 0, True), (‘is_valibale’, 3, None, 11, 11, 0, False)所以，我们利用这个属性手动生成字典1# 将一条数据转成字典方便查找2new = dict(zip([x[0] for x in cursor.description],[x for x in cursor.fetchone()]))3print(new)4# 下面是结果5{‘id’: 2, ‘type’: ‘NBA’, ‘title’: ‘考辛斯跟腱撕裂赛季报销 浓眉詹皇发声祝福’, ‘content’: ‘他遭遇左脚跟腱撕裂，将缺席赛季剩下的比赛。这无疑对考辛斯和鹈鹕队都是一个重大的打击’, ‘view_count’: 3560, ‘release_time’: datetime.datetime(2018, 1, 27, 12, 10), ‘author’: ‘xiaoylin’, ‘from’: ‘腾讯体育’, ‘is_valibale’: 1}这里利用zip函数和列表生成式来一行代码就生成成功了用字典来查询，现在就可以了1print(new[‘title’])2# 下面是结果3考辛斯跟腱撕裂赛季报销 浓眉詹皇发声祝福但是，上面的只是一条数据的，如果是多条的呢？再按上面的方法就行不通了。这时就需要用到map函数了1def new2dict(new):2 return dict(zip([x[0] for x in cursor.description],[x for x in new]))3news_list = list(map(new2dict,cursor.fetchall()))4print(news_list)5# 下面是结果6[{‘id’: 2, ‘type’: ‘NBA’, ‘title’: ‘考辛斯跟腱撕裂赛季报销 浓眉詹皇发声祝福’, ‘content’: ‘他遭遇左脚跟腱撕裂，将缺席赛季剩下的比赛。这无疑对考辛斯和鹈鹕队都是一个重大的打击’, ‘view_count’: 3560, ‘release_time’: datetime.datetime(2018, 1, 27, 12, 10), ‘author’: ‘xiaoylin’, ‘from’: ‘腾讯体育’, ‘is_valibale’: 1}, {‘id’: 3, ‘type’: ‘NBA’, ‘title’: ‘火箭挖21分大哈登得背锅 连遭浓眉大帽太尴尬’, ‘content’: ‘火箭在客场以113-115惜败于鹈鹕，4连胜终结。詹姆斯-哈登出战34分钟16投5中，其中三分球9投只有1中，罚球14罚12中，拿到23分、11助攻、5篮板但也有4次失误，其在场正负值为尴尬的-12分’, ‘view_count’: 7520, ‘release_time’: datetime.datetime(2018, 1, 27, 12, 5), ‘author’: ‘youngcao’, ‘from’: ‘腾讯体育’,‘is_valibale’: 1}, {‘id’: 4, ‘type’: ‘英超’, ‘title’: ‘足总杯-曼联4-0英乙球队晋级 桑神首秀造两球’, ‘content’: ‘2017-18赛季英格兰足总杯第4轮，曼联客场4比0击败英乙球队约维尔，顺利晋级下一轮。桑切斯迎来曼联首秀，并制造了两个入球’, ‘view_count’: 6560, ‘release_time’: datetime.datetime(2018, 1, 27, 5, 49), ‘author’: ‘ricazhang’, ‘from’: ‘腾讯体育’,‘is_valibale’: 1}, {‘id’: 5, ‘type’: ‘英超’, ‘title’: ‘这才配红魔7号！桑神首秀大腿级表演 回击嘘声质疑’, ‘content’: ‘在今天凌晨对阵约维尔的首秀也值得期待。虽然在登场的72分钟时间里没有进球，但送出1次助攻且有有6次威胁传球的数据还是十分亮眼’, ‘view_count’: 2760, ‘release_time’: datetime.datetime(2018, 1, 27, 6, 13), ‘author’: ‘yaxinhao’, ‘from’: ‘腾讯体育’, ‘is_valibale’: 1}]这里很巧妙的利用了map函数，因为多条数据就可以进行迭代了，需要操作每条数据，这样就可以想到map函数接下来我们再用面向对象的方法来用python进行查询数据库 1import pymysql 2class MysqlSearch(object): 3 def get_conn(self): 4 ‘’’连接mysql数据库’’’ 5 try: 6 self.conn = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘your password’,port=3306,charset=‘utf8’,db=‘news’) 7 except pymysql.Error as e: 8 print(e) 9 print(‘连接数据库失败’)1011 def close_conn(self):12 ‘’’关闭数据库’’’13 try:14 if self.conn:15 self.conn.close()16 except pymysql.Error as e:17 print(e)18 print(‘关闭数据库失败’)1920 def get_one(self):21 ‘’’查询一条数据’’’22 try:23 # 这个是连接数据库24 self.get_conn()25 # 查询语句26 sql = ‘SELECT FROM new WHERE type=%s’27 # 这个光标用来执行sql语句28 cursor = self.conn.cursor()29 cursor.execute(sql,(‘英超’,))30 new = cursor.fetchone()31 # 返回一个字典，让用户可以按数据类型来获取数据32 new_dict = dict(zip([x[0] for x in cursor.description],new))33 # 关闭cursor34 cursor.close()35 self.close_conn()36 return new_dict37 except AttributeError as e:38 print(e)39 return None40 def get_all(self):41 ‘’’获取所有结果’’’42 sql = ‘SELECT FROM new ‘43 self.get_conn()44 try:45 cursor = self.conn.cursor()46 cursor.execute(sql)47 news = cursor.fetchall()48 # 将数据转为字典，让用户根据键来查数据49 news_list =list(map(lambda x:dict(zip([x[0] for x in cursor.description],[d for d in x])),news))50 # 这样也行,连续用两个列表生成式51 news_list = [dict(zip([x[0] for x in cursor.description],row)) for row in news]52 cursor.close()53 self.close_conn()54 return news_list55 except AttributeError as e:56 print(e)57 return None5859def main():60 # 获取一条数据61 news = MysqlSearch()62 new = news.get_one()63 if new:64 print(new)65 else:66 print(‘操作失败’)6768 # 获取多条数据69 news = MysqlSearch()70 rest = news.get_all()71 if rest:72 print(rest)73 print(rest[7][‘type’],rest[7][‘title’])74 print(‘类型：{0},标题：{1}’.format(rest[12][‘type’],rest[12][‘title’]))75 for row in rest:76 print(row)77 else:78 print(‘没有获取到数据’)7980if name == ‘main‘:81 main()这样就可以通过实例的方法来进行查询数据库了我们还可以根据页数来进行查询指定的数据数 1 def get_more(self,page,page_size): 2 ‘’’查多少页的多少条数据’’’ 3 offset = (page-1)page_size 4 sql = ‘SELECT FROM new LIMIT %s,%s’ 5 try: 6 self.get_conn() 7 cursor = self.conn.cursor() 8 cursor.execute(sql,(offset,page_size,)) 9 news = [dict(zip([x[0] for x in cursor.description],new)) for new in cursor.fetchall()]10 cursor.close()11 self.close_conn()12 return news13 except AttributeError as e:14 print(e)15 return None1617def main():18 #获取某页的数据19 news = MysqlSearch()20 new = news.get_more(3,5)21 if new:22 for row in new:23 print(row)24 else:25 print(‘获取数据失败’)2627if name == ‘main‘:28 main()利用的是mysql的limit关键字，还有其他的，比如进行排序分组的感兴趣的可以自己尝试下3增加数据到数据库 1 def add_one(self): 2 sql = ‘INSERT INTO new(title,content,type,view_count,release_time) VALUE(%s,%s,%s,%s,%s)’ 3 try: 4 self.get_conn() 5 cursor = self.conn.cursor() 6 cursor.execute(sql, (‘title’, ‘content’, ‘type’, ‘1111’, ‘2018-02-01’)) 7 cursor.execute(sql, (‘标题’, ‘内容’, ‘类型’, ‘0000’, ‘2018-02-01’)) 8 # 一定需要提交事务，要不不会显示，只会占位在数据库 9 self.conn.commit()10 return 111 except AttributeError as e:12 print(‘Error:’, e)13 return 014 except TypeError as e:15 print(‘Error:’, e)16 # 发生错误还提交就是把执行正确的语句提交上去17 # self.conn.commit()18 # 下面这个方法是发生异常就全部不能提交,但语句执行成功的就会占位19 self.conn.rollback()20 return 021 finally:22 cursor.close()23 self.close_conn()2425 def main():26 news = OperateSQL()27 if news.add_one():28 print(‘增加数据成功’)29 else:30 print(‘发生异常，请检查！！！’)3132 if name == ‘main‘:33 main()因为是增加数据，所以需要提交事务，这就需要用到cursor.commit()来进行提交，在增加数据后，如果不提交，数据库就不会显示。还有修改数据和删除数据就不贴出来了，只是把上面的sql变量的语句改成修改或者删除的语句就可以了，如果你还不会，建议练习下END代码我放在github了，网站为https://github.com/SergioJune/gongzhonghao_code，有兴趣的可以去看看，如果可以的话希望给个star哈！这篇文章只适合入门的，如果需要学习更多的话可以去查看pymysql的文档http://pymysql.readthedocs.io/en/latest/ 。日常学python一个专注于python的公众号","tags":[{"name":"mysql","slug":"mysql","permalink":"http://sergiojune.com/tags/mysql/"}]},{"title":"我爬取了37000条球迷评论，知道了这场比赛的重要信息","date":"2018-08-07T00:43:41.976Z","path":"2018/08/07/我爬取了37000条球迷评论，知道了这场比赛的重要信息/","text":"这是日常学python的第18篇原创文章这次用python爬虫爬点好玩的东西这两天看恰好有nba决赛，是球迷的你肯定不会错过的，更何况今年的西部决赛是火箭对战勇士，今年的火箭是很强的，因为没到关键时候总会有人站出来。当然，勇士也是挺强的，毕竟不能小看库里杜兰特等四大巨头。东部的决赛我就不太知道了，一直以为是凯尔特人会苦战骑士，谁知道缺了两大主力的凯尔特人还是很强，而且还打了骑士2：0，看来这次的骑士会是凶多吉少了，不知道凯尔特人会不会成功复仇，让我们拭目以待吧！有直播就肯定有评论，所以我想爬取下球迷评论，看看他们都在聊什么！准备工作需要用到的库：&nbsp; &nbsp; requests：用于网络请求&nbsp;&nbsp;&nbsp;&nbsp;jieba：用于分词&nbsp;&nbsp;&nbsp;&nbsp;wordcloud：制作词云图&nbsp;&nbsp;&nbsp;&nbsp;numpy：制作背景图片词云背景图片：上面的库都是可以直接用pip进行下载的，但是wordcloud会报错，报错如下：我们需要去官网下载whl文件进行手动安装官网：https://www.lfd.uci.edu/~gohlke/pythonlibs/然后找到对应自己安装的python版本进行下载最后在命令行下安装即可pip install “文件路径+whl文件名”接下来寻找目标网页文字直播地址：https://www.zhibo8.cc/zhibo/nba/2018/0517123898.htm?redirect=zhibo在这个网页通过抓包（按下f12）课知道下面这个链接是返回评论信息，而且是个json链接为：https://cache.zhibo8.cc/json/2018/nba/0517123898_384.htm?key=0.6512348313080727通过多次分析知道上面加粗的是直播间的信息，后面的下划线之后的是评论的页数，最后的key参数是个随机数，带不带上进行请求都没有关系用代码来获取评论信息def&nbsp;get_json(self, index):&nbsp; &nbsp; &nbsp; &nbsp;url =&nbsp;‘https://cache.zhibo8.cc/json/2018/nba/0517123898_%d.htm?key=0.1355540028791382&#39;&nbsp;% index&nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url)&nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;response.status_code ==&nbsp;200:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for&nbsp;item&nbsp;in&nbsp;response.json():&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 写入文件&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.write_file(item[‘content’])&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.num +=&nbsp;1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;1&nbsp; &nbsp; &nbsp; &nbsp;else:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;0评论信息有了，接着弄张词云图def&nbsp;get_wordcloud(self):&nbsp; &nbsp; &nbsp; &nbsp;with&nbsp;open(‘comments.txt’,&nbsp;‘r’, encoding=‘utf-8’)&nbsp;as&nbsp;comments:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;text = comments.read() &nbsp;# 加载数据&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;words =&nbsp;‘ ‘.join(jieba.cut(text, cut_all=True)) &nbsp;# 采用结巴全分词模式&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image = np.array(Image.open(‘1.jpg’)) &nbsp;# 背景图片&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 初始化词云&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;wc = WordCloud(font_path=r’C:\\Windows\\Fonts\\simkai.ttf’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; background_color=‘white’, mask=image,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; max_font_size=100, max_words=2000)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;wc.generate(words) &nbsp;# 生成词云&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;wc.to_file(‘img.png’) &nbsp;# 生成图片&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_file = Image.open(‘img.png’) &nbsp;# 打开图片&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_file.show()好了，代码完成，看下效果：利用词云图一眼就可以看出球迷都在评论什么了，因为我爬取的是火箭主场对战勇士的第二场比赛，肯定讨论最多的就是勇士火箭了，紧接的就是杜兰特了，死亡之神，这场的杜兰特超神拿了38分还是输给了火箭，自然而然就是讨论他最多了。还有就是这场站出来的塔克，三分6中5，还破了个人季后赛的最高得分，讨论他也是很正常的事。还有一个很显眼的就是第三节，很多人都认为勇士是“勇三疯”，以为这场比赛勇士会在第三节爆发吧？其实这赛季的火箭第三节也是很强的，并不比勇士弱。完整代码已经上传到我的github上了，如果需要的话可以自行查看，如果觉得程序不错的话希望可以给个star哈！github：https://github.com/SergioJune/gongzhonghao_code写在最后如果这篇文章对你用的话，希望不要吝啬你的点赞哈！点赞和转发就是对我的最大支持，这样才有动力输出质量高的原创文章。「球迷的话点个赞？我看看有多少个是球迷」推荐文章：&lt;a href=”http://mp.weixin.qq.com/s?biz=MzU0NzY0NzQyNw==&amp;mid=2247483774&amp;idx=1&amp;sn=769c06f2cf532b645d9cb9ba63b52b81&amp;chksm=fb4a7bd2cc3df2c47851c244db40b5cddccf2e21381d7742e39b9db2bdee0f12b83f870814bf&amp;scene=21#wechat_redirect” target=”_blank” _href=”http://mp.weixin.qq.com/s?__biz=MzU0NzY0NzQyNw==&amp;mid=2247483774&amp;idx=1&amp;sn=769c06f2cf532b645d9cb9ba63b52b81&amp;chksm=fb4a7bd2cc3df2c47851c244db40b5cddccf2e21381d7742e39b9db2bdee0f12b83f870814bf&amp;scene=21#wechat_redirect&quot; style=”word-break: normal !important;”&gt;使用requests+BeautifulSoup的简单爬虫练习python爬虫常用库之requests详解日常学python代码不止bug，还有美和乐趣​","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"谈谈如何抓取ajax动态网站","date":"2018-08-07T00:43:41.950Z","path":"2018/08/07/谈谈如何抓取ajax动态网站/","text":"什么是ajax呢，简单来说，就是加载一个网页完毕之后，有些信息你你还是看不到，需要你点击某个按钮才能看到数据，或者有些网页是有很多页数据的，而你在点击下一页的时候，网页的url地址没有变化，但是内容变了，这些都可以说是ajax。如果还听不懂，我给你看看百度百科的解释吧，下面就是。Ajax 即“Asynchronous&nbsp;Javascript&nbsp;And&nbsp;XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。Ajax = 异步&nbsp;JavaScript&nbsp;和&nbsp;XML（标准通用标记语言的子集）。Ajax 是一种用于创建快速动态网页的技术。Ajax 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。&nbsp;[通过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用 Ajax）如果需要更新内容，必须重载整个网页页面。下面说下例子，我抓取过的ajax网页最难的就是网易云音乐的评论，感兴趣的可以看看利用python爬取网易云音乐，并把数据存入mysql这里的评论就是ajax加载的，其他的那个抓今日头条妹子图片的也算是ajax加载的，只不过我把它简单化了。还有很多，就不说了，说下我今天要说的ajax网站吧！http://www.kfc.com.cn/kfccda/storelist/index.aspx这个是肯德基的门面信息这里有很多页数据，每一页的数据都是ajax加载的。如果你直接用python请求上面那个url的话，估计什么数据都拿不到，不信的话可以试试哈。这时候，我们照常打开开发者工具。先把所有请求清楚，把持续日志打上勾，然后点击下一页，你会看到上面那个请求就是ajax请求的网页，里面就会有我们需要的数据，我们看看是什么样的请求是个post请求，请求成功状态码为200，请求url上面也有了，下面的from data就是我们需要post的数据，很容易就可以猜到pageIndex就是页数，所以我们可以改变这个值来进行翻页。这个网页就分析完了，这样就是解决ajax动态网页了，是不是觉得很简单，其实不是的，只是这个网页比较简单的，因为表单(from data)的数据并没有进行加密，如果进行加密的话估计你的找js文件看看参数是怎样加密的了，这就是我之前写的网易云音乐评论的爬取。看这些混淆的js寻找加密方法的话有时会让你很头痛，所以经常有人会选择用selenium这些来进行爬取，但是用这些会使爬虫的性能降低，所以这个方法在工作里是不允许的。所以必须学会怎样应对这些ajax。贴下代码import&nbsp;requestspage =&nbsp;1while&nbsp;True:&nbsp; &nbsp;url =&nbsp;‘http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#39;&nbsp; &nbsp;data = {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘cname’:&nbsp;‘广州’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘pid’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘pageIndex’: page,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘pageSize’:&nbsp;‘10’&nbsp; &nbsp;}&nbsp; &nbsp;response = requests.post(url, data=data)&nbsp; &nbsp;print(response.json())&nbsp; &nbsp;if&nbsp;response.json().get(‘Table1’,&nbsp;‘’):&nbsp; &nbsp; &nbsp; &nbsp;page +=&nbsp;1&nbsp; &nbsp;else:&nbsp; &nbsp; &nbsp; &nbsp;break可以看到去掉from data，不用十行代码就可以把数据都爬下来了，所以说这个网站适合练手，大家可以去试试。写在最后下篇文章我会写下复杂点的ajax请求，这个网站http://drugs.dxy.cn/不知道有多少人想看，想看的话点个赞吧！或者你可以自己先试试哈推荐文章如何爬取asp动态网页？搞定可恶的动态参数，这一文告诉你！利用python爬取网易云音乐，并把数据存入mysql日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"说说数据库","date":"2018-08-07T00:43:41.883Z","path":"2018/08/07/说说数据库/","text":"爬了数据，只能放在记事本上？小的数据还是可以的，但是当你遇到较多的数据，放在记事本上就不是很好了，这时就需要用到数据库来存储了，那我们今天的主题就是入门数据库，在入门数据库之前，我首先介绍下什么是数据库。进入正题1什么是数据库在这里我把百度百科的贴出来给大家哈，我也不好解释，毕竟自己页没有到那种境界数据库(Database)是按照 数据结构来组织、 存储和管理数据的仓库，它产生于距今六十多年前，随着 信息技术和市场的发展，特别是二十世纪九十年代以后， 数据管理不再仅仅是存储和管理数据，而转变成用户所需要的各种数据管理的方式。数据库有很多种 类型，从最简单的存储有各种数据的 表格到能够进行海量 数据存储的大型&nbsp;数据库系统都在各个方面得到了广泛的应用。在信息化社会，充分有效地管理和利用各类信息资源，是进行科学研究和决策管理的前提条件。数据库技术是管理信息系统、办公自动化系统、决策支持系统等各类信息系统的核心部分，是进行科学研究和决策管理的重要技术手段。数据有两种，一种是关系型数据库，另一种是非关系型数据。关系型数据就是以行和列的形式存储数据，以便于用户理解，这一系列的行和列称为表，一组表组成了数据库。表与表之间的数据记录有关系。非关系型数据，也叫NoSQL，就是Not only SQL。其性能是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能会非常高，而可扩展性同样也是基于键值对的，数据之间没有耦合性，所以非常容易水平扩展。比如我们即将学到的mongodb和redis都是这类型的数据库。2学习数据库今天我们先学习下关系型数据库的一种MySQL。那么我们就要先下载好mysql这个工具，网站我直接贴出来了https://dev.mysql.com/downloads/，在上面选择自己的版本进行下载安装即可。或者去下载XAMPP集成包，里面也有MySQL程序。百度即有下载。下载了数据库，还要下载个可视化工具，这样可以清储看到数据内容。可视化工具是Navicat，下载地址为https://www.navicat.com/en/products。注册码的百度找就有了，土豪请无视吧，下载安装后打开是这样的 然后点击连接，先连接一个本地数据库吧，填好上面的数据，注意上面只需填连接名和密码即可，本地连接的ip，端口和名字都是固定的，无需自己写了，然后点击测试看看，连接成功就可以按下确定了，这样就成功连接到你的本地数据库了。注意：密码是自己设置的，可以点击mysql的这个来设置，如下图END最后，你可以先自己在可视化工具尝试建立自己的第一个数据库看看，不会也没关系，我在明天继续更新mysql的相关基础语法。​","tags":[{"name":"mysql","slug":"mysql","permalink":"http://sergiojune.com/tags/mysql/"}]},{"title":"使用requests+BeautifulSoup的简单爬虫练习","date":"2018-08-07T00:43:41.851Z","path":"2018/08/07/使用requests+BeautifulSoup的简单爬虫练习/","text":"这是日常学python的第17篇原创文章上篇文章说了BeautifulSoup库之后，今篇文章就是利用上篇的知识来爬取我们今天的主题网站：猫眼电影top100。这个网站也挺容易的，所以大家可以先自己爬取下，遇到问题再来看下这篇文章哈。这篇文章主要是练习而已，别无用处，大佬请绕道哈！1、本文用到的库及网站requestsBeautifulSoup目标网站：http://maoyan.com/board/42、分析目标网站很容易找到我们想要的信息，上面的5的箭头都是我们想要的信息，分别是电影图片地址、电影名字、主演、上演时间和评分。内容有了，接下来就是获取下一页的链接。这里有两种方法，第一种就是在首页获取所有页的链接，第二种方法就是获取每个页面的下一页的链接。在这里由于只是给了部分页面的链接出来，所以我们获取的是下一页的链接，这样子方便点。好了，分析完毕，接下来代码撸起。3.敲代码什么都不管，立即来个get请求import requestsfrom bs4 import BeautifulSoupurl_start = ‘http://maoyan.com/board/4&#39;response = requests.get(url_start)if response.status_code == 200: &nbsp; &nbsp;soup = BeautifulSoup(response.text, ‘lxml’)print(response.text)输出结果：惊不惊喜，意不意外？如果你经常玩爬虫的，这个就见怪不怪了，我们被反爬了。我们试下加个请求头试试。url_start = ‘http://maoyan.com/board/4&#39;headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36’}response = requests.get(url_start, headers=headers)这样就可以正常返回了，因为一般的网站都会在请求头上加个反爬的，所以遇到了反爬也不要着急，加个请求头试试？接下来用BeautifulSoupL来获取内容imgs = soup.select(‘dd .board-img’) &nbsp;# 这是获取图片链接titles = soup.select(‘dd .board-item-main .name’) &nbsp;# 这是获取电影名字starses = soup.select(‘dd .board-item-main .movie-item-info .star’) &nbsp;# 这是获取电影主演times = soup.select(‘dd .board-item-main .movie-item-info .releasetime’) &nbsp;# 这是获取电影上映时间scores = soup.select(‘dd .board-item-main .score-num’) &nbsp;# 这是获取评分这里每个获取的语句都包含了每个不同电影的信息，这样就不能和正则那样一次把每个电影的信息都在同一个字符里面了。就比如我获取的图片，一个语句获取的是这个页面的所有电影图片的链接，我们存储的时候就要分别取出来了。这里我用到的是for循环0到9把相同的坐标的信息存进同一个字典里面。films = [] &nbsp;# 存储一个页面的所有电影信息 &nbsp; &nbsp;for x in range(0, 10): &nbsp; &nbsp; &nbsp; &nbsp;# 这个是获取属性的链接 &nbsp; &nbsp; &nbsp; &nbsp;img = imgs[x][‘data-src’] &nbsp; &nbsp; &nbsp; &nbsp;# 下面的都是获取标签的内容并去掉两端空格 &nbsp; &nbsp; &nbsp; &nbsp;title = titles[x].get_text().strip() &nbsp; &nbsp; &nbsp; &nbsp;stars = starses[x].get_text().strip()[3:] &nbsp;# 使用切片是去掉主演二字 &nbsp; &nbsp; &nbsp; &nbsp;time = times[x].get_text().strip()[5:] &nbsp;# 使用切片是去掉上映时间二字 &nbsp; &nbsp; &nbsp; &nbsp;score = scores[x].get_text().strip() &nbsp; &nbsp; &nbsp; &nbsp;film = {‘title’: title, ‘img’: img, ‘stars’: stars, ‘time’: time, ‘score’: score} &nbsp; &nbsp; &nbsp; &nbsp;films.append(film)接下来就是获取每一页的链接pages = soup.select(‘.list-pager li a’) &nbsp;# 可以看到下一页的链接在最后一个a标签 &nbsp; &nbsp;page = pages[len(pages)-1][‘href’]后面的就简单了，就是利用循环把所有页面的内容都去取出来就可以了，代码就不贴出来了。写在最后这个就是BeautifulSoup库的小练习，用到昨天的内容不多，只是用到了选择器部分和获取文本内容和属性部分，感觉还是正则比较好用点哈，我一个正则就可以获取每个电影的详细内容了，如下:&lt;dd&gt;.?board-index.?&gt;([\\d]{1,3})&lt;/i&gt;.?title=“(.?)”.?class=”star“&gt;(.?)&lt;/p&gt;.?class=”releasetime“&gt;(.?)&lt;/p&gt;.?class=”integer“&gt;(.?)&lt;/i&gt;.?class=”fraction“&gt;(.?)&lt;/i&gt;还需要用到个匹配模式哈：re.S就可以了。所以本人推荐使用正则表达式哈。需要完整代码的请查看我的github哈！github：https://github.com/SergioJune/gongzhonghao_code/blob/master/python3_spider/index.py如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf◐◑&nbsp;python爬虫常用库之BeautifulSoup详解◐◑&nbsp;老司机带你用python来爬取妹子图日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"入门mysql数据库","date":"2018-08-07T00:43:41.806Z","path":"2018/08/07/入门mysql数据库/","text":"上篇文章简单说了下数据库，并教了怎样安装mysql工具和可视化工具，不知道你现在安装好了没？那今天我们继续说mysql，今天说下mysql语句。进入正题mysql语句分为两种，一种是DDL，就是数据定义语句，比如那些CREATE,ALTER。另一种是DML，就是数据管理语句，比如增删改查，我所说的就是DML，那么开始吧！1创建并使用数据库我们先打开navicat可视化管理工具，连接好我们的本地数据库然后点击查询，进行写我们的mysql语句那么正式开始创建数据库并使用这里我创建了个school的数据库，并使用了他。运行语句可以直接点上面的运行，也可以这样注意：那个数据库的引号是个反引号，在tab键的上方，还有mysql的注释是两个划线再加一个空格。语句结束后最好加个 ; 来结束在这里我们也可以看看我们已有的数据库上面成功创建后你将会在左侧看到这个数据库，这样就成功了。2进行建表上面创建了一个student的表格，并且有id，name，sex，age，address和in_school这几个栏位，其中id是自增的，用了AUTO_INCREMENT来声明的。NOT NULL 就是代表该栏位不能为空，NULL就是相反的。而INT,CHAR这些是数据类型，mysql的数据类型有int,char,varchar(括号里的是自定义长度),datetime等。那么，接下来点击运行，结果报了错，说我没有主键，那我们可以在id栏位加上主键声明再次运行，就成功了，你点开数据库，你会看到这个3进行增删改查操作1).往数据库添加数据格式为：INSERT INTO tablename VALUE(v1,v2,v3,…),如下：这是往students表中添加数据，VALUE后的括号填的值需要对应刚才你创建表时候的栏位，要不会报错。但是如果每个都需要这样岂不是很麻烦？不怕。若是只想填几个栏位，也可以这样填这里是在表名加个你需要填的栏位即可，后面的数据也要记得一 一对应，还有的就是，那些是NOT NULL 的必须要填，否则也会报错。添加了数据之后效果如下图还有一个问题就是，可不可以一次性添加多个数据？答案是肯定可以。想一次性添加多个数据时，只需要把VALUE改成VALUES即可，然后每个数据用逗号隔开，如下：2).查询表格中的数据最简单的格式为：SELECT data FROM tablename查询students表格的所有数据* 是代表所有数据，实际开发中并不会这样查询，因为会影响效率。我们可以指定某一栏位进行查询，如下这样代表只查name和sex这两个栏位的。我们还可以按条件来进行查询，比如只查询性别为女的，嘻嘻这样就查到了性别为女的数据，可以看到，WHERE后面跟的就是条件我们还可以将结果进行排序这里可以看到他是按照id来倒序来排的，ORDER BY 后面跟的是排序条件，而那个DESC是倒序，默认是顺序，为ASC。查询还不止这些，比如还可以指定偏移数来查询多少数据，如下这里是查询年龄大于19岁的学生，并且按照id来降序来排，查询了第一页的3条数据。其中LIMIT就是指定查询多少页的多少数据。上述的 1 代表的是偏移数， 而 3 就是代表数据数，所有表示的是偏移一个数据来查下面的三个数据。其实，查询是有一条很长的表达式的，如下：SELECT data FROM tablename WHERE condition GROUP BY con HAVING condition ORDER BY condition LIMIT offset,num我们在写查询语句时，必须按照上面的顺序，粗体字就是关键字，要不会出错，没有时可以省略。再说说上面还没有说过的几个关键字。GROUP BY:这个是分组。HAVING:这个是按条件来分组。好了，查询的就这么多。3).修改数据格式为：UPDATE tablename SET data [WHERE condition]中括号里的是可以省略，为删除的条件，省略时就为更改所有数据上面的就是修改表格students的表格，其中SET的数据是修改的数据4).删除数据格式为：DELETE FROM tablename [WHERE condition]中括号和修改数据的意思一样，也挺简单的。注意：不写WHERE时是删除该表格的所有数据这是删除students表的性别为男的数据END好了，如果你跟着我一步步做，恭喜你，学会了简单的数据库语句，现在留个练习给你们，可以检验下自己有没有学会操作mysql：1.创建一个数据库,然后设计一个新闻表(数据类型要使用合理)2.使用SQL语句向数据表写入十五条不同的数据3.使用SQL语句查询类别为“百家”的新闻数据4.使用SQL语句删除一条新闻数据5.使用SQL语句查询所有的新闻，以添加时间的倒序进行排列6.使用SQL语句查询第二页数据(每一页5条数据)学会了mysql语句，如果想学更多请去官方文档哈，这是网址https://dev.mysql.com/doc/refman/5.7/en/。在下一篇文章我将会讲述如何用python来连接操作数据库。记得来看看哈！祝大家今天情人节快乐哈，我没什么礼物送给大家，只能送点学习资料，如果需要的话可以在后天回复资源即可获得哈 ！多谢大家捧场。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://sergiojune.com/tags/mysql/"}]},{"title":"如何爬取asp动态网页？搞定可恶的动态参数，这一文告诉你！","date":"2018-08-07T00:43:41.780Z","path":"2018/08/07/如何爬取asp动态网页？搞定可恶的动态参数，这一文告诉你！/","text":"这个asp网站是我的学校的电费查询系统，需要学校的内网才能查询，所以这文说下思路和我遇到的一些坑。我搞这个网站主要是为了方便查电费而已，其实也方便不了多少。而且这个asp网站还不是很容易爬，因为里面有两个可变的参数，会根据页面来变化。好了，先看看页面这个网站需要先登陆进自己的宿舍才能进去，还有很烂的验证码，不过我实现到验证码写入的时候发现这个验证码是可以随便填的，这个就感觉有点垃圾。这个登陆页面有很多坑，下面说下1.上面右边所指的就是两个动态变化的参数，怎么来的呢？是根据上一个页面来的，每个页面都会带有这两个参数，所以我们需要每次访问一次都需要匹配下这两个值就行动态更换，如果不跟换的话，会得不到数据，还会出现下面这个错误。‘236|error|500|回发或回调参数无效。在配置中使用&nbsp;&lt;pages&nbsp;enableEventValidation=”true”/&gt;&nbsp;或在页面中使用&nbsp;&lt;%@&nbsp;Page&nbsp;EnableEventValidation=”true”&nbsp;%&gt;&nbsp;启用了事件验证。出于安全目的，此功能验证回发或回调事件的参数是否来源于最初呈现这些事件的服务器控件。如果数据有效并且是预期的，则使用 ClientScriptManager.RegisterForEventValidation 方法来注册回发或回调数据以进行验证。|’这个就说明你没有更换好上面所说的两个参数注意：第一次访问这个网站是不会有宿舍楼层宿舍号这些数据的，需要进行匹配上面的两个可变参数再进行post才会有数据。2.在你选好你的宿舍楼层号之后表单数据就会出现变化可以看到表单的参数顺序和上面的不一样了，所以在选好宿舍楼层之后我们需要把变单顺序改变后再把参数post出去，要不还会出现上面那个坑，就是回调参数无效第一个箭头所指的参数也需要改变，不过第二个参数是txtname2，也就是每层楼的默认宿舍值，这个固定也没事，不会出错，时间的话还是需要根据自己访问时间来进行变化的，要不也会出现错误，还是同样的错误，也就是下面的这个错误，可想而知asp网站对这些参数是有很挑剔的要求。236|error|500|回发或回调参数无效。在配置中使用 &lt;pages enableEventValidation=”true”/&gt; 或在页面中使用 &lt;%@ Page EnableEventValidation=”true” %&gt; 启用了事件验证。出于安全目的，此功能验证回发或回调事件的参数是否来源于最初呈现这些事件的服务器控件。如果数据有效并且是预期的，则使用 ClientScriptManager.RegisterForEventValidation 方法来注册回发或回调数据以进行验证。|3.这个电费查询按钮，不是ajax，会有新的请求，而且是对同一个网址的不同请求方式，第一次请求时get请求，用于获取asp网页的那两个动态参数，第二次是将动态参数就行post发送出去，这样就会有数据了，如果你是第一次就post的话，会没有数据，网页还是会报错误，同样还是那个错误哈。下面是表单数据self.data = {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTTARGET’:&nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$btnSelect’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTARGUMENT’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘VIEWSTATE’:&nbsp;self.data[‘VIEWSTATE’],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTVALIDATION’:&nbsp;self.data[‘EVENTVALIDATION’],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘hidJZ’:&nbsp;‘jz’+name,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$TextBox1’: (datetime.now()-timedelta(days=30)).strftime(‘%Y-%m-%d’),&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$TextBox2’: datetime.now().strftime(‘%Y-%m-%d’),&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$txtDBBH’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$ddlCZFS’:&nbsp;‘—-全部—-‘,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$toolbarButtom$pagesize’:&nbsp;‘1’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘box_page_state_changed’:&nbsp;‘false’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘2_collapsed’:&nbsp;‘false’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘6_selectedRows’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘box_disabled_control_before_postbac’:&nbsp;‘10’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘box_ajax_mark’:&nbsp;‘true’&nbsp; &nbsp; &nbsp; &nbsp;}坑说完了，说说部分代码的作用吧def&nbsp;get_value(self, html):&nbsp;&nbsp;# 获取表单的两个参数VIEWSTATE和EVENTVALIDATION&nbsp; &nbsp; &nbsp; &nbsp;try:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;soup = BeautifulSoup(html,&nbsp;‘lxml’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value = soup.select(‘input[type=”hidden”]’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values = [v&nbsp;for&nbsp;v&nbsp;in&nbsp;value&nbsp;if&nbsp;‘/w’&nbsp;in&nbsp;str(v)]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;state = values[0][‘value’]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action = values[1][‘value’]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘VIEWSTATE’] = state&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘EVENTVALIDATION’] = action&nbsp; &nbsp; &nbsp; &nbsp;except&nbsp;IndexError&nbsp;as&nbsp;e: &nbsp;# 证明这个不是首页，需要另外的规则&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;match = re.search(‘VIEWSTATE|(.?)|.?EVENTVALIDATION|(.*?)|‘, html)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘VIEWSTATE’] = match.group(1)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘EVENTVALIDATION’] = match.group(2)&nbsp; &nbsp; &nbsp; &nbsp;except&nbsp;Exception&nbsp;as&nbsp;e:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘get_value’, e)这个就是获取两个动态参数的方法，每次根据html元素来获取def&nbsp;get_name(self, jz, html=None): &nbsp;# 输入宿舍号&nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;html:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 表单顺序需要改变&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data = {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘ScriptManager1’:&nbsp;‘UpdatePanel1|txtjz2’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘hidtime’: datetime.now().strftime(‘%Y-%m-%d %H:%M:%S’),&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘Radio1’:&nbsp;‘1’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtjz2’: jz,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtname2’:&nbsp;‘001001001001001’, &nbsp;# 这个初始化值可以随意，但不能为空&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtpwd2’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtyzm2’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTTARGET’:&nbsp;‘txtjz2’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTARGUMENT’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘LASTFOCUS’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘VIEWSTATE’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTVALIDATION’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘ASYNCPOST’:&nbsp;‘true’&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.get_value(html) &nbsp;# 换下参数&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;html =&nbsp;self.get_html()&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;html:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;soup = BeautifulSoup(html,&nbsp;‘lxml’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dormitory_num = soup.select(‘select[name=”txtname2”] option’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dormitory_num = [(p.text, p[‘value’])&nbsp;for&nbsp;p&nbsp;in&nbsp;dormitory_num]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for&nbsp;index, p&nbsp;in&nbsp;enumerate(dormitory_num):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(index,&nbsp;‘宿舍号：’, p[0])&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.get_value(html)&nbsp; &nbsp; &nbsp; &nbsp;while&nbsp;True:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num = input(‘请输入你的宿舍，输入左边的编号即可’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num = re.match(‘\\d+’, num)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;num&nbsp;and&nbsp;int(num.group()) &lt; len(dormitory_num):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num = int(num.group())&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘请输入正确的宿舍编号’)&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;dormitory_num[num][1]这个是获取宿舍号，表单顺序需要改变def get_chapter(self):&nbsp; &nbsp; &nbsp; &nbsp;# 获取验证码&nbsp; &nbsp; &nbsp; &nbsp;url =&nbsp;‘http://172.18.2.42:8000/ValidateCode.aspx&#39;&nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url, headers=self.headers)&nbsp; &nbsp; &nbsp; &nbsp;with&nbsp;open(‘code.jpg’,&nbsp;‘wb’)&nbsp;as&nbsp;f:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(response.content)&nbsp; &nbsp; &nbsp; &nbsp;image = Image.open(‘code.jpg’)&nbsp; &nbsp; &nbsp; &nbsp;image.show()&nbsp; &nbsp; &nbsp; &nbsp;code =&nbsp;input(‘请输入验证码’)&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;code这个是获取验证码的方法，获取验证码是很简单的，就是找到请求的url进行请求就可以了。至于识别，我这里是手动输入，你也可以选择接入打码平台或者用深度学习模型来识别。其他的就不多说了。需要源码的可以在我的GitHub上找：https://github.com/SergioJune/gongzhonghao_code/blob/master/python_play/query.py写在最后如果这篇文章对你用的话，希望不要吝啬你的点赞哈！点赞和转发就是对我的最大支持，这样才有动力输出质量高的原创文章。「点赞是一种态度！」推荐文章：我爬取了37000条球迷评论，知道了这场比赛的重要信息爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf","date":"2018-08-07T00:43:41.761Z","path":"2018/08/07/爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf/","text":"这是日常学python的第15篇原创文章前几篇文章我们学习了requests库和正则，还有个urllib库，我上篇文章也用了requests库来教大家去爬那些返回json格式的网页，挺好玩的。有读者让我来个正则的，所以我今天就来个正则+requests来进行爬取。今天原来是想爬小说的，但想到我不怎么看小说，读者也是都喜欢学习的，对吧？嘻嘻！所以我来爬个与python相关的内容，恰好前几天我又看到别人推荐的一本python进阶书，这本书的作者是我们的python大神kennethreitz征集各路爱好python的人所写的，下面是地址：中文版:http://pythonguidecn.readthedocs.io/zh/latest/英文版：http://docs.python-guide.org/en/latest/这本书适合于一切有python的学习者，不管你是初入python的小白，还是熟练使用python的老手，都适用。但是不看也没有影响你学习爬虫哈，这个只是说些python的奇淫技巧。由于这本书在网上只有英语的电子版，可我英语渣渣，所以爬个中文版的网页然后把他弄成电子版。若想直接获取该书电子版，可以在公众号「日常学python」后台回复『进阶』直接获取。本篇文章用到的工具如下：requests库正则表达式Sigil：将html网页转成epub电子书epub转pdf:http://cn.epubee.com/epub转pdf.html好了，下面详细分析：1分析网站内容可以看到首页中有整本书的内容链接，所以可以直接爬首页获取整本书的链接。熟练地按下f12查看网页请求，非常容易找到这个请求网站为：http://pythonguidecn.readthedocs.io/zh/latest/请求方式为get，状态码为200，而且返回的是html元素，所以我们可以用正则来匹配所需要的内容。那看看我们的匹配内容所在的地方可以看到这个内容的地址和内容标题都在这个a标签上，所以正则很容易，如下：toctree-l1.?reference internal“ href=”([^“]?)”&gt;(.?)&lt;/a&gt;不知道你的正则学得怎样了，这里还是简单说下：.：这个是概括字符集，为匹配除换行符以外的任意字符：这个是数量词，匹配的次数为0次以上?：加了这个问号表示非贪婪，一般默认为贪婪[^”]：这个表示不匹配双引号，挺好用的实在不记得的可以看看我这篇文章，这里不详细说了,不记得就点开爬虫必学知识之正则表达式下篇看看这里需要注意的是：在这里获取的网址列表里面有个内容的导航，如下：所有我们在匹配完之后还需要再将这些带#号的网址给过滤掉。接下来的就是获取每个网页的内容可以看到内容都在这个div标签内，所以和上面一样，用正则就可以获取了。ps: 其实这里用BeautifulSoup更好用，我会在后面文章中讲到哈！匹配内容的正则为：section“.?(&lt;h1&gt;.?)&lt;div class=”sphinxsidebar因为我的那个工具是把这些内容的html下载下来就可以了，所以接下来不需要清洗里面的html元素。内容分析完毕，接下来的就容易了，就是用个循环把遍历所有文章，然后就利用正则把他爬下来就可以了。2实操部分import re, requestsclass Spider(object): &nbsp; &nbsp;def init(self, headers, url): &nbsp; &nbsp; &nbsp; &nbsp;self.headers = headers &nbsp; &nbsp; &nbsp; &nbsp;self.url = url &nbsp; &nbsp;def get_hrefs(self): &nbsp; &nbsp; &nbsp; &nbsp;‘’’获取书本的所有链接’’’ &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(self.url, self.headers) &nbsp; &nbsp; &nbsp; &nbsp;if response.status_code == 200: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response.encoding = ‘utf-8’ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hrefs = re.findall(‘toctree-l1.?reference internal” href=”([^”]?)”&gt;(.*?)&lt;/a&gt;’, response.text, re.S) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return hrefs &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘访问书本内容失败，状态码为’, response.status_code) &nbsp; &nbsp;def get_page(self, url): &nbsp; &nbsp; &nbsp; &nbsp;‘’’获取首页’’’ &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url, self.headers) &nbsp; &nbsp; &nbsp; &nbsp;response.encoding = ‘utf-8’ &nbsp; &nbsp; &nbsp; &nbsp;content = re.findall(‘section”.?(&lt;h1&gt;.?)&lt;div class=”sphinxsidebar’, response.text, re.S) &nbsp; &nbsp; &nbsp; &nbsp;return content[0] &nbsp; &nbsp;def get_content(self, href): &nbsp; &nbsp; &nbsp; &nbsp;‘’’获取每个页面的内容’’’ &nbsp; &nbsp; &nbsp; &nbsp;if href: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;href = self.url + href &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(href, self.headers) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response.encoding = ‘utf-8’ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;content = re.findall(‘section”.?(&lt;h1&gt;.?)&lt;div class=”sphinxsidebar’, response.text, re.S) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if content: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return content[0] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘正则获取失败’) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘获取内容失败’) &nbsp; &nbsp;def run(self): &nbsp; &nbsp; &nbsp; &nbsp;‘’’循环获取整本书内容’’’ &nbsp; &nbsp; &nbsp; &nbsp;self.num = 0 &nbsp; &nbsp; &nbsp; &nbsp;hrefs = self.get_hrefs() &nbsp; &nbsp; &nbsp; &nbsp;content = self.get_page(self.url) &nbsp; &nbsp; &nbsp; &nbsp;with open(str(self.num)+‘Python最佳实践指南.html’, ‘w’, encoding=‘utf-8’) as f: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(content) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘写入目录成功’) &nbsp; &nbsp; &nbsp; &nbsp;for href, title in hrefs: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if “#” in href: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;continue &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.num += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;content = self.get_content(href) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with open(str(self.num)+title+‘.html’, ‘w’, encoding=‘utf-8’) as f: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(content) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载第’+str(self.num)+‘章成功’) &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载完毕’)def main(): &nbsp; &nbsp;url = ‘http://pythonguidecn.readthedocs.io/zh/latest/&#39; &nbsp; &nbsp;headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36’} &nbsp; &nbsp;spider = Spider(headers, url) &nbsp; &nbsp;spider.run()if name == ‘main‘: &nbsp; &nbsp;main()点击运行，感觉美滋滋，可惜啊，代码总是爱玩弄你，赶紧报了个错： &nbsp;File “E:/anaconda/python_project/newspaper/spider.py”, line 52, in run &nbsp; &nbsp;with open(str(self.num)+title+‘.html’, ‘w’, encoding=‘utf-8’) as f:FileNotFoundError: [Errno 2] No such file or directory: ‘38与C/C++库交互.html’一眼看下去，还挺郁闷的，我没有打开文件的，都是在写文件，为什么报了这个错？仔细一看报错内容，这个名字有问题啊，你看38与C/C++库交互.html这个在window系统是以为你在&nbsp;38与C&nbsp;的&nbsp;C++库交互.html&nbsp;下的，怪不得会报错，所以，我在这里加了这个代码把/给替换掉3把内容整成pdf点击Sigil 的&nbsp;+&nbsp;号把刚才下载的内容导入生成目录添加书名作者添加封面：点击左上角的&nbsp;工具&nbsp;-&gt;&nbsp;添加封面&nbsp;即可点击保存即可完成转pdf：http://cn.epubee.com/epub转pdf.html这个很容易就不说了。结语好了，文章内容就这么多，下个文章就是学习新内容了。期待ing。上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑老司机带你用python来爬取妹子图◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;爬虫必学知识之正则表达式下篇日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://sergiojune.com/tags/正则表达式/"}]},{"title":"爬虫必学知识之正则表达式下篇","date":"2018-08-07T00:43:41.734Z","path":"2018/08/07/爬虫必学知识之正则表达式下篇/","text":"这是日常学python的第13篇原创文章继上篇文章说了正则表达式的简单用法，那今天我们就继续说一下正则表达式的复杂的用法。好了，废话不多说，直接进入正题。正则表达式情景：当你想要匹配一个qq号，qq号码长度为5-10位，那根据上篇文章的说法，很容易就可以想到该正则：[0-9]{5,10}这样是可以的，但是当你匹配一个长度大于10的号码时就会出错，这时就会去该字符串的前10个数字出来，如下：import rea=‘221753259265’r=re.findall(‘[0-9]{5,10}’,a)#明显当查找的字符串长度大于8位时就会出错，只会截取前一部分长度print(r)# 结果[‘2217532592’]这样的话你就会得到一个错误的qq号码。这时就需要引入边界匹配了：^：这个是从左边开始匹配，规定左边的首个字符$：这个是从右边开始匹配，规定右边的首个字母现在再写个匹配qq号码的正则r=re.findall(‘^[0-9]{5,10}$’,a)#这个表示从左边起为5-10的数字长度，右边也是一样print(‘第一个匹配结果：’,r)a = ‘2217532592’r=re.findall(‘^[0-9]{5,10}$’,a)print(‘第二个匹配结果：’,r)# 结果第一个匹配结果： []第二个匹配结果： [‘2217532592’]这样就可以匹配到了，是不是很神奇？组：前面我们有用 [ ] 来匹配，中括号里面表示的是或关系，而这里的组表示的是并关系，并且用小括号括起来 ( )。比如：重复 python 字样三次import rea=‘pythonpythonpythonjakjpythonpythonsdjjpythonpythonpythonsd’r=re.findall(‘(python){3}’,a)print(r)# 结果[‘python’, ‘python’]这里的结果不是返回三个python，而是返回这个组，当符合一次就会将此组添加到返回列表中一次。这个组还挺好用的，再看下这个需求：获取下列英文中的life和python之间的内容。a=‘life is short,i use python’r=re.findall(‘life(.)python’,a,re.S)print(r) # 这样获取的就是组内的内容# 结果[‘ is short,i use ‘]这个组还常用，因为在我们经常在用正则来解析html元素时，经常需要获取两个标签之间的内容，标签是确定的，标签内容不确定，就可以用这个了。如下这个html元素：&lt;strong&gt;&lt;a href=“#py2”&gt;python进阶 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“#python3”&gt;python入门 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“#vce”&gt;vce解决方法 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“demo06.html#new” target=“_blank”&gt;百度 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“mailto: 2217532592@qq.com“&gt;反馈意见&lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“img/1.jpg”&gt;下载图片 &lt;/a&gt; &nbsp; &nbsp;&lt;/strong&gt;这样就可以用组来获取a标签的内容了：&lt;a .?&gt;(.?)&lt;/a&gt;。？表示非贪婪哦！re.findall(pattern,string,flags)：这个方法的前两个参数对你们来说都很熟悉了，第一个参数为正则表达式，第二个参数为要进行匹配的字符串，而第三个可选参数为匹配模式，有如下几种匹配模式：re.I(re.IGNORECASE) ：使匹配对大小写不敏感re.L(re.LOCAL)：做本地化识别（locale-aware）匹配re.M(re.MULTILINE)：多行匹配，影响 ^ 和 $re.S(re.DOTALL)：使 . 匹配包括换行在内的所有字符(这个常用)re.U(re.UNICODE)：根据Unicode字符集解析字符。这个标志影响 \\w, \\W, \\b, \\B.re.X(re.VERBOSE)：该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解当需要写多个匹配模式时，可以用 | 分隔每个模式代码如下：a=‘Java12Python89’r=re.findall(‘python’,a,re.I)print(r)a=‘hsjhj h123jfkksf hajkGH\\nkj fjfk’r=re.findall(‘.’,a,re.I|re.S)print(r)# 结果[‘Python’][‘h’, ‘s’, ‘j’, ‘h’, ‘j’, ‘ ‘, ‘h’, ‘1’, ‘2’, ‘3’, ‘j’, ‘f’, ‘k’, ‘k’, ‘s’, ‘f’, ‘ ‘, ‘h’, ‘a’, ‘j’, ‘k’, ‘G’, ‘H’, ‘\\n’, ‘k’, ‘j’, ‘ ‘, ‘f’, ‘j’, ‘f’, ‘k’]正则除了可以用来检索字符串，还可以用来替换字符串，常见的可以用来替换那些文本中的空格，制表符和回车等，这些都是用一个正则就可以搞定的了。python中用这个方法来进行正则替换re.sub(pattern,&nbsp;repl,&nbsp;string,&nbsp;count=0,&nbsp;flags=0)&nbsp;pattern ：正则表达式repl ：替换后的字符串，可为函数string ：要进行替换的字符串count ：替换的次数，顺序为 从左往右，默认值为0，表示无限次。falgs ： 匹配模式，和findall()差不多代码如下：import rea=‘skjC#ksjfc#jkdsc#’r=re.sub(‘c#’,‘gg’,a)#返回值是替换后的字符串print(r)print(a)r=re.sub(‘c#’,‘gg’,a,1) # 这个加了替换次数print(r)r=re.sub(‘c#’,‘gg’,a,1,re.I) # 加了匹配模式，忽视大小写print(r)# 结果skjC#ksjfggjkdsggskjC#ksjfggjkdsc#skjggksjfc#jkdsc#我们试试第二个参数为函数的情况def convert(value):#他是把对象传进去这个参数 &nbsp; &nbsp;print(value) &nbsp; &nbsp;#可以通过group()方法来获取内容 &nbsp; &nbsp;return ‘!!’+value.group()+“!!”r=re.sub(‘c#’,convert,a,flags=re.I)#接收个参数后，更改后的内容为他的返回值print(r)# 结果&lt;_sre.SRE_Match object; span=(3, 5), match=‘C#’&gt;&lt;_sre.SRE_Match object; span=(9, 11), match=‘c#’&gt;&lt;_sre.SRE_Match object; span=(15, 17), match=‘c#’&gt;skj!!C#!!ksjf!!c#!!jkds!!c#!!这个第二个参数为convert函数，里面的.group() 方法是获取匹配后的字符串的值，所以我们就可以根据匹配后的字符串来进行相对应的替换内容，比如这个简单的小需求：把字符串中的数字大于50的改为99，小于的就改为11。a=‘ds+45sd78asd12568asd45asd74ew+9ddf12sd45’def func(value): &nbsp; &nbsp;if int(value.group())&gt;50: &nbsp; &nbsp; &nbsp; &nbsp;return ‘99’ &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp;return ‘11’r=re.sub(‘\\d{1,2}’,func,a)print(r)# 结果ds+11sd99asd119911asd11asd99ew+11ddf11sd11另谈两个函数re.match(pattern,string,flags) ：这个是从字符串的首个字母开始匹配，若首个字母不符合，就会返回None, 反之返回一个 Match对象。而他只会匹配第一个结果，不会返回所有符合结果的内容。参数内容与findall()方法一样。re.search(pattern,string,flags) ：这个与match方法差不多，不过不是从首字符开始匹配，也是只返回一个正确的匹配内容。代码：import rea=‘pythonphpjavacphp’r=re.match(‘php’,a)#这个一开始没有就返回Noneprint(r)r=re.search(‘php’,a)#这个搜索到之后就返回一个对象#返回的对象可以通过group()方法来获取他的内容print(r)# 获取匹配内容print(r.group())# 结果None&lt;_sre.SRE_Match object; span=(6, 9), match=‘php’&gt;php这两个函数返回的内容的几个属性：group() ：获取匹配的内容statr() ：获取到匹配字符的起始位置end() ：获取匹配到字符的结束位置span() ：获取匹配到字符的起始和结束位置，元组形式返回。前面提到组的概念，试下这两个方法的组的用法：import re#获取life和python之间的内容a=‘life is short,i use python’r=re.search(‘life(.)python’,a,re.S)#用小括号的就是一组print(r.group(1))#这个下标1就是对应的中间部分#也可以获取中间的两部分a=‘javawoshipythonjunephp’r=re.search(‘java(.)python(.)php’,a)#两个小括号就是分成了两组print(r.group(1),r.group(2))#分别打印第一第二组print(r.groups())#这个获取所有分组信息# 结果 is short,i use woshi june(‘woshi’, ‘june’)上面的代码注释已经很清楚了，还有个group()方法是获取整个正则匹配的内容，不按分组。match()方法也一样，就不演示了。最后一个问题：怎样拆分含有多种分隔符的字符串？比如：kfs;hsji’fhsikfbhsfk=jsf/shj。要将不属于字母的都去掉，你是不是会想到用字符串的循环，然后再一个一个分割出来？我告诉你，学了正则之后，再也不用这么麻烦了。re库里面有个split()方法，如下：re.split(pattern, string, maxsplit=0)，参数看名字应该就能知道。直接一行代码进行分割：a = ‘kfs;hsjifhsikfbhsfk=jsf/shj’r = re.split(‘[;*=/]’, a)print(r)# 结果[‘kfs’, ‘hsjifhsikf’, ‘bhsfk’, ‘jsf’, ‘shj’]是不是很完美？所以说正则必须得学！END这个正则复杂点的已经说完了，还有些进阶的，不过暂时没有用到，就不打算说了，需要的可以去百度看看哈！留个小练习证明自己正则学得好怎么样：1.kevintian126@126.com&nbsp;2. 1136667341@qq.com&nbsp;3. meiya@cn-meiya.com&nbsp;4. wq901200@hotmail.com&nbsp;5. meiyahr@163.com6.&nbsp;meiyuan@0757info.com&nbsp;7. chingpeplo@sina.com&nbsp;8. tony@erene.com.com9. melodylu@buynow.com用正则把上面的@与com之间的内容匹配出来，可以把你的答案写在留言区上，过两天在留言区公布答案哈！上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑爬虫必学知识之正则表达式上篇◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;python使用requests+re简单入门爬虫日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://sergiojune.com/tags/正则表达式/"}]},{"title":"爬虫必学知识之正则表达式上篇","date":"2018-08-07T00:43:41.697Z","path":"2018/08/07/爬虫必学知识之正则表达式上篇/","text":"这是日常学python的第12篇文章在向网页进行了提交请求之类的之后，我们可以得到了网页的返回内容，里面自然而然会有我们想要的数据，但是html元素文本这么多，我们不可能一 一去找我们需要的数据，这时就需要用到正则表达式了，正则表达式是学爬虫必须学的内容，而且不止python可以用，java等其他语言都可以用，所以学了好处大大。什么是正则表达式？正则表达式就是一个特殊的字符序列，可以用于检测一个字符串是否与我们的所设定的字符串相匹配。功能有快速检索文本和快速替换一些文本的操作。python里面有个处理正则表达式的库 re。有个方法findall(pattern,string,flags)用来匹配正则达式，我们就先用这个方法处理下。参数如下：pattern：正则表达式string：要进行匹配的字符串flags：匹配的模式结果是一个匹配内容的列表‘\\d’ : 这个表示匹配单个0-9数字‘\\D’ : 与\\d相反，匹配数字以外的内容用代码来感受下：import re#这个库是用来使用正则表达式的a=‘sg+sga5g45gaae3f23hgt13’r=re.findall(‘\\d’,a)#这个就是根据\\d的正则表达式来查找对应字符，其中\\d是对应0-9的数字#查找非数字用\\Dr1=re.findall(‘\\D’,a)print(r)print(r1)#结果[‘5’, ‘4’, ‘5’, ‘3’, ‘2’, ‘3’, ‘1’, ‘3’][‘s’, ‘g’, ‘+’, ‘s’, ‘g’, ‘a’, ‘g’, ‘g’, ‘a’, ‘a’, ‘e’, ‘f’, ‘h’, ‘g’, ‘t’]可以看到找出了字符串中的数字和非数字如果我们在匹配一个字符串时，中间内容是有多个变化的，我们需要应变多种不同的字符，如这个字符串a=‘abc,acc,agc,anc,afc,adc,aec’如果需要匹配这个字符串时，我们就需要用到&nbsp;[ ]&nbsp;,用中括号括起来的字符，里面的内容表示或关系，那来看看代码import rea=‘abc,acc,agc,anc,afc,adc,aec’#现在查找上面的中间字符为c或者f的字符串#这个要求可以用到字符集来实现r=re.findall(‘a[cf]c’,a)#[]表示字符集，里面的内容是或关系# 结果[‘acc’, ‘afc’]上面匹配了中间字符是c或者是f的字符串，匹配中间字符非c和非f,可以在前面加个 ^ 符号import rer=re.findall(‘a[……cf]c’,a)#[c-f]就是表示从c到fprint(r)# 结果[‘abc’, ‘agc’, ‘anc’, ‘adc’, ‘aec’]上面只处理了中间字符为chu者f的字符串，但是没有匹配全部的，若要匹配全部，可以加个 - 符号，表示范围，如下import rer=re.findall(‘a[b-n]c’,a)#[c-f]就是表示从c到fprint(r)# 结果[‘abc’, ‘acc’, ‘agc’, ‘anc’, ‘afc’, ‘adc’, ‘aec’][b-n]:这个就是表示b到n的字符匹配汉字：[\\u4E00-\\u9FA5]概括字符集：用一个 \\ + 字母&nbsp;表示一系列的字符的元字符，只能匹配单个字符，常用的如下\\w：匹配数字和字符「不包括&amp;符号」只匹配单词，数字和下划线\\W：与\\w相反，这个包括空格和回车\\s：匹配空格字符，如空格，回车和制表符\\S：与\\s相反.：匹配除换行符之外的其他字符还有前面的\\d和\\D也是有个小技巧：如果想要匹配所有字符，就可以把上面的两个相反的合并起来就可以了。代码如下：#概括字符集,就是用一个\\加个字母来表示一类字符，比如刚开始的\\d,\\Dimport rea=‘hdsk\\n122\\rs3$ dkl%df36\\t5&amp;’r=re.findall(‘\\w’,a)#这个是匹配数字和单词print(r)#也可以匹配非数字非单词r=re.findall(‘\\W’,a)print(r)#匹配空格字符和制表符等其他字符r=re.findall(‘\\s’,a)print(r)#匹配除换行符之外的其他字符r=re.findall(‘.’,a)print(r)# 结果[‘h’, ‘d’, ‘s’, ‘k’, ‘1’, ‘2’, ‘2’, ‘s’, ‘3’, ‘d’, ‘k’, ‘l’, ‘d’, ‘f’, ‘3’, ‘6’, ‘5’][‘\\n’, ‘\\r’, ‘$’, ‘ ‘, ‘%’, ‘\\t’, ‘&amp;’][‘\\n’, ‘\\r’, ‘ ‘, ‘\\t’][‘h’, ‘d’, ‘s’, ‘k’, ‘1’, ‘2’, ‘2’, ‘\\r’, ‘s’, ‘3’, ‘$’, ‘ ‘, ‘d’, ‘k’, ‘l’, ‘%’, ‘d’, ‘f’, ‘3’, ‘6’, ‘\\t’, ‘5’, ‘&amp;’]数量词：当一个字符需要连续重复匹配多次时，就要用到这个。如匹配三个字符组成的字符串：[a-zA-z]{3}&nbsp;，大括号里面的表示重复次数。若要匹配三到六个字符，大括号的就需要这样写：{3,6}.代码如下;#数量词，当一个字符需要多次重复匹配时就需要用到import rea=‘python java111php23 html’r=re.findall(‘[a-z]{3}’,a)#重复多次就用大括号，括号内的数表示重复的次数print(r)#也可以重复一个范围，表示匹配3到6个字符r=re.findall(‘[a-z]{3,6}’,a)print(r)#这样就可以把单词都找出来了# 结果[‘pyt’, ‘hon’, ‘jav’, ‘php’, ‘htm’][‘python’, ‘java’, ‘php’, ‘html’]其他数量词表示： ：匹配零次或无限多次+：匹配一次或以上?：匹配零次或者一次a=‘pytho243python34pythonn’#表示匹配对应内容0次或者无限次r=re.findall(‘python*’,a)#这个就是代表对n字符的数量词匹配print(r)#+表示匹配内容1次或者无限次r=re.findall(‘python+’,a)print(r)#?表示可以匹配0次或者1次,注意这个？和上面的非贪婪代表的意思不一样r=re.findall(‘python?’,a)print(r)# 结果[‘pytho’, ‘python’, ‘pythonn’][‘python’, ‘pythonn’][‘pytho’, ‘python’, ‘python’]贪婪匹配：正则表达式默认为贪婪匹配，即匹配符合字符串的最大长度，如上面的[a-zA-z]{3,6}，他会趋于匹配长度为6的字符串，匹配到条件不满足时才停止匹配。非贪婪匹配：就是趋于匹配长度最小的字符串，匹配满足第一个条件就会停止匹配r=re.findall(‘[a-z]{3,6}’,a)# 贪婪匹配print(r)r=re.findall(‘[a-z]{3,6}?’,a)print(r)#由于是非贪婪，所以匹配当第一个条件满足时就停止匹配# 结果[‘python’, ‘java’, ‘php’, ‘html’][‘pyt’, ‘hon’, ‘jav’, ‘php’, ‘htm’]END这篇文章只是介绍了下正则表达式的简单用法，可以用来入门正则，下一篇文章讲正则表达式高级点的用法。留个小练习：写一个正则来匹配生日，字符串为：2005-06-092005-6-92005 6 92005，06，09可以把答案写在留言区哈！上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑python爬虫常用库之requests详解◐◑&nbsp;python使用requests+re简单入门爬虫◐◑python爬虫常用库之urllib详解日常学python代码不止bug,还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://sergiojune.com/tags/正则表达式/"}]},{"title":"老司机带你用python来爬取妹子图","date":"2018-08-07T00:43:41.675Z","path":"2018/08/07/老司机带你用python来爬取妹子图/","text":"这是日常学python的第14篇原创文章我前几篇文章都是说一些python爬虫库的用法，还没有说怎样利用好这些知识玩一些好玩的东西。那我今天带大家玩好玩又刺激的，嘻嘻！对了，requests库和正则表达式很重要的，一定要学会！一定要学会！！一定要学会！！！我现在的爬虫基本都是用这两样东西来爬的。所以学不学你看着办吧。来到今天的重点，我今天发现一个网站很好爬的，非常适合新手，我没有设置请求头什么的爬了很多遍很没有封我ip和给我返回403之类的，所以他对我们第一次玩爬虫的人来说很友好。这个网站就是今日头条。最重要的是这里面有很多美女图片，我们可以把它们爬下来！！！是不是想想都要流鼻血啊？文章首发于公众号「日常学python」我们今天要爬的就是他的图集，先看看网站。搜索美女，然后点击图集，可以看到下面这些内容我们要做的就是把上面的图片给爬下来。那开始分析网站。按下f12，然后点击network，刷新下你可以看到这些进行寻找哪个请求返回这些图片的，在网页上可以看到图片会随着你下拉网页而进行显示更多的图片，这是动态加载的，所以可以轻松知道这个可以在xhr文件中找到，果然，你看不断往下拉，不断地发送请求，点击这个请求看看是返回什么数据可以看到这是个json，里面有图片的url，这个就是我们要找的东西，那我们可以用json库来解析，还有这个网站是get请求，这样就可以用requests库来发送然后解析下就可以了，非常简单。那么分析就到这里，直接上代码import requests, ospath_a = os.path.abspath(‘.’)kw = ‘’while True: &nbsp; &nbsp;kw = input(‘请输入你要获取的图片(若想结束请输入1)’) &nbsp; &nbsp;if kw == ‘1’: &nbsp; &nbsp; &nbsp; &nbsp;print(‘已退出，你下载的图片已保存在’+path_a+‘,请查看！’) &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp;for x in range(0, 1000, 20): &nbsp; &nbsp; &nbsp; &nbsp;url = ‘https://www.toutiao.com/search_content/?offset=&#39;+str(x)+‘&amp;format=json&amp;keyword=%s&amp;autoload=true&amp;count=20&amp;cur_tab=3&amp;from=gallery’ % kw &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url) &nbsp; &nbsp; &nbsp; &nbsp;data = response.json()[‘data’] &nbsp; &nbsp; &nbsp; &nbsp;if not data: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载’+kw+‘图片完毕，请换个关键词继续’) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp; &nbsp; &nbsp;n = 1 &nbsp;# 记录文章数 &nbsp; &nbsp; &nbsp; &nbsp;for atlas in data: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 创建目录 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;title = atlas[‘title’] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(atlas) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;try: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if title not in os.listdir(‘.’): &nbsp;# 防止文件名已经存在 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.mkdir(title) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;except OSError as e: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘文件名出错，创建目录失败，重新创建一个随机名字’) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;title = kw + ‘文件名出错’+str(x) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if title not in os.listdir(‘.’): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.mkdir(title) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;k = 1 &nbsp;# 记录下载的图片数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;path = os.path.join(path_a, title) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 转进图片目录 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.chdir(path) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for image in atlas[‘image_list’]: &nbsp;# 这个链接获取的图片是小张的，看着不够爽，所以下面替换成大的图片 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_url = image[‘url’].replace(‘list’, ‘large’) &nbsp;# 改个链接获取大的图片 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;atlas = requests.get(‘http:’+image_url).content &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with open(str(k)+‘.jpg’, ‘wb’) as f: &nbsp;# 把图片写入文件内 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(atlas) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载完第%d个文章的%d幅图完成’ % (x+n, k)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;k += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 转出图片目录 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.chdir(path_a)这个只用了requests库基本就能完成了，os库是用来操作文件目录的，这里就不详细说了。可以看到，代码量非常少，除开注释就大概四十行吧，是不是比其他语言简洁多了？是不是requests库很好用？这里可以充分体现了人生苦短，我用python的真理。而且，他还可换关键字继续搜，你想搜什么照片都可以。下篇文章写个requests库和正则来爬内容的文章，让你们感受下正则的强大！最后给你们看下结果不说那么多了，我要去买营养快线了。上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑爬虫必学知识之正则表达式上篇◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;爬虫必学知识之正则表达式下篇日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"教你用python登陆豆瓣并爬取影评","date":"2018-08-07T00:43:41.654Z","path":"2018/08/07/教你用python登陆豆瓣并爬取影评/","text":"在上篇文章爬取豆瓣电影top250后，想想既然爬了电影，干脆就连影评也爬了，这样可以看看人们评价的电影，再加上刚出不久的移动迷官3好像挺热的，干脆就爬他吧，爬完看看好不好看！进入主题1.去找目标网页并找到所要找的数据一进去网页就条件反射打开开发者工具，很容易就看到了这个鼠标所点的就是我接下来要爬的网站，先看看他的response和请求头之类的信息，他的请求方式时get，response是一个网页结构，这就好办了，我们就可以用正则来匹配出所要的数据，正则还是个很好用的东西，请大家务必要学会啊。那接下来就动手敲代码咯！2.用re+requests获取数据获取信息先把数据写入txt文件中（打开的文件要指定编码为utf-8，要不会出现编码问题，因为window的默认编码方式是gbk，而你的编码为utf-8）正则表达式和网址一点击运行，只运行了两页，就出了问题，因为这个评论不止两页调试了下，在获取完第二页的时候他返回了个不存在的网页，导致我的正则表达式捕捉不到数据，出现了个空的page，所以就只下载了两页，这应该是被反爬了，继续回网页看看需要加什么请求头，然而我把全部的请求头的信息都加了，还是没用，这就触及到我的盲区了（尴尬脸），但是我可以百度啊，百度一看，看见有人说模拟登陆就可以了，那好，我就来模拟登陆一波！！！3.模拟登陆豆瓣首先需要看看登陆需要什么参数，这个参数是在豆瓣的登陆网址，先打开登陆，打开开发者工具(要不会看不到后面这个所需要的网页)，填好信息点击登陆，然后点击这个login网页，往下拉就会看到From Data 这个框，这个就是登陆所要的参数直接把他们复制过来即可然后就用post把信息发到服务器完成登陆，但是这有个问题，怎么保存登陆信息呢？这就需要用到Session()来保留了，但是注意，只需要建立一个会话信息就可以了，不是每个都用这个方法，我初学时就是犯这个错误以至于我搞了很久还没有登陆成功。代码如下然后用这个post上去，注意！注意！注意！post的网址是登陆网址，不是你要爬的网址，我刚学时也是被这个坑了很久(怎么感觉我很多问题)，还有其他用requests的都需要替换成self.ssession()​最后这样大功告成，由于只能获取500条这是因为豆瓣只开放了500条评论信息，多一条都不肯给4.登陆多了需要填验证码由于我多次登陆注销，然后我就需要填验证码了，然而这还是难不到我，还是分析网页找出验证码图片然后下载下来自己填写，还没有那些大佬那么厉害可以用人工智能来填写，代码如下还有将数据保留到数据库，我就不贴了，代码和上篇文章的差不多通过这个我学会了使用session来保存会话信息来登陆简单网页，还可以填写验证码，自己还是觉得有点高大上的，嘻嘻。由于本人还没学数据分析，就只能到这里，而生成词云也有点不会，直接复制粘贴来无趣，所以就先不写了，等大神你来写吧！最后非常感谢你看完了我的文章，如果觉得有用可以点赞，转发哈！若需要完整代码在微信公众号日常学python后台回复影评即可，若需要python相关的电子书也可以回复pdf获得，日后还会有更多福利发给你日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"反爬虫与反反爬虫","date":"2018-08-07T00:43:41.627Z","path":"2018/08/07/反爬虫与反反爬虫/","text":"喜欢爬虫的伙伴都知道，在爬网站的内容的时候并不是一爬就可以了，有时候就会遇到一些网站的反爬虫，折回让你爬不到数据，给你返回一些404，403或者500的状态码，这有时候会让人苦不堪言，就如我昨天发的爬网易云音乐评论，在你爬的数据较多时，网站认为你是一个机器，就不让你爬了，网易云就给我返回了一个{“code”:-460,”msg”:”Cheating”}，你不看下他的返回内容还不知道自己被反爬虫，不过不用担心，既然网页有反爬虫，可我们也有反反爬虫，今天就给大家说说反爬虫与反反爬虫。1网页的反爬虫1.通过网页的请求头首先我们先看看网易云音乐评论的请求头User-Agent：这个是保存用户访问该网站的浏览器的信息，我上面这个表示的是我通过window的浏览器来访问这个网站的，如果你是用python来直接请求这个网站的时候，这个的信息会带有python的字眼，所以网站管理员可以通过这个来进行反爬虫。Referer：当浏览器发送请求时，一般都会带上这个，这个可以让网站管理者知道我是通过哪个链接访问到这个网站的，上面就说明我是从网易云音乐的主页来访问到这个页面的，若你是用python来直接请求是，就没有访问来源，那么管理者就轻而易举地判断你是机器在操作。authorization:有的网站还会有这个请求头，这个是在用户在访问该网站的时候就会分配一个id给用户，然后在后台验证该id有没有访问权限从而来进行发爬虫。2.用户访问网站的ip当你这个ip在不断地访问一个网站来获取数据时，网页后台也会判断你是一个机器。就比如我昨天爬的网易云音乐评论，我刚开始爬的一首《海阔天空》时，因为评论较少，所以我容易就得到所有数据，但是当我选择爬一首较多评论的《等你下课》时，在我爬到800多页的时候我就爬不了，这是因为你这个ip的用户在不断地访问这个网站，他已经把你视为机器，所以就爬不了，暂时把你的ip给封了。2我们的反反爬虫1.添加请求头既然在请求网页的时候需要请求头，那么我们只需要在post或者get的时候把我们的请求头加上就可以了，怎样加？可以使用requests库来添加，在post，get或者其他方法是加上headers参数就可以了，而请求头不需要复制所有的信息，只需要上面的三个之中一个就可以，至于哪个自己判断，或者直接添加所有也可以，这样我们就可以继续爬了。2.使用代理ip若是网站把你的ip给封了，你添加什么的请求头也都没有用了，那我们就只有等他解封我们才可以继续爬吗？我可以十分自信告诉你：不需要，我们可以使用代理ip来继续爬，我们可以爬取网络上的免费ip来爬，至于免费的代理ip质量怎样你们应该知道，有必要可以买些不免费的，这样好点，我们平时的练习用免费的代理ip就可以了，可以自己爬取一些免费代理ip建成ip池，然后爬的时候就把ip随机取出来，我偷偷告诉你：小编明天的文章就是教你怎样搭建自己的代理ip池。END结束语：上面的只是个人在爬一些网站时候遇到的一些反爬虫，这只是很简单的，还有那些动态网站的反爬虫自己还没有接触，等到以后接触了，再一 一补充。最后给大家在爬虫上的建议，就是爬取速度不要太快，最好每几个就隔几秒，不要给服务器造成太大的压力，也可以在爬虫的时候选择一些访问量少点的时间段，这是对服务器好，也是对你自己好！日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"搭建自己的代理ip池","date":"2018-08-07T00:43:41.597Z","path":"2018/08/07/搭建自己的代理ip池/","text":"​继上一篇说了反爬虫之后，我说今天这篇文章会搭建一个属于自己的代理ip池，所以，为了不食言，就写了这篇文章，那好废话不多说，进入正题1目标网站爬取代理ip，这也需要找网页，这就得看看哪个网页提供这些代理ip了，本人知道了几个免费提供代理ip的网站，如下：无忧代理ip芝麻代理ip西刺代理ip云连代理ip我选择了爬取西刺代理的网站。2分析网站结构我们需要获取的高匿代理，按下F12打开开发者工具上面我们要获取的数据是ip地址，端口和类型这三个就可以了，可以看到，这些数据都在一个tr标签里，但是有两个不同的tr标签，这样可以用正则表达式，利用相隔的html结构先把整个内容匹配下来，再把重要信息匹配下来，最后就把他弄成这个样子{‘https’: ‘https://ip:端口&#39;}存入列表即可，最后就随机获取一个ip，然后可以先判断是否有用，再拿来做你此时项目的代理ip，判断是否用的方法就是随便拿一个百度获取别的网站，加上代理ip发送get请求，看看status_code()的返回码是不是200，即可，就如下面这样3代码部分1.匹配数据，并挑选数据存入列表2.随机获取ip，并写好ip格式我这里是把他存入列表，现抓现用，是因为我现在的爬虫项目都是很小的，只需要这些就可以了。END以上就是我简单搭建的代理ip池了，等到以后慢慢完善，你可以把他们存入你的数据库，然后要用的时候，就随机拿出来，先看看有没有用，没用的话就删除，有用就拿来用即可。日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"利用python爬取豆瓣电影Top250，并把数据放入MySQL数据库","date":"2018-08-07T00:43:41.580Z","path":"2018/08/07/python爬取豆瓣电影Top250/","text":"在学习了python基础后，一心想着快速入门爬虫，因为我就是为爬虫而学的python,所以就找了这个豆瓣电影来爬取。好了，废话不多说，进入正题1.找到网页并分析网页结构首先进入豆瓣电影Top250这个网页，按下f12打开开发者工具，如下图然后开始分析网页，点击开发者工具左上角的有个箭头的东西去找你需要找的数据，在这里我发现每个电影的信息都是在&lt;li&gt;的标签内，所以可以用正则表达式来先提取每一个电影，然后在分别提取每个电影中的数据。每个电影现在的数据都可以获取了，但是这个url只有25个电影，怎样获取下一页的呢？这里我们可以在每个页面获取下一页的链接，然后通过循环来继续获取下一页的电影数据即可我们可以先用开发者工具的箭头点一下后页，然后就显示右边的箭头数据出来，这里我们也可以用正则表达式来获取下一页的链接，然后接下来的工作就是循环了，好了分析结束，开始敲代码吧！2.用面向对象的方法进行爬取数据先用requests对网页进行请求，获取网页的html结构，在这里，为了防止网页的反爬虫技术，我加了个请求头（记得使用requests库之前先导入，没有的可以在命令行通过 pip install requests 进行下载）请求头在开发者工具中查看，如下图所示接下用正则表达式进行获取数据先匹配每一个电影和每一页数据（使用正则表达式的库是re）接下来获取每个电影的数据注意：获取到上面的数据，有的是空的，所以还需要进行判断是否为空，为了好看，我用了三元表达式进行判断，完成之后把他们存入字典接下来就是进行循环取下一页的数据了3.如果你有点数据库基础的话，还可以把他们存入数据库，在这里我把这些数据存入MySQL数据库，代码如下，需要自己先建好数据库好表格这是操作数据库的类（使用的库为pymysql）然后回到爬虫类进行把数据存入数据库4.成功后你就会在数据库中查到以下数据END最后，非常感谢你看完了这篇文章，喜欢的话，或者有什么问题的话欢迎去我的微信公众号日常学python后台回复我，我会认真回答的。ps:如果需要完整代码的话可以在微信公众号日常学python后台回复top250即可，或者想要什么学习资源也可以后台找我哦","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"python爬虫常用库之requests详解","date":"2018-08-07T00:43:41.532Z","path":"2018/08/07/python爬虫常用库之requests详解/","text":"在使用了urllib库之后，感觉很麻烦，比如获取个cookie都需要分几步，代码又多，这和python的风格好像有点不太像哈，那有没有更加容易点的请求库呢？答案是有的，那就是第三方库requests,这个库的作者是大名鼎鼎的kennethreitz，创作这个库的原因就是想让python开发者更加容易地发起请求，处理请求。里面还有个名字：HTTP for Humans,顾名思义，就是用来请求http的。想看源代码的可以在github上搜索他的名字就可以看到了。接下来介绍下怎样用这个库吧！因为这是第三方库，所以我们需要下载，需要在命令行输入pip install requests如果你装的是anacoda可以忽略这条安装好了就来进行使用吧1进行简单的操作发送一个get请求# 发送请求import requestsresponse = requests.get(‘http://httpbin.org/get&#39;)# 获取返回的html信息print(response.text)这样就发送了一个get请求，并且还打印了返回的内容，这个不再需要知道网页是哪个编码的，不过有时会出现编码问题，但是你也可以指定编码类型，如：response.encoding = ‘utf-8’指定完成后就可以正常编码了，前提你得知道网页的编码类型。出了上面这些，我们还可以获取下面的信息print(response.headers)# 请求状态码print(response.status_code)# 获取网页的二进制内容print(response.content)print(response.url) # 获取请求的urlprint(response.cookies) # 获取cookie是不是觉得很容易，一行代码就可以了。不再需要几步代码什么的了。接下来被发爬的话带上个请求头来进行请求# 还可以添加请求头进行请求headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36’}response = requests.get(‘http://httpbin.org/get’, headers=headers )print(response.headers)print(response.text)加个请求头也就是加个关键字参数而已还可以进行带参数的get请求# 进行带参数的get请求data = {‘name’: ‘june’, ‘password’: 123456}response = requests.get(‘http://httpbin.org/get&#39;, params=data)print(response.text)那如果需要登陆呢？post请求怎样发？告诉你，照样很简单# 进行post请求data = {‘name’: ‘june’, ‘password’: 123456}response = requests.post(‘http://httpbin.org/post&#39;, data=data, headers=headers)print(response.text)是不是很简单，也是加个data关键字参数，把要提交的登陆参数进行post上去。那除了上面的两个请求，还能进行别的请求吗？我可以非常开心地告诉你，可以的。比如，你要发个put请求，如这样requests.put()requests.delete()这个就是发送put请求和delete请求的，其他的请求也是这样发送，就不一 一说了。2进行复杂点的请求在登陆的时候我们有时候需要输入验证码，那怎样输呢？爬虫的看不了网页，最简单的做法就是把这个验证码的图片下载下来然后手动输入，那么我们怎样下载呢？我们可以向这个图片的url发送请求，然后把返回内容以二进制方法存入文件里面就可以了。代码如下：# 从网上读取二进制数据，比如图片response = requests.get(‘https://www.baidu.com/img/bd_logo1.png&#39;, headers=headers)# 这个是直接获取字节码，这个是要保存的文件print(response.content)# 这个是获取解码后的返回内容，这个是乱码print(response.text)# 用文件来把图片下载下来with open(‘baidu.png’, ‘wb’) as f: # 注意写的方式是以二进制方式写入 f.write(response.content) print(‘下载完毕’)还是很简单，不得不说，这个库太好用了。当我们需要上传文件的时候，比如图片，我们还可以用post方法把他发送出去# 上传文件files = {‘picture’: open(‘baidu.png’, ‘rb’)}response = requests.post(‘http://httpbin.org/post&#39;, files=files)print(response.text)获取cookie并简单处理一下# 获取cookieresponse = requests.get(‘https://www.baidu.com&#39;)for k, v in response.cookies.items(): print(k, ‘=’, v)当网页返回内容是json格式是，我们不需要用json库来解析，我们可以直接利用requests的方法进行解析，两者的效果是一样的# 解析jsonj = response.json() # 可以用json库来解析，结果一样在urllib库时保存登陆信息需要把cookie保存下来，但是在requests库里面，我们只需要用requests.session()来保存信息就可以了。# 用会话来保持登陆信息session = requests.session()response = session.get(‘http://httpbin.org/cookies/set/number/123456&#39;)print(response.text)这样就可以保存登陆了，不需要为cookie操心了，但是每次获取一个session就可以了，然后用来请求或者其他操作。不需要每次请求或者操作都创建一个sesion出来，这样是保存不了登陆信息的当一个网站不安全，需要你用证书验证的，比如这个网站https://www.12306.cn这时要访问里面的网站内容，我们就需要进行验证，代码如下# 证书验证response = requests.get(‘https://www.12306.cn&#39;, verify=False) # 不加这个关键字参数的话会出现验证错误问题，因为这个网站的协议不被信任这样就可以进行访问了，但是会有一条警告E:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning)觉得不美观的，我们还可以在请求时加个cert关键字参数，值为可信任的证书，为一个元组，写上账号和密码之类的，这里就不演示了遇到需要认证的网站，我们也可以这样from requests.auth import HTTPBasicAuth# 设置认证# requests.get(‘需要认证的网址’, auth=HTTPBasicAuth(‘user’, ‘passwd’)) # 由于找不到需要认证的网址，所以先写个主体# 还可以这样认证# requests.get(‘需要认证的网址’, auth=(‘user’, ‘passwd’)) # 这样就简单点由于我找不到需要认证的网站，所以就不演示了。requests还可以用代理ip来进行请求网站来防止ip被封以至于自己爬不了的尴尬。使用代理ip也比urllib库简单得多，代码如下：# 设置代理proxies = {‘http’: ‘http://122.114.31.177:808&#39;, ‘https’: ‘https://119.28.223.103:8088&#39;}# 在请求时添加上列代理response = requests.get(‘http://httpbin.org/get&#39;, proxies=proxies)print(response.text)上面的字典格式需要一 一对应，然后在请求时加上个关键字参数proxies就可以了。3请求异常处理在程序运行时，遇到错误时程序就会被强行停止，如果想要继续运行，就需要进行捕捉异常来让程序继续运行。在requests库中有个处理异常的库requests.exceptions这里简单地处理下请求超时的处理情况import requestsfrom requests.exceptions import ReadTimeout, ConnectTimeout, HTTPError, ConnectionError, RequestException# 捕捉异常try: response = requests.get(‘http://httpbin.org/get&#39;, timeout=0.1) # 规定时间内未响应就抛出异常 print(response.text)except ReadTimeout as e: print(‘请求超时’)except ConnectionError as e: print(‘连接失败’)except RequestException as e: print(‘请求失败’)这里捕捉了三个异常，因为ReadTimeout是ConnectionError的子类，所以先捕捉ReadTimeout,再捕捉父类的。而ConnectionError 和 RequestException 同理更多的异常处理可以查看文档哈。4最后以上均是我在学习时的笔记和个人在运用时遇到的一些坑都简单地记载了上去，希望对你有用哈，如果想看更多的用法可以去官方文档查看。还有代码我放在了github上，要的话可以上去查看。GitHub：https://github.com/SergioJune/gongzhonghao_code/tree/master/python3_spider官方文档：http://docs.python-requests.org/zh_CN/latest/index.html学习参考资料：https://edu.hellobi.com/course/157","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requests","slug":"requests","permalink":"http://sergiojune.com/tags/requests/"}]},{"title":"python爬虫常用库之BeautifulSoup详解","date":"2018-08-07T00:43:41.510Z","path":"2018/08/07/python爬虫常用库之BeautifulSoup详解/","text":"这是日常学python的第16篇原创文章经过了前面几篇文章的学习，估计你已经会爬不少中小型网站了。但是有人说，前面的正则很难唉，学不好。正则的确很难，有人说过：如果一个问题用正则解决，那么就变成了两个问题。所以说学不会是很正常的，不怕，除了正则，我们还可以用另外一个强大的库来解析html。所以，今天的主题就是来学习这个强大的库–BeautifulSoup，不过正则还是需要多多练习下的。因为是第三方库所以我们需要下载，在命令行敲下以下代码进行下载pip install beautifulsoup4安装第三方解析库pip install lxmlpip install html5lib如果不知道有什么用请往下看1.相关解析库的介绍这里官方推荐解析库为lxml,因为它的效率高。下面都是用lxml解析库来进行解析的。2.详细语法介绍本文是进行解析豆瓣图书首页https://book.douban.com/1)创建bs对象from bs4 import BeautifulSoupimport requestsresponse = requests.get(‘https://book.douban.com/&#39;).text# print(response)# 创建bs对象soup = BeautifulSoup(response, ‘lxml’) &nbsp;# 使用到了lxml解析库2)获取相关标签标签：&lt;a data-moreurl-dict=‘{“from”:”top-nav-click-main”,”uid”:”0”}’ href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;上面的a就是一个标签名字，最简单的就是&lt;a&gt;&lt;/a&gt;这样，可以简单理解为&lt;&gt;里面的第一个单词就是标签名# 获取标签print(soup.li) &nbsp;# 这个只是获取第一个li标签# 结果&lt;li class=””&gt;&lt;a data-moreurl-dict=’{“from“:“top-nav-click-main”,“uid”:“0”}‘ href=”https://www.douban.com&quot; target=”_blank”&gt;豆瓣&lt;/a&gt;&lt;/li&gt;3)获取标签的名字和内容标签的名字和内容：&lt;a &gt;豆瓣&lt;/a&gt;如上面所说，a就是标签名字，而两个标签之中所夹杂的内容就是我们所说的内容，如上，豆瓣就是该标签的内容# 获取标签名字print(soup.li.name)# 获取标签内容print(soup.li.string) &nbsp;# 这个只能是这个标签没有子标签才能正确获取，否则会返回None# 结果liNone由于这个li标签里面还有个子标签，所以它的文本内容为None下面这个就可以获取它的文本内容# 获取标签内的标签print(soup.li.a)print(soup.li.a.string) &nbsp;# 这个标签没有子标签所以可以获取到内容# 结果&lt;a data-moreurl-dict=‘{“from”:”top-nav-click-main”,”uid”:”0”}’ href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;豆瓣4)获取标签属性，有两种方法标签属性：&lt;a href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;可以简单理解为属性就是在标签名字旁边而且在前一个&lt;&gt;符号里面的，还有是有等号来进行体现的。所以上面的href就是标签属性名字，等号右边的就是属性的值，上面的值是个网址# 获取标签属性print(soup.li.a[‘href’]) &nbsp;# 第一种print(soup.li.a.attrs[‘href’]) &nbsp;# 第二种# 结果https://www.douban.comhttps://www.douban.com5)获取标签内的子标签子标签：&lt;li&gt;&lt;a&gt;豆瓣&lt;/a&gt;&lt;/li&gt;比如我们现在获取的li标签，所以a标签就是li标签的子标签# 获取标签内的标签print(soup.li.a)# 结果&lt;a data-moreurl-dict=‘{“from”:”top-nav-click-main”,”uid”:”0”}’ href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;6)获取所有子节点子节点：这个和子标签是差不多的，只不过这里是获取一个标签下的所有子标签，上面的只是获取最接近该标签的子标签# 获取子节点print(soup.div.contents) &nbsp;# 返回一个列表 第一种方法for n, tag in enumerate(soup.div.contents): &nbsp; &nbsp;print(n, tag)# 结果[‘\\n’, &lt;div class=”bd“&gt;&lt;div class=”top-nav-info“&gt;&lt;a class=”nav-login“ href=”https://www.douban.com/accounts/login?source=book“ rel=”nofollow“&gt;登录&lt;/a&gt;…0 1 &lt;div class=”bd“&gt;&lt;div class=”top-nav-info“&gt;…这个是获取div下的所有子节点，.content就是获取子节点的属性7)第二种方法获取所有子节点# 第二种方法print(soup.div.children) &nbsp;# 返回的是一个迭代器for n, tag in enumerate(soup.div.children): &nbsp; &nbsp;print(n, tag)这个是用.children获取所有的子节点，这个方法返回的是一个迭代器8)获取标签的子孙节点，就是所有后代子孙节点：&lt;ul&gt;&lt;li&gt;&lt;a&gt;豆瓣&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;从上面知道，li标签是ul标签的子标签，a标签是li标签的子标签，若此时我们获取的是ul标签，所以li标签和a标签都是ul标签的子孙节点# 获取标签的子孙节点print(soup.div.descendants) &nbsp;# 返回的是一个迭代器for n, tag in enumerate(soup.div.descendants): &nbsp; &nbsp;print(n, tag)# 结果…&lt;generator object descendants at 0x00000212C1A1E308&gt;0 1 &lt;div class=”bd“&gt;&lt;div class=”top-nav-info“&gt;&lt;a class=”nav-login“ href=”https://www.douban.com/accounts/login?source=book“ rel=”nofollow“&gt;登录&lt;/a&gt;…这里用到了.descendants属性，获取的是div标签的子孙节点，而且返回结果是一个迭代器9)获取父节点和所有祖先节点既然有了子节点和子孙节点，反过来也是有父节点和祖先节点的，所以都很容易理解的# 获取父节点print(soup.li.parent) &nbsp;# 返回整个父节点# 获取祖先节点print(soup.li.parents) &nbsp;# 返回的是一个生成器for n, tag in enumerate(soup.li.parents): &nbsp; &nbsp;print(n, tag).parent属性是获取父节点，返回来的是整个父节点，里面包含该子节点。.parents就是获取所有的祖先节点，返回的是一个生成器10)获取兄弟节点兄弟节点：&lt;ul&gt;&lt;li&gt;&lt;a&gt;豆瓣1&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a&gt;豆瓣2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a&gt;豆瓣3&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;比如上面的html代码，里面的li标签都是ul标签的子节点，而li标签都是处于同级的，所以上面的li标签都是各自的兄弟。这就是兄弟节点。# 获取兄弟节点print(soup.li.next_siblings) &nbsp;# 获取该标签的所有同级节点，不包括本身 &nbsp;返回的是一个生成器for x in soup.li.next_siblings: &nbsp; &nbsp;print(x)# 结果&lt;generator object next_siblings at 0x000002A04501F308&gt;&lt;li class=”on“&gt;&lt;a data-moreurl-dict=’{“from“:“top-nav-click-book”,“uid”:“0”}‘ href=”https://book.douban.com&quot;&gt;读书&lt;/a&gt;&lt;/li&gt;….next_siblings属性是获取该标签的所有在他后面的兄弟节点，不包括他本身。同时返回结果也是一个迭代器同理，既然有获取他的下一个所有兄弟标签，也有获取他前面的所有兄弟标签soup.li.previous_siblings如果只是获取一个即可，可以选择把上面的属性后面的s字母去掉即可，如下soup.li.previous_sibling &nbsp;# 获取前一个兄弟节点soup.li.next_sibling &nbsp;# 获取后一个兄弟节点3.bs库的更高级的用法在前面我们可以获取标签的名字、属性、内容和所有的祖孙标签。但是当我们需要获取任意一个指定属性的标签还是有点困难的，所以，此时有了下面这个方法：soup.find_all( name , attrs , recursive , text , **kwargs )name：需要获取的标签名attrs：接收一个字典，为属性的键值，或者直接用关键字参数来替代也可以，下面recursive：设置是否搜索直接子节点text：对应的字符串内容limit：设置搜索的数量1)先使用name参数来进行搜索# 先使用name参数print(soup.find_all(‘li’)) &nbsp;# 返回一个列表，所有的li标签名字# 结果[&lt;li class=””&gt;&lt;a data-moreurl-dict=’{“from“:“top-nav-click-main”,“uid”:“0”}‘ href=”https://www.douban.com&quot; target=”_blank”&gt;豆瓣&lt;/a&gt;&lt;/li&gt;, &lt;li class=”on”&gt;…这里获取了所有标签名字为li的标签2)使用name和attrs参数# 使用name和attrs参数print(soup.find_all(‘div’, {‘class’: ‘more-meta’})) &nbsp;# 这个对上个进行了筛选,属性参数填的是一个字典类型的# 结果[&lt;div class=”more-meta“&gt;&lt;h4 class=”title“&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;刺 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;/h4&gt;…这里搜索了具有属性为class=’more-meta’的div标签3)根据关键字参数来搜索# 对相关属性进行进行查找也可以这样print(soup.find_all(class_=‘more-meta’)) &nbsp;# 使用关键字参数，因为class是python关键字，所以关键字参数时需要加多一个下划线来进行区别# 结果和上面的结果一样…这里注意，我们找的是class属性为more-meta的标签，用了关键字参数，但是python里面有class关键字，所以为了不使语法出错，所以需要在class加个下划线其他参数的就不再介绍了，可以自行去官网查看4)find()方法此方法与find_all()方法一样，只不过这个方法只是查找一个标签而已，后者是查找所有符合条件的标签。还有很多类似的方法，用法都差不多，就不再一一演示了，需要的可以去官网查看5)select()方法这个方法是使用css选择器来进行筛选标签的。css选择器：就是根据标签的名字，id和class属性来选择标签。通过标签名：直接写该标签名，如&nbsp;li a&nbsp; ,这个就是找li标签下的a标签通过class属性：用. 符号加class属性值，如&nbsp;.title .time&nbsp;这个就是找class值为title下的class值为time的标签通过id属性：用# 加id属性值来进行查找，如&nbsp;#img #width&nbsp;这个就是找id值为img下的id值为width的标签上面三者可以混合使用，如&nbsp;ul .title #width如果还不太会的话，可以直接在浏览器上按下f12来查看位置在箭头所指的位置就是选择器的表达代码如下# 还可以用标签选择器来进行筛选元素, 返回的都是一个列表print(soup.select(‘ul li div’)) &nbsp;# 这个是根据标签名进行筛选print(soup.select(‘.info .title’)) &nbsp;# 这个是根据class来进行筛选print(soup.select(‘#footer #icp’)) &nbsp;# 这个是根据id来进行筛选# 上面的可以进行混合使用print(soup.select(‘ul li .cover a img’))这里的获取属性和文本内容# 获取属性for attr in soup.select(‘ul li .cover a img’): &nbsp; &nbsp;# print(attr.attrs[‘alt’]) &nbsp; &nbsp;# 也可以这样 &nbsp; &nbsp;print(attr[‘alt’])# 获取标签的内容for tag in soup.select(‘li’): &nbsp; &nbsp;print(tag.get_text()) &nbsp;# 里面可以包含子标签，会将子标签的内容连同输出.get_tex()方法和前面的.string属性有点不一样哈，这里的他会获取该标签的所有文本内容，不管有没有子标签写在最后以上的这些都是个人在学习过程中做的一点笔记。还有点不足，如果有错误的话欢迎大佬指出哈。如果想要查看更多相关用法可以去官方文档查看：http://beautifulsoup.readthedocs.io/zh_CN/latest/学习参考资料：https://edu.hellobi.com/course/157如果这篇文章对你有用，点个赞，转个发如何？还有，祝大家今天愚人节快乐MORE延伸阅读◐◑爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;老司机带你用python来爬取妹子图日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requests","slug":"requests","permalink":"http://sergiojune.com/tags/requests/"}]},{"title":"谈谈今晚以及在跑步时的一些随想","date":"2018-05-24T14:11:38.000Z","path":"2018/05/24/谈谈今晚以及在跑步时的一些随想/","text":"今天可以算是我的第一次在讲台上分享我的技术，准备了两天左右吧，其实也没那么长，就是做了个ppt而已，由于第一次做，有很多地方都没有做好，比如字体太小了，背景和字体颜色混合了导致看不清字等等此类的问题。如果下次再做的话一定要做得好一点。还有就是我的分享有点垃圾哈，讲的内容自我感觉不是很好，也不知道需要讲什么地方，以后还是多观察下别人怎么分享吧。这次需要表扬自己的是上讲台讲台没有预料中的那么紧张，虽然讲话不是很自如，不过也比以前好多了，希望下次能更进一步吧！加油！ 说说跑步的感想！这两星期我经常跑步，跑完步还会做50个俯卧撑，虽然不太标准，不过也好过没有做吧。在跑步时会跑多几圈就会很累，就想放弃，不过我最后还是咬牙就挺着过去了。这两天跑步呢，我就想，追逐梦想就和跑步一样吧。就比如我把梦想都分成很多个小目标，跑步的圈数或者步数就是这些小小的目标。在刚开始的时候，你跑步会很顺利，小目标也是这样子，刚开始的小目标也是很容易就可以实现的。可是当你跑步的步数多了，也就是小目标实现多了，之后的每一步或每一个小目标都会显得举步维艰，这时候就需要你自己的意志力去支持着自己继续跑下去。所以说梦想也需要用自己的一生去实现，后面可能会越来越难，可你每实现的一个小目标都会把很多人甩在身后面，这些人就相当于你在跑步过程中的路人。 人生是一场永无止尽的跑步！还有这几天会经常不知道干嘛，然而还有很多事情没做，先把这些事列下来吧！线性代数课程补上 离散数学课程及作业补上 高等数学课程补上 scrapy的学习 马蜂窝网站的全站抓取 淘宝商品抓取 亚马逊模拟登陆 爬虫面试题 看 图解http 看 网络是怎样连接 了解下各种排序 不列出来都不知道有这么多事情没做，以后觉得没事的时候可以找这些事情做了。 写完，睡觉！晚安！","tags":[]},{"title":"python爬虫常用库之urllib详解","date":"2018-03-11T16:00:19.000Z","path":"2018/03/12/python爬虫常用库之urllib详解/","text":"以下为个人在学习过程中做的笔记总结之爬虫常用库urlliburlib库为python3的HTTP内置请求库urilib的四个模块：urllib.request:用于获取网页的响应内容urllib.error:异常处理模块，用于处理异常的模块urllib.parse:用于解析urlurllib.robotparse:用于解析robots.txt，主要用于看哪些网站不能进行爬取，不过少用1urllib.requesturllib.request.urlopen(url,data=None,[timeout,],cafile=None,cadefault=False,context=None)url:为请求网址data:请求时需要发送的参数timeout:超时设置，在该时间范围内返回请求内容就不会报错示例代码： 1from urllib import request 2 3# 请求获取网页返回内容 4response = request.urlopen(‘https://movie.douban.com/&#39;) 5# 获取网页返回内容 6print(response.read().decode(‘utf-8’)) 7# 获取状态码 8print(response.status) 9# 获取请求头10print(response.getheaders())1# 对请求头进行遍历2for k, v in response.getheaders():3 print(k, ‘=’, v)可以使用上面的代码对一些网站进行请求了，但是当需要一些反爬网站时，这就不行了，这时我们需要适当地增加请求头进行请求，这时就需要使用复杂一点的代码了，这时我们需要用到Request对象代码示例：1# 请求头2headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0’}3requests = request.Request(‘https://movie.douban.com/&#39;, headers=headers) # 加入自己的请求头更加接近浏览器4# 进行请求,把Request对象传入urlopen参数中5response = request.urlopen(requests)6print(response.read().decode(‘utf-8’))这个我添加了请求头进行请求，使我发送的请求更加接近浏览器的行为。可以对应一些反爬网站了如果网站需要进行登陆，这时需要用到post方法，用上面的也是可以的。代码如下： 1from urllib import request, parse 2# 使用post方法来进行模拟登陆豆瓣 3data = {‘source’: ‘None’, 4 ‘redir’: ‘https://www.douban.com/&#39;, 5 ‘form_email’: ‘user’, 6 ‘form_password’: ‘passwd’, 7 ‘remember’: ‘on’, 8 ‘login’: ‘登录’} 9# 将data的字典类型转换为get请求方式10data = bytes(parse.urlencode(data), encoding=‘utf-8’)11requests = request.Request(‘https://accounts.douban.com/login&#39;, headers=headers, data=data, method=‘POST’)12response = request.urlopen(requests)13print(response.read().decode(‘utf-8’))这里我用到了data的参数把登陆需要的参数传进去，还加了个请求方法Methodparse.urlencode()后面有讲这里还有另外一种添加请求头的方法Request.add_header(): 参数有两个，分别为请求头对应的键和值，这种方法一次只能添加一个请求头，添加多个需要用到循环或者直接用前面的方法添加多个请求头在登陆了网站之后，我们需要用到cookie来保存登陆信息，这时就需要获取cookie了。urllib获取cookie比较麻烦。代码示例如下： 1from http import cookiejar 2# 获取cookie 3cookie = cookiejar.CookieJar() 4# 获取助手把cookie传进去 5handler = request.HTTPCookieProcessor(cookie) 6# 获取opener进行请求网站 7opener = request.build_opener(handler) 8# 请求网页 9response = opener.open(‘https://movie.douban.com/&#39;)10# 打印cookie11for c in cookie:12 print(c.name, ‘=’, c.value)单纯地打印没什么用，我们需要把他存入文件来保存，下次使用时再次加载cookie来登陆保存cookie为文件：1from http import cookiejar2# 将cookie保存在文件中3filename = ‘cookie.txt’4cookie = cookiejar.MozillaCookieJar(filename) # 表示使用Mozilla的cookie方式存储和读取5handler = request.HTTPCookieProcessor(cookie)6opener = request.build_opener(handler)7opener.open(‘https://movie.douban.com/&#39;)8# 保存文件9cookie.save(ignore_discard=True, ignore_expires=True)另一种保存方法：1from http import cookiejar2cookie = cookiejar.LWPCookieJar(filename) # 表示 Set-Cookie3 文件格式存储和读取3handler = request.HTTPCookieProcessor(cookie)4opener = request.build_opener(handler)5opener.open(‘https://movie.douban.com/&#39;)6# 保存文件7cookie.save(ignore_discard=True, ignore_expires=True)这两种保存格式都是不一样的，需要保存的内容一样。保存可以了，这时就需要用到加载了，当然也可以。代码如下：1from http import cookiejar2# 从cookie文件加载到网页上实现记住登陆3cookie = cookiejar.LWPCookieJar()4# 加载文件5cookie.load(filename, ignore_discard=True, ignore_expires=True)6handler = request.HTTPCookieProcessor(cookie)7opener = request.build_opener(handler)8opener.open(‘https://movie.douban.com/&#39;)这样就可以实现不用密码进行登陆了。cookie小总结：在操作cookie时，都是分五步，如下：进行导包，至关重要的一步，不导包直接出错。获取cookie处理对象，使用cookiejar包创建cookie处理器，使用request.HTTPCookieJarProcessor()利用cookie处理器构建opener，使用request.build_opener()进行请求网站，用opener.open(),这个不能用request.urlopen()如果有时你在同一ip连续多次发送请求，会有被封ip的可能，这时我们还需要用到代理ip进行爬取，代码如下：1proxy = request.ProxyHandler({2 ‘https’: ‘https://106.60.34.111:80&#39;3})4opener = request.build_opener(proxy)5opener.open(‘https://movie.douban.com/&#39;, timeout=1)可以看到越复杂的请求都需要用到request.build_opener(),这个方法有点重要，请记住哈2urllib.error将上面的使用代理ip的请求进行异常处理，如下： 1from urllib import request, error 2try: 3 proxy = request.ProxyHandler({ 4 ‘https’: ‘https://106.60.34.111:80&#39; 5 }) 6 opener = request.build_opener(proxy) 7 opener.open(‘https://movie.douban.com/&#39;, timeout=1) 8except error.HTTPError as e: 9 print(e.reason(), e.code(), e.headers())10except error.URLError as e:11 print(e.reason)因为有时这个ip或许也被封了，有可能会抛出异常，所以我们为了让程序运行下去进而进行捕捉程序error.URLError: 这个是url的一些问题，这个异常只有一个reason属性error.HTTPError:这个是error.URLError的子类，所以在与上面的混合使用时需要将这个异常放到前面，这个异常是一些请求错误，有三个方法，.reason(), .code(), .headers(),所以在捕捉异常时通常先使用这个3urllib.parse解析url:urllib.parse.urlparse(url, scheme=’’, allow_fragments=True)简单的使用：1from urllib import request, parse2# 解析url3print(parse.urlparse(‘https://movie.douban.com/&#39;))4print(parse.urlparse(‘https://movie.douban.com/&#39;, scheme=‘http’))5print(parse.urlparse(‘movie.douban.com/‘, scheme=‘http’))6# 下面是结果7ParseResult(scheme=‘https’, netloc=‘movie.douban.com’, path=‘/‘, params=‘’, query=‘’, fragment=‘’)8ParseResult(scheme=‘https’, netloc=‘movie.douban.com’, path=‘/‘, params=‘’, query=‘’, fragment=‘’)9ParseResult(scheme=‘http’, netloc=‘’, path=‘movie.douban.com/‘, params=‘’, query=‘’, fragment=‘’)可以看出加了scheme参数和没加的返回结果是有区别的。而当scheme协议加了，而前面的url也包含协议，一般会忽略后面的scheme参数既然后解析url，那当然也有反解析url，就是把元素串连成一个url1from urllib import parse2# 将列表元素拼接成url3url = [‘http’, ‘www’, ‘baidu’, ‘com’, ‘dfdf’, ‘eddffa’] # 这里至少需要6个元素（我乱写的，请忽视）4print(parse.urlunparse(url))5# 下面是结果6http://www/baidu;com?dfdf#eddffaurlparse()接收一个列表的参数，而且列表的长度是有要求的，是必须六个参数以上，要不会抛出异常1Traceback (most recent call last):2 File “E:/anaconda/python_project/python3_spider/urllib_test.py”, line 107, in &lt;module&gt;3 print(parse.urlunparse(url))4 File “E:\\anaconda\\lib\\urllib\\parse.py”, line 454, in urlunparse5 _coerce_args(components))6ValueError: not enough values to unpack (expected 7, got 6)urllib.parse.urljoin():这个是将第二个参数的url缺少的部分用第一个参数的url补齐1# 连接两个参数的url, 将第二个参数中缺的部分用第一个参数的补齐2print(parse.urljoin(‘https://movie.douban.com/&#39;, ‘index’))3print(parse.urljoin(‘https://movie.douban.com/&#39;, ‘https://accounts.douban.com/login&#39;))4# 下面是结果5https://movie.douban.com/index6https://accounts.douban.com/loginurllib.parse.urlencode():这个方法是将字典类型的参数转为请求为get方式的字符串1data = {‘name’: ‘sergiojuue’, ‘sex’: ‘boy’}2data = parse.urlencode(data)3print(‘https://accounts.douban.com/login&#39;+data)4# 下面是结果5https://accounts.douban.com/loginname=sergiojuue&amp;sex=boy4结语还有个urllib.robotparse库少用，就不说了，留给以后需要再去查文档吧。上面的只是我在学习过程中的总结，如果有什么错误的话，欢迎在留言区指出，还有就是需要查看更多用法的请查看文档https://docs.python.org/3/library/urllib.html需要代码的可以去我的github上面fork，给个star也行！github:https://github.com/SergioJune/gongzhonghao_code/blob/master/python3_spider/urllib_test.py 学习过程中看的大多是崔庆才大佬的视屏：https://edu.hellobi.com/course/157日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"urllib","slug":"urllib","permalink":"http://sergiojune.com/tags/urllib/"}]},{"title":"2017年终总结","date":"2017-12-31T14:11:38.000Z","path":"2017/12/31/2017年终总结/","text":"今天是2017的最后一天了，对于2017，是我至今为止最重要的一年，在这一年中经历了人生的第一场高考，很高兴，这也是最后一场。而在下半年，我也进入了大学，虽然大学不是理想中的大学，但是我也很高兴，因为终于可以不再像高中那样了，在这里我可以学习我想要学的东西！ 上半年那时我还在准备高考，每天沉迷于题海之中，那段在一中的时间，是我高中过得最充实的时间，每天早上六点起床，晚上奋斗到差不多一点才睡。但是高三一年的勤奋拉近不了我在高中挥霍了两年的时间所带来的与别人的距离，所以在每次模拟试下来之后都在想，为啥前两年不学多一点呢？这样子在高三就不会这么辛苦，也就不会有这么大的坑让给高三的一年来填。但是我也不会后悔，因为在那两年里，我学会了踢球，懂得了人生有太多的意外，比如我在高二那年的足球班赛，我也认识了一大堆志同道合的人。所以说，对自己走过的路不曾后悔。在这半年，我度过了第一个百日誓师，还记得那天康亚华校长的谈话，还记得我们跑步的激情，还记得那天之后对未来的憧憬。。。 百日誓师，壮观！！！ 下面是我们班在百日誓师的照片 最后放张我们学校的照片吧，离开后才发现学校是真的美！ 最后六月份，走上了考场，离开了学校，最终的结果也对得起我在高三奋斗的一年！ 下半年下半年我走进了大学，大学虽然没有想象的大和好，但这始终会是我在余下四年继续坚持我奋斗的地方，所以我也爱上了这个地方。 看！ 我的大学还是很美的。还有我的班，每个都很帅很漂亮，有木有！！！在这大学半年里，我学了很多编程知识，比如c语言，python，html+css+javascript和java，学了这么多，都是因为去尝试吧，因为我暂时不知道我要往哪个方向发展，现在我知道了，我准备往爬虫，分析大数据方向，或许还有人工智能，还有就是以python为主的前端在学了这么多，自己模仿了两个网页，也做了一个简单的查看新闻app，总之还没有深入，准备下一年深入学习。不过在学习学校的课程是失败的，很多课都没有认真学习，英语也没有学好，觉得蛮遗憾的，浪费了这么多时间。还有，我也根据了网上的教程自己搭建了一个属于自己的博客，也挺不错的。而在自己博客定的小目标也没有完成多少，可能要完成的东西太多了，所以也没有完成，下一次就知道应该定多少了，嘻嘻！我在最后的一个月还参加了一个打卡活动，里面的早起和坚持锻炼我基本上算是坚持下来了，算是比较欣慰了。 引用知识星球人里一段话 人生那么长，做着枯燥而无意义的工作，每时每刻都是煎熬。 人生那么短，如果每一步成长都是令人激动的，那你一定不会说:”后悔”。 尝试，不断试错，不断碰壁，不断重来，你会发现人生有那么多的可能。会发现世界上有那么美丽的事情让我们满心欢喜。 献给现在迷茫，未来成功的你。 不忘初心，方得始终。2018，我来了！！！","tags":[{"name":"hard","slug":"hard","permalink":"http://sergiojune.com/tags/hard/"}]},{"title":"html表格和表单","date":"2017-11-29T21:07:33.000Z","path":"2017/11/30/html表格和表单/","text":"HTML表格 表格的组成标签是table，tr，和td，还有一个th标签，th标签和td标签的作用一样，都是用来表示表格中的列，只是th标签多了文字加粗和文字居中样式 table是表格的标记，tr是代表表格中的行，td是表格的行，格式为： &lt;table&gt; &lt;tr&gt;&lt;!--表格的行--&gt; &lt;td&gt;&lt;/td&gt;&lt;!--表格的列--&gt; &lt;/tr&gt; &lt;/table&gt; caption标签是表格的标题，每个表格只能有一个标题 当表结构较复杂时，可以使用表结构来进行优化，就是将表格分为三部分，分别为表头，主体内容和脚注 1.thead标签：这个是表头标签，双标记 2.tbody标签：这个是表主体内容的标签，双标记 3.tfoot：这个是脚注标签，表尾部内容，双标记 表格的属性： 1.width：表格的宽度，单位可以是px或者百分比，height这个高度属性也和他差不多 2.align：表格相对周围元素的对齐方式，有left，right，center三个值可选 3.border;表格边框的宽度，单位是px，不设置时表格没有边框显示 4.BGcolor：表格的背景颜色，可以用rgb（x，x，x）或十六进制或颜色名来写值 5.cellpadding：单元格边缘和他内容之间的宽度，单位是像素和百分比 6.cellspacing：单元格之间的空白距离，单位是像素和百分比 7.frame：规定外侧边框可不可见，有void，above，below，hsides，vsides，lhs，rhs，box和border可选 8.rules：规定内侧边框可不可见，有none，groups，rows，cols和all可选 td列标签的两个重要属性： 1.colspan：跨列属性 2.rowspan：跨行属性 ###用这个表格标签做了个表格 步骤： 1.先把这个表格的整体部分写出来，把表格先分成5行5列； 2.把文字的居中，用cellpadding属性，把值改成0就可以； 3.使用跨行跨列属性，记得使用后要把相对应的行或列删除，因为当使用跨行或者跨列后，该行或者该列就变成两行或者两列； 4.最后把相对应的背景颜色设置就可 HTML表单 表单一般用来填写注册信息或者登陆信息 表单用Form标签，双标志 表单中的元素用input标签，单标志，通过type属性来确定表单的内容，格式为 &lt;Form&gt; &lt;input type=&quot;属性值&quot;/&gt; &lt;/Form&gt;","tags":[{"name":"HTML","slug":"HTML","permalink":"http://sergiojune.com/tags/HTML/"}]},{"title":"HTML基础","date":"2017-11-27T21:17:44.000Z","path":"2017/11/28/HTML基础/","text":"这个HTML基础笔记很早就想写的了，由于自己一直拖着，不想写，所以才留到现在写，现在我基本把HTML+css+JavaScript学完了，就等着我的笔记了。所以我决定今周一天写一篇笔记！！！好了，废话不多说了，写下今天所要写的笔记吧。 HTML是一种描述性的标记语言，并不是一种编程语言，想要装逼的小伙伴要记住了！！！ 基本语法为&lt;标记符&gt;中间这里填内容&lt;/标识符&gt;—&gt;前面的标识符为开始标志，后面的标识符为结束标志 如 &lt;p&gt;基本语法&lt;/p&gt; 记住网页源代码的效果并不是网页的最终效果 html的文档结构如下： html标签：是个双标记，就是有开始和结束标志。它标识网页的开始与结束，所有的HTML元素都要放在这个标记里面 head标签：双标记，标示网页文件的头部信息，如标题和搜索引擎等等 title标签：双标记，标识网页的标题，放在head标签里面 body标签：双标记，网页的主体部分，网页里显示的东西都放在里面 简单网页就是这样组成的 &lt;!DOCTYPE html&gt;&lt;!--这个用于声明文档类型，写在网页的第一行--&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;html网页&lt;/title&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; meta标记：用于定义文件信息，便于搜索引擎查找，放在标记内。可以设置关键字（keywords），描述（description）和作者（author）等，格式如下 1.&lt;meta name=&quot;keywords&quot; content=&quot;关键字&quot;/&gt;&lt;!--这个是设置关键字--&gt; 2.&lt;meta name=&quot;description&quot; content=&quot;描述内容&quot;/&gt;&lt;!--这个是设置描述内容--&gt; 3.&lt;meta name=&quot;author&quot; content=&quot;SergioJune&quot;/&gt;&lt;!--这个是设置作者--&gt; 4.&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html,charset=utf-8&quot;/&gt;&lt;!--这个是设置网站的编码方式--&gt; 5.&lt;meta http-equiv=&quot;1refresh&quot; content=&quot;5,URL=跳转地址&quot;/&gt;&lt;!--这个是设置网页自动跳转，由于网页问题，我防止他自动跳转，我在前面的值加了个1，要的时候记得去掉 content的第一个值为开始自动跳转的等待时间，单位为秒--&gt; html的注释： &lt;!--注释内容--&gt; pre标记：双标记，保留文本的原来格式，即文本格式原来是怎样，在网页输出也是怎样 在网页中输出特殊字符：（只是常用的几个，其他的请自行查文档） &amp;nbsp;&lt;!--表示空格--&gt; &amp;reg;&lt;!--表示商标--&gt; &amp;lt;&lt;!--表示左边尖括号--&gt; &amp;rt;&lt;!--表示右边尖括号--&gt; HTML的列表：分为三种列表，分别是有序列表，无序列表和自定义列表，这三种均放在body标签内 1.无序列表：由 ul和li标记组成 ，下面为格式和图片 &lt;ul type=&quot;circle&quot;&gt;&lt;!--无序列表--&gt; &lt;li&gt;&lt;!--每一个小项--&gt; 把C语言给学完， &lt;/li&gt; &lt;li &gt; 有空也需要把Java复习一下，要不全忘记了 &lt;/li&gt; &lt;li&gt; 每天学习两个英语音标（英语渣渣） &lt;/li&gt; &lt;li type=&quot;square&quot;&gt;&lt;!--这个单独设置了样式，所以显示图标为正方形--&gt; 有空的话可以把python学完 &lt;/li&gt; &lt;/ul&gt; 2.有序列表：由 o l和li标签组成 ，下面为格式和图片 &lt;ol type=&quot;A&quot; start=&quot;26&quot;&gt;&lt;!--有序列表,里面有样式设置，type属性是指按什么排序，start属性是从那个开始--&gt; &lt;li&gt;&lt;!~--这个是每个小项--&gt; 把C语言给学完， &lt;/li&gt; &lt;li &gt; 有空也需要把Java复习一下，要不全忘记了 &lt;/li&gt; &lt;li&gt; 每天学习两个英语音标（英语渣渣） &lt;/li&gt; &lt;li &gt; 有空的话可以把python学完 &lt;/li&gt; &lt;/ol&gt; 3.自定义列表：有 dl，dt和dd组成 ，而且这三个标签需要组合起来使用才有效 &lt;dl&gt;&lt;!--自定义列表--&gt; &lt;dt&gt;&lt;!--列表的主项--&gt; 数码 &lt;/dt&gt; &lt;dd&gt;&lt;!--该列表的仔细分类--&gt; 单反相机 &lt;/dd&gt; &lt;dd&gt; CPU &lt;/dd&gt; &lt;dd&gt; 笔记本 &lt;/dd&gt; &lt;dt&gt; 手机 &lt;/dt&gt; &lt;dd&gt; 小米 &lt;/dd&gt; &lt;dd&gt; 苹果 &lt;/dd&gt; &lt;dd&gt; 魅族 &lt;/dd&gt; &lt;/dl&gt; img标签：单标记，属性有src（必写），alt，vspace，align等属性，下面展示格式加效果图 &lt;img src=&quot;img/1.jpg&quot; align=&quot;left&quot; hspace=&quot;100&quot; height=&quot;180&quot; width=&quot;180&quot;&gt; &lt;!--src是图片的url地址值，alt是文字，当图片无法加载时显示的文字--&gt; &lt;!--hspace是距离图片的水平像素，width和height是图片的宽和高--&gt; a标签：超链接标签，双标记， 1.href属性;需要跳转的链接，可以是内部的，也可以是外部的，当该值是 # 时，表示的是空链接，就是点击无效果，用来调试的 2.target属性：表示链接窗口的属性，有_self（在本窗口跳转），_blank（在新窗口跳转），_top，_parent属性 3.title属性：链接提示性文字，当鼠标悬停时可以看到的文字 4.name属性：给链接命名，定义锚时有用 5.锚的用法和定义;使用命名锚记可以在文档中设置标记，这些标记通常放在文档的特定主题处或顶部。然后可以创建到这些命名锚记的链接，这些链接可快速将访问者带到指定位置。 创建到命名锚记的链接的过程分为两步。首先，创建命名锚记，然后创建到该命名锚记的链接。 下面给出代码展示 &lt;a href=&quot;#py2&quot;&gt;python进阶 &lt;/a&gt;&lt;!--这个就是锚导航，点击后在本网页跳转，记得格式是 #锚名 --&gt; &lt;a href=&quot;http://coding.imooc.com/class/136.html&quot; target=&quot;_blank&quot; title=&quot;点击跳转到相关课程&quot; name=&quot;py3&quot;&gt;&lt;/a&gt;&lt;!--这个是命名锚记，就是上面点击后跳转的地方--&gt; 6.它还可以发送邮件，写什么反馈邮件时可以用到，格式为： &lt;a href=&quot;mailto:邮件地址&quot;&gt;&lt;/a&gt; 7.也可以下载文件，格式为： &lt;a href=&quot;文件名&quot;&gt;&lt;/a&gt; 写了这么点东西用了我几个小时，累啊，感觉写的东西都是很基础的东西，第一次写，还不太会，希望能体谅下，以后慢慢进步！加油，明天继续！！！","tags":[{"name":"HTML","slug":"HTML","permalink":"http://sergiojune.com/tags/HTML/"}]},{"title":"学习html第一天","date":"2017-11-08T01:00:19.000Z","path":"2017/11/08/学习HTML的第一天 /","text":"其实今天不是学习HTML的第一天，算是第三天了，由于前两天有点忙，又不想写，所以一直搁置到现在，不过没事，写了总比没写好一点！这三天看的慕课网的视频，学完了html基础，但是HTML的许多元素还没有记住，等以后边写边记吧。自己跟着视频做了一个表格和一个表单，最后还模仿了一个网页，虽然有点丑，不过还是感到有点满意 这是我做的表单 这是我做的注册表单 最后是我仿的一个假网页，虽然很丑，但还是很有成就感的！嘻嘻 我也在这两天学完了简单的HTML，接下来进入css，听说学完之后做的页面会漂亮很多，蛮期待的过两天我还要把我学HTML和css所做的笔记做个总结上传上来 你看，满满的笔记（字丑ing） 好了，就码这么多了，因为实在不知道说什么了，等想到再补充吧。 感觉自己离全栈工程师又近了一步 做自己喜欢的事是世界上最幸福的人","tags":[{"name":"HTML","slug":"HTML","permalink":"http://sergiojune.com/tags/HTML/"}]},{"title":"Hello World","date":"2017-11-04T23:28:16.000Z","path":"2017/11/05/helloworld/","text":"昨天博客基本搭建完毕， 搞到10点多， 只能说身心疲惫，但是觉得都值。 终于有了自己的后花园，以后发表什么还不是我说了算吗？ 哈哈 这篇文章想了很久，几个小时，总想不出要写什么， 因为觉得自己拥有了博客， 总得发个东西装逼一下， 但细想一下，又没什么好写， 想一下文采真的烂，不过没事，谁都是这样子过来的， 以后会变好的！ 最后分享一首美国诗！ 纽约时间比加州时间早三个小时， New York is 3 hours ahead of California, 但加州时间并没有变慢。 but it does not make California slow. 有人22岁就毕业了， Someone graduated at the age of 22, 但等了五年才找到好的工作！ but waited 5 years before securing a good job! 有人25岁就当上CEO， Someone became a CEO at 25, 却在50岁去世。 and died at 50. 也有人迟到50岁才当上CEO， While another became a CEO at 50, 然后活到90岁。 and lived to 90 years. 有人依然单身， Someone is still single, 同时也有人已婚。 while someone else got married. 奥巴马55岁就退休， Obama retires at 55, 川普70岁才开始当总统。 but Trump starts at 70. 世上每个人本来就有自己的发展时区。 Absolutely everyone in this world works based on their &gt;Time Zone. 身边有些人看似走在你前面， People around you might seem to go ahead of you, 也有人看似走在你后面。 some might seem to be behind you. 但其实每个人在自己的时区有自己的步程。 But everyone is running their own RACE, in their own&gt;TIME. 不用嫉妒或嘲笑他们。 Don’t envy them or mock them. 他们都在自己的时区里，你也是！ They are in their TIME ZONE, and you are in yours! 生命就是等待正确的行动时机。 Life is about waiting for the right moment to act. 所以，放轻松。 So, RELAX. 你没有落后。 You’re not LATE. 你没有领先。 You’re not EARLY. 在命运为你安排的属于自己的时区里，一切都准时。 You are very much ON TIME, and in your TIME ZONE Destiny set up for you. 我的全栈之路开启了！！！","tags":[{"name":"run","slug":"run","permalink":"http://sergiojune.com/tags/run/"}]}]