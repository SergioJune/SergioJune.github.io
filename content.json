[{"title":"为了一窥国足输韩国之后人们的评论，我爬了懂球帝APP","date":"2019-01-17T09:45:38.189Z","path":"2019/01/17/为了一窥国足输韩国之后人们的评论，我爬了懂球帝APP/","text":"如果你是个足球迷的话，估计或多或少都会看一下昨晚中国踢韩国的比赛，因为不管他们踢得怎样，我们还是深爱着他们，那句话说得好，“国足虐我千百遍，我待国足如初恋”。更何况他们两场都踢赢了，所以面对第三场实力有点强的韩国队也是希望能赢的，毕竟我们也在十二强赛上赢过他们！ 如果你不是个足球迷，但你也可以看看，可以学习下如何去抓APP的数据。好了，废话不多说，开始抓取！ 1.抓包分析请求手机抓包我们可以用 Fiddler 软件来抓取，如果不懂怎样抓的话，可以看看这篇文章抓包软件&nbsp;Fiddler&nbsp;了解一下？ 配置好之后，开始抓包。 首先找到需要爬取的文章 懂球帝app截图 文章链接为：https://m.dongqiudi.com/article_share/896482.html 在配置好抓包之后，点击下方的评论，可以看到 评论截图 抓包截图 很容易就找到文章评论的请求，就是下面这个 可以看到请求的链接为：https://api.dongqiudi.com/v2/article/896482/comment?sort=down&amp;version=177 ， 请求方法为GET，接下来就好办了，我们再看看滑下去查看更多的评论的请求。 可以看到，向下翻页多了两个参数，不过容易知道，next 参数就是一个时间戳，而 pn 参数就是页数吧，从 0 开始的。 但是怎样判断所有评论已经爬完了呢？我们可以看看数据的详情，下面将 json 数据格式化，在下图可以看到在 data 里面有下一页的数据，那这就容易了，哈哈 分析了，接下来就是代码部分了。 2.代码部分 这是主体部分，先从第一个评论链接中爬取评论以及找出下一页的评论地址进而继续爬取。这里是把数据库存进 mongodb 中。 主要的爬取逻辑，可以看出来是比较简单的，因为没有涉及到什么加密参数之类的，但是有一个问题，每一次进行请求的时候，有时候是会返回带有相同的评论的，所以我们也需要在数据库简单地进行去重。 下面是入库和去重的代码部分 剩下的就没有了。 3.查看所得的数据由于数据分析还不熟悉，所以暂时只制作词云图。 需要先将数据写到文本上 词云图是： 可以看出，昨晚国足输一场，也被很多人喷了，但是还是有很多人是一直支持的，永远都为国足加油，里面也说到了，中国和韩国是有一定差距的，而且还有点大，输了也正常不过了，没必要喷，再说我觉得昨晚的比赛已经比第一场的比赛好很多(第二次没看)，还是有进步的，我对国足未来淘汰赛也是充满期望的，我相信能走得更远！ 下一场 踢泰国， 20号，有人看吗？","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"足球","slug":"足球","permalink":"http://sergiojune.com/tags/足球/"}]},{"title":"学习用python操作mysql","date":"2019-01-14T07:57:01.219Z","path":"2019/01/14/学习用python操作mysql/","text":"首先祝大家新年快乐哈！学生的估计明天也要上课了，工作的估计早就去上班了，我也快要上课了，哈哈，新年这段时间一直没有写过文章，一直忙于跑亲戚和学习，感觉有点对不起关注我的粉丝。所以，今天决定抽空写一篇技术文章来给大家看看，继上篇写了入门mysql之后，还没有学习如何用python来操作数据库，那我今天就带大家来学习如何用python操操作数据库。还有文末有福利，这算是给大家的新年礼物(记得点赞哦)进入正题工欲善其事，必先利其器。所以第一步，我们先下载第三方库。在这里，我用到的是pymysql库。下载库：在命令行输入1pip install pymysql下载后可检验一下是否成功下载。直接在命令行进入python然后导库即可1C:\\Users\\June&gt;python2Python 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win323Type “help”, “copyright”, “credits” or “license” for more information.4&gt;&gt;&gt; import pymysql5&gt;&gt;&gt;看到这个画面就说明下载成功了，接下来学习如何操作数据库了！！！1连接数据库1import pymysql2# 连接数据库3db = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘your password’,db=‘news’,port=3306,charset=‘utf8’)以上的参数是必填的，host: 这个是ip地址，因为我这里是本地的，所以填127.0.0.1，也可以填localhost。user：用户名，如果你也是本地的，就填root好了passwd：这个是密码，填上你自己设的密码就可以了db：这个是数据库名，我这里选的是news数据库port：这个是端口，本地的一般都是3306charset：这个是编码方式，要和你数据库的编码方式一致，要不会连接失败连接上了，怎么验证呢？这里我们可以选择查一条数据 1try: 2 db = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘your password’,db=‘news’,port=3306,charset=‘utf8’) 3 # 检验数据库是否连接成功 4 cursor = db.cursor() 5 # 这个是执行sql语句，返回的是影响的条数 6 data = cursor.execute(‘SELECT FROM new‘) 7 # 得到一条数据 8 one = cursor.fetchone() 9 print(data)10 print(one)11except pymysql.Error as e:12 print(e)13 print(‘操作数据库失败’)14finally:15 # 如果连接成功就要关闭数据库16 if db:17 db.close()代码解读：因为在连接数据库中，有时会发生连接失败等异常，所以这里就进行捕捉异常，这里的异常都是在 pymsql.Error 里面。上面的代码看不懂也没关系，因为我接下来会说，如果运行后有结果证明连接成功。在用完后，一定要记得关闭数据库连接，防止资源泄露问题。2对数据进行查询 1import pymysql 2try: 3 conn = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘password’,db=‘news’,charset=‘utf8’,port=3306) 4 # 这个是光标，用来操作数据库语句 5 cursor = conn.cursor() 6 # 执行sql语句 7 cursor.execute(‘SELECT FROM new‘) 8 print(cursor.fetchone()) 9 # 关闭光标10 cursor.close()11except pymysql.Error as e:12 print(e)13 print(‘操作数据库失败’)14finally:15 if conn:16 conn.close()代码解读：cursor():这个是光标，用来执行mysql语句的，用完后也是需要关闭的excute()：这个是执行语句，执行参数的mysql语句fetchone()：这个是查看执行语句后的一条数据fetchall()：这个是查看所有数据在查询数据后，返回的是一整条数据，有没有可以按字典形式来查询的呢？来试试！1print(cursor.fetchone()[‘name’])23Traceback (most recent call last):4 File “E:/anaconda/python_project/mysql_test/test2.py”, line 8, in &lt;module&gt;5 print(cursor.fetchone()[‘name’])6TypeError: tuple indices must be integers or slices, not str查了之后，编译器想都不想就给了我这个错误，说这是个元组，不能这样操作。虽然python没有提供，但是我们可以手动转成字典来查询啊cursor这里有个属性：description。获取的是数据库每个栏位情况，如下：1print(cursor.description)2# 下面是结果3((‘id’, 3, None, 11, 11, 0, False), (‘type’, 253, None, 5, 5, 0, False), (‘title’, 253, None, 50, 50, 0, False), (‘content’, 253, None, 2000, 2000, 0, False), (‘view_count’, 3, None, 11, 11, 0, False), (‘release_time’, 12, None, 19, 19, 0, False), (‘author’, 253, None, 20, 20, 0, True), (‘from’, 253, None, 20, 20, 0, True), (‘is_valibale’, 3, None, 11, 11, 0, False)所以，我们利用这个属性手动生成字典1# 将一条数据转成字典方便查找2new = dict(zip([x[0] for x in cursor.description],[x for x in cursor.fetchone()]))3print(new)4# 下面是结果5{‘id’: 2, ‘type’: ‘NBA’, ‘title’: ‘考辛斯跟腱撕裂赛季报销 浓眉詹皇发声祝福’, ‘content’: ‘他遭遇左脚跟腱撕裂，将缺席赛季剩下的比赛。这无疑对考辛斯和鹈鹕队都是一个重大的打击’, ‘view_count’: 3560, ‘release_time’: datetime.datetime(2018, 1, 27, 12, 10), ‘author’: ‘xiaoylin’, ‘from’: ‘腾讯体育’, ‘is_valibale’: 1}这里利用zip函数和列表生成式来一行代码就生成成功了用字典来查询，现在就可以了1print(new[‘title’])2# 下面是结果3考辛斯跟腱撕裂赛季报销 浓眉詹皇发声祝福但是，上面的只是一条数据的，如果是多条的呢？再按上面的方法就行不通了。这时就需要用到map函数了1def new2dict(new):2 return dict(zip([x[0] for x in cursor.description],[x for x in new]))3news_list = list(map(new2dict,cursor.fetchall()))4print(news_list)5# 下面是结果6[{‘id’: 2, ‘type’: ‘NBA’, ‘title’: ‘考辛斯跟腱撕裂赛季报销 浓眉詹皇发声祝福’, ‘content’: ‘他遭遇左脚跟腱撕裂，将缺席赛季剩下的比赛。这无疑对考辛斯和鹈鹕队都是一个重大的打击’, ‘view_count’: 3560, ‘release_time’: datetime.datetime(2018, 1, 27, 12, 10), ‘author’: ‘xiaoylin’, ‘from’: ‘腾讯体育’, ‘is_valibale’: 1}, {‘id’: 3, ‘type’: ‘NBA’, ‘title’: ‘火箭挖21分大哈登得背锅 连遭浓眉大帽太尴尬’, ‘content’: ‘火箭在客场以113-115惜败于鹈鹕，4连胜终结。詹姆斯-哈登出战34分钟16投5中，其中三分球9投只有1中，罚球14罚12中，拿到23分、11助攻、5篮板但也有4次失误，其在场正负值为尴尬的-12分’, ‘view_count’: 7520, ‘release_time’: datetime.datetime(2018, 1, 27, 12, 5), ‘author’: ‘youngcao’, ‘from’: ‘腾讯体育’,‘is_valibale’: 1}, {‘id’: 4, ‘type’: ‘英超’, ‘title’: ‘足总杯-曼联4-0英乙球队晋级 桑神首秀造两球’, ‘content’: ‘2017-18赛季英格兰足总杯第4轮，曼联客场4比0击败英乙球队约维尔，顺利晋级下一轮。桑切斯迎来曼联首秀，并制造了两个入球’, ‘view_count’: 6560, ‘release_time’: datetime.datetime(2018, 1, 27, 5, 49), ‘author’: ‘ricazhang’, ‘from’: ‘腾讯体育’,‘is_valibale’: 1}, {‘id’: 5, ‘type’: ‘英超’, ‘title’: ‘这才配红魔7号！桑神首秀大腿级表演 回击嘘声质疑’, ‘content’: ‘在今天凌晨对阵约维尔的首秀也值得期待。虽然在登场的72分钟时间里没有进球，但送出1次助攻且有有6次威胁传球的数据还是十分亮眼’, ‘view_count’: 2760, ‘release_time’: datetime.datetime(2018, 1, 27, 6, 13), ‘author’: ‘yaxinhao’, ‘from’: ‘腾讯体育’, ‘is_valibale’: 1}]这里很巧妙的利用了map函数，因为多条数据就可以进行迭代了，需要操作每条数据，这样就可以想到map函数接下来我们再用面向对象的方法来用python进行查询数据库 1import pymysql 2class MysqlSearch(object): 3 def get_conn(self): 4 ‘’’连接mysql数据库’’’ 5 try: 6 self.conn = pymysql.connect(host=‘127.0.0.1’,user=‘root’,passwd=‘your password’,port=3306,charset=‘utf8’,db=‘news’) 7 except pymysql.Error as e: 8 print(e) 9 print(‘连接数据库失败’)1011 def close_conn(self):12 ‘’’关闭数据库’’’13 try:14 if self.conn:15 self.conn.close()16 except pymysql.Error as e:17 print(e)18 print(‘关闭数据库失败’)1920 def get_one(self):21 ‘’’查询一条数据’’’22 try:23 # 这个是连接数据库24 self.get_conn()25 # 查询语句26 sql = ‘SELECT FROM new WHERE type=%s’27 # 这个光标用来执行sql语句28 cursor = self.conn.cursor()29 cursor.execute(sql,(‘英超’,))30 new = cursor.fetchone()31 # 返回一个字典，让用户可以按数据类型来获取数据32 new_dict = dict(zip([x[0] for x in cursor.description],new))33 # 关闭cursor34 cursor.close()35 self.close_conn()36 return new_dict37 except AttributeError as e:38 print(e)39 return None40 def get_all(self):41 ‘’’获取所有结果’’’42 sql = ‘SELECT FROM new ‘43 self.get_conn()44 try:45 cursor = self.conn.cursor()46 cursor.execute(sql)47 news = cursor.fetchall()48 # 将数据转为字典，让用户根据键来查数据49 news_list =list(map(lambda x:dict(zip([x[0] for x in cursor.description],[d for d in x])),news))50 # 这样也行,连续用两个列表生成式51 news_list = [dict(zip([x[0] for x in cursor.description],row)) for row in news]52 cursor.close()53 self.close_conn()54 return news_list55 except AttributeError as e:56 print(e)57 return None5859def main():60 # 获取一条数据61 news = MysqlSearch()62 new = news.get_one()63 if new:64 print(new)65 else:66 print(‘操作失败’)6768 # 获取多条数据69 news = MysqlSearch()70 rest = news.get_all()71 if rest:72 print(rest)73 print(rest[7][‘type’],rest[7][‘title’])74 print(‘类型：{0},标题：{1}’.format(rest[12][‘type’],rest[12][‘title’]))75 for row in rest:76 print(row)77 else:78 print(‘没有获取到数据’)7980if name == ‘main‘:81 main()这样就可以通过实例的方法来进行查询数据库了我们还可以根据页数来进行查询指定的数据数 1 def get_more(self,page,page_size): 2 ‘’’查多少页的多少条数据’’’ 3 offset = (page-1)page_size 4 sql = ‘SELECT FROM new LIMIT %s,%s’ 5 try: 6 self.get_conn() 7 cursor = self.conn.cursor() 8 cursor.execute(sql,(offset,page_size,)) 9 news = [dict(zip([x[0] for x in cursor.description],new)) for new in cursor.fetchall()]10 cursor.close()11 self.close_conn()12 return news13 except AttributeError as e:14 print(e)15 return None1617def main():18 #获取某页的数据19 news = MysqlSearch()20 new = news.get_more(3,5)21 if new:22 for row in new:23 print(row)24 else:25 print(‘获取数据失败’)2627if name == ‘main‘:28 main()利用的是mysql的limit关键字，还有其他的，比如进行排序分组的感兴趣的可以自己尝试下3增加数据到数据库 1 def add_one(self): 2 sql = ‘INSERT INTO new(title,content,type,view_count,release_time) VALUE(%s,%s,%s,%s,%s)’ 3 try: 4 self.get_conn() 5 cursor = self.conn.cursor() 6 cursor.execute(sql, (‘title’, ‘content’, ‘type’, ‘1111’, ‘2018-02-01’)) 7 cursor.execute(sql, (‘标题’, ‘内容’, ‘类型’, ‘0000’, ‘2018-02-01’)) 8 # 一定需要提交事务，要不不会显示，只会占位在数据库 9 self.conn.commit()10 return 111 except AttributeError as e:12 print(‘Error:’, e)13 return 014 except TypeError as e:15 print(‘Error:’, e)16 # 发生错误还提交就是把执行正确的语句提交上去17 # self.conn.commit()18 # 下面这个方法是发生异常就全部不能提交,但语句执行成功的就会占位19 self.conn.rollback()20 return 021 finally:22 cursor.close()23 self.close_conn()2425 def main():26 news = OperateSQL()27 if news.add_one():28 print(‘增加数据成功’)29 else:30 print(‘发生异常，请检查！！！’)3132 if name == ‘main‘:33 main()因为是增加数据，所以需要提交事务，这就需要用到cursor.commit()来进行提交，在增加数据后，如果不提交，数据库就不会显示。还有修改数据和删除数据就不贴出来了，只是把上面的sql变量的语句改成修改或者删除的语句就可以了，如果你还不会，建议练习下END代码我放在github了，网站为https://github.com/SergioJune/gongzhonghao_code，有兴趣的可以去看看，如果可以的话希望给个star哈！这篇文章只适合入门的，如果需要学习更多的话可以去查看pymysql的文档http://pymysql.readthedocs.io/en/latest/ 。日常学python一个专注于python的公众号","tags":[{"name":"mysql","slug":"mysql","permalink":"http://sergiojune.com/tags/mysql/"}]},{"title":"谈谈如何抓取ajax动态网站","date":"2019-01-14T07:56:55.592Z","path":"2019/01/14/谈谈如何抓取ajax动态网站/","text":"什么是ajax呢，简单来说，就是加载一个网页完毕之后，有些信息你你还是看不到，需要你点击某个按钮才能看到数据，或者有些网页是有很多页数据的，而你在点击下一页的时候，网页的url地址没有变化，但是内容变了，这些都可以说是ajax。如果还听不懂，我给你看看百度百科的解释吧，下面就是。Ajax 即“Asynchronous&nbsp;Javascript&nbsp;And&nbsp;XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。Ajax = 异步&nbsp;JavaScript&nbsp;和&nbsp;XML（标准通用标记语言的子集）。Ajax 是一种用于创建快速动态网页的技术。Ajax 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。&nbsp;[通过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用 Ajax）如果需要更新内容，必须重载整个网页页面。下面说下例子，我抓取过的ajax网页最难的就是网易云音乐的评论，感兴趣的可以看看利用python爬取网易云音乐，并把数据存入mysql这里的评论就是ajax加载的，其他的那个抓今日头条妹子图片的也算是ajax加载的，只不过我把它简单化了。还有很多，就不说了，说下我今天要说的ajax网站吧！http://www.kfc.com.cn/kfccda/storelist/index.aspx这个是肯德基的门面信息这里有很多页数据，每一页的数据都是ajax加载的。如果你直接用python请求上面那个url的话，估计什么数据都拿不到，不信的话可以试试哈。这时候，我们照常打开开发者工具。先把所有请求清楚，把持续日志打上勾，然后点击下一页，你会看到上面那个请求就是ajax请求的网页，里面就会有我们需要的数据，我们看看是什么样的请求是个post请求，请求成功状态码为200，请求url上面也有了，下面的from data就是我们需要post的数据，很容易就可以猜到pageIndex就是页数，所以我们可以改变这个值来进行翻页。这个网页就分析完了，这样就是解决ajax动态网页了，是不是觉得很简单，其实不是的，只是这个网页比较简单的，因为表单(from data)的数据并没有进行加密，如果进行加密的话估计你的找js文件看看参数是怎样加密的了，这就是我之前写的网易云音乐评论的爬取。看这些混淆的js寻找加密方法的话有时会让你很头痛，所以经常有人会选择用selenium这些来进行爬取，但是用这些会使爬虫的性能降低，所以这个方法在工作里是不允许的。所以必须学会怎样应对这些ajax。贴下代码import&nbsp;requestspage =&nbsp;1while&nbsp;True:&nbsp; &nbsp;url =&nbsp;‘http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#39;&nbsp; &nbsp;data = {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘cname’:&nbsp;‘广州’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘pid’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘pageIndex’: page,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘pageSize’:&nbsp;‘10’&nbsp; &nbsp;}&nbsp; &nbsp;response = requests.post(url, data=data)&nbsp; &nbsp;print(response.json())&nbsp; &nbsp;if&nbsp;response.json().get(‘Table1’,&nbsp;‘’):&nbsp; &nbsp; &nbsp; &nbsp;page +=&nbsp;1&nbsp; &nbsp;else:&nbsp; &nbsp; &nbsp; &nbsp;break可以看到去掉from data，不用十行代码就可以把数据都爬下来了，所以说这个网站适合练手，大家可以去试试。写在最后下篇文章我会写下复杂点的ajax请求，这个网站http://drugs.dxy.cn/不知道有多少人想看，想看的话点个赞吧！或者你可以自己先试试哈推荐文章如何爬取asp动态网页？搞定可恶的动态参数，这一文告诉你！利用python爬取网易云音乐，并把数据存入mysql日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"我爬取了37000条球迷评论，知道了这场比赛的重要信息","date":"2019-01-14T07:56:55.494Z","path":"2019/01/14/我爬取了37000条球迷评论，知道了这场比赛的重要信息/","text":"这是日常学python的第18篇原创文章这次用python爬虫爬点好玩的东西这两天看恰好有nba决赛，是球迷的你肯定不会错过的，更何况今年的西部决赛是火箭对战勇士，今年的火箭是很强的，因为没到关键时候总会有人站出来。当然，勇士也是挺强的，毕竟不能小看库里杜兰特等四大巨头。东部的决赛我就不太知道了，一直以为是凯尔特人会苦战骑士，谁知道缺了两大主力的凯尔特人还是很强，而且还打了骑士2：0，看来这次的骑士会是凶多吉少了，不知道凯尔特人会不会成功复仇，让我们拭目以待吧！有直播就肯定有评论，所以我想爬取下球迷评论，看看他们都在聊什么！准备工作需要用到的库：&nbsp; &nbsp; requests：用于网络请求&nbsp;&nbsp;&nbsp;&nbsp;jieba：用于分词&nbsp;&nbsp;&nbsp;&nbsp;wordcloud：制作词云图&nbsp;&nbsp;&nbsp;&nbsp;numpy：制作背景图片词云背景图片：上面的库都是可以直接用pip进行下载的，但是wordcloud会报错，报错如下：我们需要去官网下载whl文件进行手动安装官网：https://www.lfd.uci.edu/~gohlke/pythonlibs/然后找到对应自己安装的python版本进行下载最后在命令行下安装即可pip install “文件路径+whl文件名”接下来寻找目标网页文字直播地址：https://www.zhibo8.cc/zhibo/nba/2018/0517123898.htm?redirect=zhibo在这个网页通过抓包（按下f12）课知道下面这个链接是返回评论信息，而且是个json链接为：https://cache.zhibo8.cc/json/2018/nba/0517123898_384.htm?key=0.6512348313080727通过多次分析知道上面加粗的是直播间的信息，后面的下划线之后的是评论的页数，最后的key参数是个随机数，带不带上进行请求都没有关系用代码来获取评论信息def&nbsp;get_json(self, index):&nbsp; &nbsp; &nbsp; &nbsp;url =&nbsp;‘https://cache.zhibo8.cc/json/2018/nba/0517123898_%d.htm?key=0.1355540028791382&#39;&nbsp;% index&nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url)&nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;response.status_code ==&nbsp;200:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for&nbsp;item&nbsp;in&nbsp;response.json():&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 写入文件&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.write_file(item[‘content’])&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.num +=&nbsp;1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;1&nbsp; &nbsp; &nbsp; &nbsp;else:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;0评论信息有了，接着弄张词云图def&nbsp;get_wordcloud(self):&nbsp; &nbsp; &nbsp; &nbsp;with&nbsp;open(‘comments.txt’,&nbsp;‘r’, encoding=‘utf-8’)&nbsp;as&nbsp;comments:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;text = comments.read() &nbsp;# 加载数据&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;words =&nbsp;‘ ‘.join(jieba.cut(text, cut_all=True)) &nbsp;# 采用结巴全分词模式&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image = np.array(Image.open(‘1.jpg’)) &nbsp;# 背景图片&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 初始化词云&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;wc = WordCloud(font_path=r’C:\\Windows\\Fonts\\simkai.ttf’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; background_color=‘white’, mask=image,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; max_font_size=100, max_words=2000)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;wc.generate(words) &nbsp;# 生成词云&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;wc.to_file(‘img.png’) &nbsp;# 生成图片&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_file = Image.open(‘img.png’) &nbsp;# 打开图片&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_file.show()好了，代码完成，看下效果：利用词云图一眼就可以看出球迷都在评论什么了，因为我爬取的是火箭主场对战勇士的第二场比赛，肯定讨论最多的就是勇士火箭了，紧接的就是杜兰特了，死亡之神，这场的杜兰特超神拿了38分还是输给了火箭，自然而然就是讨论他最多了。还有就是这场站出来的塔克，三分6中5，还破了个人季后赛的最高得分，讨论他也是很正常的事。还有一个很显眼的就是第三节，很多人都认为勇士是“勇三疯”，以为这场比赛勇士会在第三节爆发吧？其实这赛季的火箭第三节也是很强的，并不比勇士弱。完整代码已经上传到我的github上了，如果需要的话可以自行查看，如果觉得程序不错的话希望可以给个star哈！github：https://github.com/SergioJune/gongzhonghao_code写在最后如果这篇文章对你用的话，希望不要吝啬你的点赞哈！点赞和转发就是对我的最大支持，这样才有动力输出质量高的原创文章。「球迷的话点个赞？我看看有多少个是球迷」推荐文章：&lt;a href=”http://mp.weixin.qq.com/s?biz=MzU0NzY0NzQyNw==&amp;mid=2247483774&amp;idx=1&amp;sn=769c06f2cf532b645d9cb9ba63b52b81&amp;chksm=fb4a7bd2cc3df2c47851c244db40b5cddccf2e21381d7742e39b9db2bdee0f12b83f870814bf&amp;scene=21#wechat_redirect” target=”_blank” _href=”http://mp.weixin.qq.com/s?__biz=MzU0NzY0NzQyNw==&amp;mid=2247483774&amp;idx=1&amp;sn=769c06f2cf532b645d9cb9ba63b52b81&amp;chksm=fb4a7bd2cc3df2c47851c244db40b5cddccf2e21381d7742e39b9db2bdee0f12b83f870814bf&amp;scene=21#wechat_redirect&quot; style=”word-break: normal !important;”&gt;使用requests+BeautifulSoup的简单爬虫练习python爬虫常用库之requests详解日常学python代码不止bug，还有美和乐趣​","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"老司机带你用python来爬取妹子图","date":"2019-01-14T07:56:44.981Z","path":"2019/01/14/老司机带你用python来爬取妹子图/","text":"这是日常学python的第14篇原创文章我前几篇文章都是说一些python爬虫库的用法，还没有说怎样利用好这些知识玩一些好玩的东西。那我今天带大家玩好玩又刺激的，嘻嘻！对了，requests库和正则表达式很重要的，一定要学会！一定要学会！！一定要学会！！！我现在的爬虫基本都是用这两样东西来爬的。所以学不学你看着办吧。来到今天的重点，我今天发现一个网站很好爬的，非常适合新手，我没有设置请求头什么的爬了很多遍很没有封我ip和给我返回403之类的，所以他对我们第一次玩爬虫的人来说很友好。这个网站就是今日头条。最重要的是这里面有很多美女图片，我们可以把它们爬下来！！！是不是想想都要流鼻血啊？文章首发于公众号「日常学python」我们今天要爬的就是他的图集，先看看网站。搜索美女，然后点击图集，可以看到下面这些内容我们要做的就是把上面的图片给爬下来。那开始分析网站。按下f12，然后点击network，刷新下你可以看到这些进行寻找哪个请求返回这些图片的，在网页上可以看到图片会随着你下拉网页而进行显示更多的图片，这是动态加载的，所以可以轻松知道这个可以在xhr文件中找到，果然，你看不断往下拉，不断地发送请求，点击这个请求看看是返回什么数据可以看到这是个json，里面有图片的url，这个就是我们要找的东西，那我们可以用json库来解析，还有这个网站是get请求，这样就可以用requests库来发送然后解析下就可以了，非常简单。那么分析就到这里，直接上代码import requests, ospath_a = os.path.abspath(‘.’)kw = ‘’while True: &nbsp; &nbsp;kw = input(‘请输入你要获取的图片(若想结束请输入1)’) &nbsp; &nbsp;if kw == ‘1’: &nbsp; &nbsp; &nbsp; &nbsp;print(‘已退出，你下载的图片已保存在’+path_a+‘,请查看！’) &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp;for x in range(0, 1000, 20): &nbsp; &nbsp; &nbsp; &nbsp;url = ‘https://www.toutiao.com/search_content/?offset=&#39;+str(x)+‘&amp;format=json&amp;keyword=%s&amp;autoload=true&amp;count=20&amp;cur_tab=3&amp;from=gallery’ % kw &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url) &nbsp; &nbsp; &nbsp; &nbsp;data = response.json()[‘data’] &nbsp; &nbsp; &nbsp; &nbsp;if not data: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载’+kw+‘图片完毕，请换个关键词继续’) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp; &nbsp; &nbsp;n = 1 &nbsp;# 记录文章数 &nbsp; &nbsp; &nbsp; &nbsp;for atlas in data: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 创建目录 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;title = atlas[‘title’] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(atlas) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;try: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if title not in os.listdir(‘.’): &nbsp;# 防止文件名已经存在 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.mkdir(title) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;except OSError as e: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘文件名出错，创建目录失败，重新创建一个随机名字’) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;title = kw + ‘文件名出错’+str(x) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if title not in os.listdir(‘.’): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.mkdir(title) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;k = 1 &nbsp;# 记录下载的图片数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;path = os.path.join(path_a, title) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 转进图片目录 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.chdir(path) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for image in atlas[‘image_list’]: &nbsp;# 这个链接获取的图片是小张的，看着不够爽，所以下面替换成大的图片 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_url = image[‘url’].replace(‘list’, ‘large’) &nbsp;# 改个链接获取大的图片 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;atlas = requests.get(‘http:’+image_url).content &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with open(str(k)+‘.jpg’, ‘wb’) as f: &nbsp;# 把图片写入文件内 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(atlas) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载完第%d个文章的%d幅图完成’ % (x+n, k)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;k += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 转出图片目录 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;os.chdir(path_a)这个只用了requests库基本就能完成了，os库是用来操作文件目录的，这里就不详细说了。可以看到，代码量非常少，除开注释就大概四十行吧，是不是比其他语言简洁多了？是不是requests库很好用？这里可以充分体现了人生苦短，我用python的真理。而且，他还可换关键字继续搜，你想搜什么照片都可以。下篇文章写个requests库和正则来爬内容的文章，让你们感受下正则的强大！最后给你们看下结果不说那么多了，我要去买营养快线了。上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑爬虫必学知识之正则表达式上篇◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;爬虫必学知识之正则表达式下篇日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"教你用python登陆豆瓣并爬取影评","date":"2019-01-14T07:56:44.957Z","path":"2019/01/14/教你用python登陆豆瓣并爬取影评/","text":"在上篇文章爬取豆瓣电影top250后，想想既然爬了电影，干脆就连影评也爬了，这样可以看看人们评价的电影，再加上刚出不久的移动迷官3好像挺热的，干脆就爬他吧，爬完看看好不好看！进入主题1.去找目标网页并找到所要找的数据一进去网页就条件反射打开开发者工具，很容易就看到了这个鼠标所点的就是我接下来要爬的网站，先看看他的response和请求头之类的信息，他的请求方式时get，response是一个网页结构，这就好办了，我们就可以用正则来匹配出所要的数据，正则还是个很好用的东西，请大家务必要学会啊。那接下来就动手敲代码咯！2.用re+requests获取数据获取信息先把数据写入txt文件中（打开的文件要指定编码为utf-8，要不会出现编码问题，因为window的默认编码方式是gbk，而你的编码为utf-8）正则表达式和网址一点击运行，只运行了两页，就出了问题，因为这个评论不止两页调试了下，在获取完第二页的时候他返回了个不存在的网页，导致我的正则表达式捕捉不到数据，出现了个空的page，所以就只下载了两页，这应该是被反爬了，继续回网页看看需要加什么请求头，然而我把全部的请求头的信息都加了，还是没用，这就触及到我的盲区了（尴尬脸），但是我可以百度啊，百度一看，看见有人说模拟登陆就可以了，那好，我就来模拟登陆一波！！！3.模拟登陆豆瓣首先需要看看登陆需要什么参数，这个参数是在豆瓣的登陆网址，先打开登陆，打开开发者工具(要不会看不到后面这个所需要的网页)，填好信息点击登陆，然后点击这个login网页，往下拉就会看到From Data 这个框，这个就是登陆所要的参数直接把他们复制过来即可然后就用post把信息发到服务器完成登陆，但是这有个问题，怎么保存登陆信息呢？这就需要用到Session()来保留了，但是注意，只需要建立一个会话信息就可以了，不是每个都用这个方法，我初学时就是犯这个错误以至于我搞了很久还没有登陆成功。代码如下然后用这个post上去，注意！注意！注意！post的网址是登陆网址，不是你要爬的网址，我刚学时也是被这个坑了很久(怎么感觉我很多问题)，还有其他用requests的都需要替换成self.ssession()​最后这样大功告成，由于只能获取500条这是因为豆瓣只开放了500条评论信息，多一条都不肯给4.登陆多了需要填验证码由于我多次登陆注销，然后我就需要填验证码了，然而这还是难不到我，还是分析网页找出验证码图片然后下载下来自己填写，还没有那些大佬那么厉害可以用人工智能来填写，代码如下还有将数据保留到数据库，我就不贴了，代码和上篇文章的差不多通过这个我学会了使用session来保存会话信息来登陆简单网页，还可以填写验证码，自己还是觉得有点高大上的，嘻嘻。由于本人还没学数据分析，就只能到这里，而生成词云也有点不会，直接复制粘贴来无趣，所以就先不写了，等大神你来写吧！最后非常感谢你看完了我的文章，如果觉得有用可以点赞，转发哈！若需要完整代码在微信公众号日常学python后台回复影评即可，若需要python相关的电子书也可以回复pdf获得，日后还会有更多福利发给你日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"反爬虫与反反爬虫","date":"2019-01-14T07:56:44.942Z","path":"2019/01/14/反爬虫与反反爬虫/","text":"喜欢爬虫的伙伴都知道，在爬网站的内容的时候并不是一爬就可以了，有时候就会遇到一些网站的反爬虫，折回让你爬不到数据，给你返回一些404，403或者500的状态码，这有时候会让人苦不堪言，就如我昨天发的爬网易云音乐评论，在你爬的数据较多时，网站认为你是一个机器，就不让你爬了，网易云就给我返回了一个{“code”:-460,”msg”:”Cheating”}，你不看下他的返回内容还不知道自己被反爬虫，不过不用担心，既然网页有反爬虫，可我们也有反反爬虫，今天就给大家说说反爬虫与反反爬虫。1网页的反爬虫1.通过网页的请求头首先我们先看看网易云音乐评论的请求头User-Agent：这个是保存用户访问该网站的浏览器的信息，我上面这个表示的是我通过window的浏览器来访问这个网站的，如果你是用python来直接请求这个网站的时候，这个的信息会带有python的字眼，所以网站管理员可以通过这个来进行反爬虫。Referer：当浏览器发送请求时，一般都会带上这个，这个可以让网站管理者知道我是通过哪个链接访问到这个网站的，上面就说明我是从网易云音乐的主页来访问到这个页面的，若你是用python来直接请求是，就没有访问来源，那么管理者就轻而易举地判断你是机器在操作。authorization:有的网站还会有这个请求头，这个是在用户在访问该网站的时候就会分配一个id给用户，然后在后台验证该id有没有访问权限从而来进行发爬虫。2.用户访问网站的ip当你这个ip在不断地访问一个网站来获取数据时，网页后台也会判断你是一个机器。就比如我昨天爬的网易云音乐评论，我刚开始爬的一首《海阔天空》时，因为评论较少，所以我容易就得到所有数据，但是当我选择爬一首较多评论的《等你下课》时，在我爬到800多页的时候我就爬不了，这是因为你这个ip的用户在不断地访问这个网站，他已经把你视为机器，所以就爬不了，暂时把你的ip给封了。2我们的反反爬虫1.添加请求头既然在请求网页的时候需要请求头，那么我们只需要在post或者get的时候把我们的请求头加上就可以了，怎样加？可以使用requests库来添加，在post，get或者其他方法是加上headers参数就可以了，而请求头不需要复制所有的信息，只需要上面的三个之中一个就可以，至于哪个自己判断，或者直接添加所有也可以，这样我们就可以继续爬了。2.使用代理ip若是网站把你的ip给封了，你添加什么的请求头也都没有用了，那我们就只有等他解封我们才可以继续爬吗？我可以十分自信告诉你：不需要，我们可以使用代理ip来继续爬，我们可以爬取网络上的免费ip来爬，至于免费的代理ip质量怎样你们应该知道，有必要可以买些不免费的，这样好点，我们平时的练习用免费的代理ip就可以了，可以自己爬取一些免费代理ip建成ip池，然后爬的时候就把ip随机取出来，我偷偷告诉你：小编明天的文章就是教你怎样搭建自己的代理ip池。END结束语：上面的只是个人在爬一些网站时候遇到的一些反爬虫，这只是很简单的，还有那些动态网站的反爬虫自己还没有接触，等到以后接触了，再一 一补充。最后给大家在爬虫上的建议，就是爬取速度不要太快，最好每几个就隔几秒，不要给服务器造成太大的压力，也可以在爬虫的时候选择一些访问量少点的时间段，这是对服务器好，也是对你自己好！日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"搭建自己的代理ip池","date":"2019-01-14T07:56:44.868Z","path":"2019/01/14/搭建自己的代理ip池/","text":"​继上一篇说了反爬虫之后，我说今天这篇文章会搭建一个属于自己的代理ip池，所以，为了不食言，就写了这篇文章，那好废话不多说，进入正题1目标网站爬取代理ip，这也需要找网页，这就得看看哪个网页提供这些代理ip了，本人知道了几个免费提供代理ip的网站，如下：无忧代理ip芝麻代理ip西刺代理ip云连代理ip我选择了爬取西刺代理的网站。2分析网站结构我们需要获取的高匿代理，按下F12打开开发者工具上面我们要获取的数据是ip地址，端口和类型这三个就可以了，可以看到，这些数据都在一个tr标签里，但是有两个不同的tr标签，这样可以用正则表达式，利用相隔的html结构先把整个内容匹配下来，再把重要信息匹配下来，最后就把他弄成这个样子{‘https’: ‘https://ip:端口&#39;}存入列表即可，最后就随机获取一个ip，然后可以先判断是否有用，再拿来做你此时项目的代理ip，判断是否用的方法就是随便拿一个百度获取别的网站，加上代理ip发送get请求，看看status_code()的返回码是不是200，即可，就如下面这样3代码部分1.匹配数据，并挑选数据存入列表2.随机获取ip，并写好ip格式我这里是把他存入列表，现抓现用，是因为我现在的爬虫项目都是很小的，只需要这些就可以了。END以上就是我简单搭建的代理ip池了，等到以后慢慢完善，你可以把他们存入你的数据库，然后要用的时候，就随机拿出来，先看看有没有用，没用的话就删除，有用就拿来用即可。日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"利用python爬取豆瓣电影Top250，并把数据放入MySQL数据库","date":"2019-01-14T07:56:44.844Z","path":"2019/01/14/python爬取豆瓣电影Top250/","text":"在学习了python基础后，一心想着快速入门爬虫，因为我就是为爬虫而学的python,所以就找了这个豆瓣电影来爬取。好了，废话不多说，进入正题1.找到网页并分析网页结构首先进入豆瓣电影Top250这个网页，按下f12打开开发者工具，如下图然后开始分析网页，点击开发者工具左上角的有个箭头的东西去找你需要找的数据，在这里我发现每个电影的信息都是在&lt;li&gt;的标签内，所以可以用正则表达式来先提取每一个电影，然后在分别提取每个电影中的数据。每个电影现在的数据都可以获取了，但是这个url只有25个电影，怎样获取下一页的呢？这里我们可以在每个页面获取下一页的链接，然后通过循环来继续获取下一页的电影数据即可我们可以先用开发者工具的箭头点一下后页，然后就显示右边的箭头数据出来，这里我们也可以用正则表达式来获取下一页的链接，然后接下来的工作就是循环了，好了分析结束，开始敲代码吧！2.用面向对象的方法进行爬取数据先用requests对网页进行请求，获取网页的html结构，在这里，为了防止网页的反爬虫技术，我加了个请求头（记得使用requests库之前先导入，没有的可以在命令行通过 pip install requests 进行下载）请求头在开发者工具中查看，如下图所示接下用正则表达式进行获取数据先匹配每一个电影和每一页数据（使用正则表达式的库是re）接下来获取每个电影的数据注意：获取到上面的数据，有的是空的，所以还需要进行判断是否为空，为了好看，我用了三元表达式进行判断，完成之后把他们存入字典接下来就是进行循环取下一页的数据了3.如果你有点数据库基础的话，还可以把他们存入数据库，在这里我把这些数据存入MySQL数据库，代码如下，需要自己先建好数据库好表格这是操作数据库的类（使用的库为pymysql）然后回到爬虫类进行把数据存入数据库4.成功后你就会在数据库中查到以下数据END最后，非常感谢你看完了这篇文章，喜欢的话，或者有什么问题的话欢迎去我的微信公众号日常学python后台回复我，我会认真回答的。ps:如果需要完整代码的话可以在微信公众号日常学python后台回复top250即可，或者想要什么学习资源也可以后台找我哦","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"python爬虫常用库之requests详解","date":"2019-01-14T07:56:44.795Z","path":"2019/01/14/python爬虫常用库之requests详解/","text":"在使用了urllib库之后，感觉很麻烦，比如获取个cookie都需要分几步，代码又多，这和python的风格好像有点不太像哈，那有没有更加容易点的请求库呢？答案是有的，那就是第三方库requests,这个库的作者是大名鼎鼎的kennethreitz，创作这个库的原因就是想让python开发者更加容易地发起请求，处理请求。里面还有个名字：HTTP for Humans,顾名思义，就是用来请求http的。想看源代码的可以在github上搜索他的名字就可以看到了。接下来介绍下怎样用这个库吧！因为这是第三方库，所以我们需要下载，需要在命令行输入pip install requests如果你装的是anacoda可以忽略这条安装好了就来进行使用吧1进行简单的操作发送一个get请求# 发送请求import requestsresponse = requests.get(‘http://httpbin.org/get&#39;)# 获取返回的html信息print(response.text)这样就发送了一个get请求，并且还打印了返回的内容，这个不再需要知道网页是哪个编码的，不过有时会出现编码问题，但是你也可以指定编码类型，如：response.encoding = ‘utf-8’指定完成后就可以正常编码了，前提你得知道网页的编码类型。出了上面这些，我们还可以获取下面的信息print(response.headers)# 请求状态码print(response.status_code)# 获取网页的二进制内容print(response.content)print(response.url) # 获取请求的urlprint(response.cookies) # 获取cookie是不是觉得很容易，一行代码就可以了。不再需要几步代码什么的了。接下来被发爬的话带上个请求头来进行请求# 还可以添加请求头进行请求headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36’}response = requests.get(‘http://httpbin.org/get’, headers=headers )print(response.headers)print(response.text)加个请求头也就是加个关键字参数而已还可以进行带参数的get请求# 进行带参数的get请求data = {‘name’: ‘june’, ‘password’: 123456}response = requests.get(‘http://httpbin.org/get&#39;, params=data)print(response.text)那如果需要登陆呢？post请求怎样发？告诉你，照样很简单# 进行post请求data = {‘name’: ‘june’, ‘password’: 123456}response = requests.post(‘http://httpbin.org/post&#39;, data=data, headers=headers)print(response.text)是不是很简单，也是加个data关键字参数，把要提交的登陆参数进行post上去。那除了上面的两个请求，还能进行别的请求吗？我可以非常开心地告诉你，可以的。比如，你要发个put请求，如这样requests.put()requests.delete()这个就是发送put请求和delete请求的，其他的请求也是这样发送，就不一 一说了。2进行复杂点的请求在登陆的时候我们有时候需要输入验证码，那怎样输呢？爬虫的看不了网页，最简单的做法就是把这个验证码的图片下载下来然后手动输入，那么我们怎样下载呢？我们可以向这个图片的url发送请求，然后把返回内容以二进制方法存入文件里面就可以了。代码如下：# 从网上读取二进制数据，比如图片response = requests.get(‘https://www.baidu.com/img/bd_logo1.png&#39;, headers=headers)# 这个是直接获取字节码，这个是要保存的文件print(response.content)# 这个是获取解码后的返回内容，这个是乱码print(response.text)# 用文件来把图片下载下来with open(‘baidu.png’, ‘wb’) as f: # 注意写的方式是以二进制方式写入 f.write(response.content) print(‘下载完毕’)还是很简单，不得不说，这个库太好用了。当我们需要上传文件的时候，比如图片，我们还可以用post方法把他发送出去# 上传文件files = {‘picture’: open(‘baidu.png’, ‘rb’)}response = requests.post(‘http://httpbin.org/post&#39;, files=files)print(response.text)获取cookie并简单处理一下# 获取cookieresponse = requests.get(‘https://www.baidu.com&#39;)for k, v in response.cookies.items(): print(k, ‘=’, v)当网页返回内容是json格式是，我们不需要用json库来解析，我们可以直接利用requests的方法进行解析，两者的效果是一样的# 解析jsonj = response.json() # 可以用json库来解析，结果一样在urllib库时保存登陆信息需要把cookie保存下来，但是在requests库里面，我们只需要用requests.session()来保存信息就可以了。# 用会话来保持登陆信息session = requests.session()response = session.get(‘http://httpbin.org/cookies/set/number/123456&#39;)print(response.text)这样就可以保存登陆了，不需要为cookie操心了，但是每次获取一个session就可以了，然后用来请求或者其他操作。不需要每次请求或者操作都创建一个sesion出来，这样是保存不了登陆信息的当一个网站不安全，需要你用证书验证的，比如这个网站https://www.12306.cn这时要访问里面的网站内容，我们就需要进行验证，代码如下# 证书验证response = requests.get(‘https://www.12306.cn&#39;, verify=False) # 不加这个关键字参数的话会出现验证错误问题，因为这个网站的协议不被信任这样就可以进行访问了，但是会有一条警告E:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning)觉得不美观的，我们还可以在请求时加个cert关键字参数，值为可信任的证书，为一个元组，写上账号和密码之类的，这里就不演示了遇到需要认证的网站，我们也可以这样from requests.auth import HTTPBasicAuth# 设置认证# requests.get(‘需要认证的网址’, auth=HTTPBasicAuth(‘user’, ‘passwd’)) # 由于找不到需要认证的网址，所以先写个主体# 还可以这样认证# requests.get(‘需要认证的网址’, auth=(‘user’, ‘passwd’)) # 这样就简单点由于我找不到需要认证的网站，所以就不演示了。requests还可以用代理ip来进行请求网站来防止ip被封以至于自己爬不了的尴尬。使用代理ip也比urllib库简单得多，代码如下：# 设置代理proxies = {‘http’: ‘http://122.114.31.177:808&#39;, ‘https’: ‘https://119.28.223.103:8088&#39;}# 在请求时添加上列代理response = requests.get(‘http://httpbin.org/get&#39;, proxies=proxies)print(response.text)上面的字典格式需要一 一对应，然后在请求时加上个关键字参数proxies就可以了。3请求异常处理在程序运行时，遇到错误时程序就会被强行停止，如果想要继续运行，就需要进行捕捉异常来让程序继续运行。在requests库中有个处理异常的库requests.exceptions这里简单地处理下请求超时的处理情况import requestsfrom requests.exceptions import ReadTimeout, ConnectTimeout, HTTPError, ConnectionError, RequestException# 捕捉异常try: response = requests.get(‘http://httpbin.org/get&#39;, timeout=0.1) # 规定时间内未响应就抛出异常 print(response.text)except ReadTimeout as e: print(‘请求超时’)except ConnectionError as e: print(‘连接失败’)except RequestException as e: print(‘请求失败’)这里捕捉了三个异常，因为ReadTimeout是ConnectionError的子类，所以先捕捉ReadTimeout,再捕捉父类的。而ConnectionError 和 RequestException 同理更多的异常处理可以查看文档哈。4最后以上均是我在学习时的笔记和个人在运用时遇到的一些坑都简单地记载了上去，希望对你有用哈，如果想看更多的用法可以去官方文档查看。还有代码我放在了github上，要的话可以上去查看。GitHub：https://github.com/SergioJune/gongzhonghao_code/tree/master/python3_spider官方文档：http://docs.python-requests.org/zh_CN/latest/index.html学习参考资料：https://edu.hellobi.com/course/157","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requests","slug":"requests","permalink":"http://sergiojune.com/tags/requests/"}]},{"title":"python爬虫常用库之BeautifulSoup详解","date":"2019-01-14T07:56:44.755Z","path":"2019/01/14/python爬虫常用库之BeautifulSoup详解/","text":"这是日常学python的第16篇原创文章经过了前面几篇文章的学习，估计你已经会爬不少中小型网站了。但是有人说，前面的正则很难唉，学不好。正则的确很难，有人说过：如果一个问题用正则解决，那么就变成了两个问题。所以说学不会是很正常的，不怕，除了正则，我们还可以用另外一个强大的库来解析html。所以，今天的主题就是来学习这个强大的库–BeautifulSoup，不过正则还是需要多多练习下的。因为是第三方库所以我们需要下载，在命令行敲下以下代码进行下载pip install beautifulsoup4安装第三方解析库pip install lxmlpip install html5lib如果不知道有什么用请往下看1.相关解析库的介绍这里官方推荐解析库为lxml,因为它的效率高。下面都是用lxml解析库来进行解析的。2.详细语法介绍本文是进行解析豆瓣图书首页https://book.douban.com/1)创建bs对象from bs4 import BeautifulSoupimport requestsresponse = requests.get(‘https://book.douban.com/&#39;).text# print(response)# 创建bs对象soup = BeautifulSoup(response, ‘lxml’) &nbsp;# 使用到了lxml解析库2)获取相关标签标签：&lt;a data-moreurl-dict=‘{“from”:”top-nav-click-main”,”uid”:”0”}’ href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;上面的a就是一个标签名字，最简单的就是&lt;a&gt;&lt;/a&gt;这样，可以简单理解为&lt;&gt;里面的第一个单词就是标签名# 获取标签print(soup.li) &nbsp;# 这个只是获取第一个li标签# 结果&lt;li class=””&gt;&lt;a data-moreurl-dict=’{“from“:“top-nav-click-main”,“uid”:“0”}‘ href=”https://www.douban.com&quot; target=”_blank”&gt;豆瓣&lt;/a&gt;&lt;/li&gt;3)获取标签的名字和内容标签的名字和内容：&lt;a &gt;豆瓣&lt;/a&gt;如上面所说，a就是标签名字，而两个标签之中所夹杂的内容就是我们所说的内容，如上，豆瓣就是该标签的内容# 获取标签名字print(soup.li.name)# 获取标签内容print(soup.li.string) &nbsp;# 这个只能是这个标签没有子标签才能正确获取，否则会返回None# 结果liNone由于这个li标签里面还有个子标签，所以它的文本内容为None下面这个就可以获取它的文本内容# 获取标签内的标签print(soup.li.a)print(soup.li.a.string) &nbsp;# 这个标签没有子标签所以可以获取到内容# 结果&lt;a data-moreurl-dict=‘{“from”:”top-nav-click-main”,”uid”:”0”}’ href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;豆瓣4)获取标签属性，有两种方法标签属性：&lt;a href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;可以简单理解为属性就是在标签名字旁边而且在前一个&lt;&gt;符号里面的，还有是有等号来进行体现的。所以上面的href就是标签属性名字，等号右边的就是属性的值，上面的值是个网址# 获取标签属性print(soup.li.a[‘href’]) &nbsp;# 第一种print(soup.li.a.attrs[‘href’]) &nbsp;# 第二种# 结果https://www.douban.comhttps://www.douban.com5)获取标签内的子标签子标签：&lt;li&gt;&lt;a&gt;豆瓣&lt;/a&gt;&lt;/li&gt;比如我们现在获取的li标签，所以a标签就是li标签的子标签# 获取标签内的标签print(soup.li.a)# 结果&lt;a data-moreurl-dict=‘{“from”:”top-nav-click-main”,”uid”:”0”}’ href=“https://www.douban.com&quot; target=“_blank”&gt;豆瓣&lt;/a&gt;6)获取所有子节点子节点：这个和子标签是差不多的，只不过这里是获取一个标签下的所有子标签，上面的只是获取最接近该标签的子标签# 获取子节点print(soup.div.contents) &nbsp;# 返回一个列表 第一种方法for n, tag in enumerate(soup.div.contents): &nbsp; &nbsp;print(n, tag)# 结果[‘\\n’, &lt;div class=”bd“&gt;&lt;div class=”top-nav-info“&gt;&lt;a class=”nav-login“ href=”https://www.douban.com/accounts/login?source=book“ rel=”nofollow“&gt;登录&lt;/a&gt;…0 1 &lt;div class=”bd“&gt;&lt;div class=”top-nav-info“&gt;…这个是获取div下的所有子节点，.content就是获取子节点的属性7)第二种方法获取所有子节点# 第二种方法print(soup.div.children) &nbsp;# 返回的是一个迭代器for n, tag in enumerate(soup.div.children): &nbsp; &nbsp;print(n, tag)这个是用.children获取所有的子节点，这个方法返回的是一个迭代器8)获取标签的子孙节点，就是所有后代子孙节点：&lt;ul&gt;&lt;li&gt;&lt;a&gt;豆瓣&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;从上面知道，li标签是ul标签的子标签，a标签是li标签的子标签，若此时我们获取的是ul标签，所以li标签和a标签都是ul标签的子孙节点# 获取标签的子孙节点print(soup.div.descendants) &nbsp;# 返回的是一个迭代器for n, tag in enumerate(soup.div.descendants): &nbsp; &nbsp;print(n, tag)# 结果…&lt;generator object descendants at 0x00000212C1A1E308&gt;0 1 &lt;div class=”bd“&gt;&lt;div class=”top-nav-info“&gt;&lt;a class=”nav-login“ href=”https://www.douban.com/accounts/login?source=book“ rel=”nofollow“&gt;登录&lt;/a&gt;…这里用到了.descendants属性，获取的是div标签的子孙节点，而且返回结果是一个迭代器9)获取父节点和所有祖先节点既然有了子节点和子孙节点，反过来也是有父节点和祖先节点的，所以都很容易理解的# 获取父节点print(soup.li.parent) &nbsp;# 返回整个父节点# 获取祖先节点print(soup.li.parents) &nbsp;# 返回的是一个生成器for n, tag in enumerate(soup.li.parents): &nbsp; &nbsp;print(n, tag).parent属性是获取父节点，返回来的是整个父节点，里面包含该子节点。.parents就是获取所有的祖先节点，返回的是一个生成器10)获取兄弟节点兄弟节点：&lt;ul&gt;&lt;li&gt;&lt;a&gt;豆瓣1&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a&gt;豆瓣2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a&gt;豆瓣3&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;比如上面的html代码，里面的li标签都是ul标签的子节点，而li标签都是处于同级的，所以上面的li标签都是各自的兄弟。这就是兄弟节点。# 获取兄弟节点print(soup.li.next_siblings) &nbsp;# 获取该标签的所有同级节点，不包括本身 &nbsp;返回的是一个生成器for x in soup.li.next_siblings: &nbsp; &nbsp;print(x)# 结果&lt;generator object next_siblings at 0x000002A04501F308&gt;&lt;li class=”on“&gt;&lt;a data-moreurl-dict=’{“from“:“top-nav-click-book”,“uid”:“0”}‘ href=”https://book.douban.com&quot;&gt;读书&lt;/a&gt;&lt;/li&gt;….next_siblings属性是获取该标签的所有在他后面的兄弟节点，不包括他本身。同时返回结果也是一个迭代器同理，既然有获取他的下一个所有兄弟标签，也有获取他前面的所有兄弟标签soup.li.previous_siblings如果只是获取一个即可，可以选择把上面的属性后面的s字母去掉即可，如下soup.li.previous_sibling &nbsp;# 获取前一个兄弟节点soup.li.next_sibling &nbsp;# 获取后一个兄弟节点3.bs库的更高级的用法在前面我们可以获取标签的名字、属性、内容和所有的祖孙标签。但是当我们需要获取任意一个指定属性的标签还是有点困难的，所以，此时有了下面这个方法：soup.find_all( name , attrs , recursive , text , **kwargs )name：需要获取的标签名attrs：接收一个字典，为属性的键值，或者直接用关键字参数来替代也可以，下面recursive：设置是否搜索直接子节点text：对应的字符串内容limit：设置搜索的数量1)先使用name参数来进行搜索# 先使用name参数print(soup.find_all(‘li’)) &nbsp;# 返回一个列表，所有的li标签名字# 结果[&lt;li class=””&gt;&lt;a data-moreurl-dict=’{“from“:“top-nav-click-main”,“uid”:“0”}‘ href=”https://www.douban.com&quot; target=”_blank”&gt;豆瓣&lt;/a&gt;&lt;/li&gt;, &lt;li class=”on”&gt;…这里获取了所有标签名字为li的标签2)使用name和attrs参数# 使用name和attrs参数print(soup.find_all(‘div’, {‘class’: ‘more-meta’})) &nbsp;# 这个对上个进行了筛选,属性参数填的是一个字典类型的# 结果[&lt;div class=”more-meta“&gt;&lt;h4 class=”title“&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;刺 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;/h4&gt;…这里搜索了具有属性为class=’more-meta’的div标签3)根据关键字参数来搜索# 对相关属性进行进行查找也可以这样print(soup.find_all(class_=‘more-meta’)) &nbsp;# 使用关键字参数，因为class是python关键字，所以关键字参数时需要加多一个下划线来进行区别# 结果和上面的结果一样…这里注意，我们找的是class属性为more-meta的标签，用了关键字参数，但是python里面有class关键字，所以为了不使语法出错，所以需要在class加个下划线其他参数的就不再介绍了，可以自行去官网查看4)find()方法此方法与find_all()方法一样，只不过这个方法只是查找一个标签而已，后者是查找所有符合条件的标签。还有很多类似的方法，用法都差不多，就不再一一演示了，需要的可以去官网查看5)select()方法这个方法是使用css选择器来进行筛选标签的。css选择器：就是根据标签的名字，id和class属性来选择标签。通过标签名：直接写该标签名，如&nbsp;li a&nbsp; ,这个就是找li标签下的a标签通过class属性：用. 符号加class属性值，如&nbsp;.title .time&nbsp;这个就是找class值为title下的class值为time的标签通过id属性：用# 加id属性值来进行查找，如&nbsp;#img #width&nbsp;这个就是找id值为img下的id值为width的标签上面三者可以混合使用，如&nbsp;ul .title #width如果还不太会的话，可以直接在浏览器上按下f12来查看位置在箭头所指的位置就是选择器的表达代码如下# 还可以用标签选择器来进行筛选元素, 返回的都是一个列表print(soup.select(‘ul li div’)) &nbsp;# 这个是根据标签名进行筛选print(soup.select(‘.info .title’)) &nbsp;# 这个是根据class来进行筛选print(soup.select(‘#footer #icp’)) &nbsp;# 这个是根据id来进行筛选# 上面的可以进行混合使用print(soup.select(‘ul li .cover a img’))这里的获取属性和文本内容# 获取属性for attr in soup.select(‘ul li .cover a img’): &nbsp; &nbsp;# print(attr.attrs[‘alt’]) &nbsp; &nbsp;# 也可以这样 &nbsp; &nbsp;print(attr[‘alt’])# 获取标签的内容for tag in soup.select(‘li’): &nbsp; &nbsp;print(tag.get_text()) &nbsp;# 里面可以包含子标签，会将子标签的内容连同输出.get_tex()方法和前面的.string属性有点不一样哈，这里的他会获取该标签的所有文本内容，不管有没有子标签写在最后以上的这些都是个人在学习过程中做的一点笔记。还有点不足，如果有错误的话欢迎大佬指出哈。如果想要查看更多相关用法可以去官方文档查看：http://beautifulsoup.readthedocs.io/zh_CN/latest/学习参考资料：https://edu.hellobi.com/course/157如果这篇文章对你有用，点个赞，转个发如何？还有，祝大家今天愚人节快乐MORE延伸阅读◐◑爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;老司机带你用python来爬取妹子图日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requests","slug":"requests","permalink":"http://sergiojune.com/tags/requests/"}]},{"title":"利用python爬取网易云音乐评论，并把数据存入mysql","date":"2019-01-14T07:56:44.729Z","path":"2019/01/14/python3.x爬取网易云音乐评论/","text":"在简单学习了python爬虫后，又想继续折腾，进而找到了这个网易云音乐，因为本人平时就是用它听的歌，也喜欢看歌里的评论，所以就爬网易云音乐评论吧，那么开始吧！正式进入主题首先还是去找目标网页并开始分析网页结构，如下上面的三个箭头都是所要找的数据，分别是评论用户，评论和点赞数，都可以用正则表达式找出来，接下来继续找怎样找到下一页的数据，还是用开发者工具，但是当点击下一页的时候，网页的url没有变，说明网页是动态加载，所以就不能在当前网页找数据了，应该在他的xhr文件里找，所以点入network看看，然后也点击下一页一看，果然有想要的看到这里，就兴奋地去敲代码了一点击运行，结果什么东西都没有，但是他的状态码是200，明显请求成功啊，却没有东西返回，再去network仔细看看这个网页，看到他是个post请求，也看到了需要post两个参数params和ensSecKey一看到这个，密密麻麻的数字和字母，就猜应该是被加密了，不过可以复制下来看看有没有用。接下来看下他的Response，咦，这是个json，不是html结构的，所以需要用到Json库来进行解析现在开始敲代码吧，先把上面的两个参数复制过来看看。现在把每条评论的评论用户和点赞数和评论获取出来可以看到，利用json.loads()方法把数据转成python格式里的字典后就可以把想要的数据取出来了，但是，下一页怎样取？总不能每次都复制粘贴那两个参数吧？那唯一的方法就是不爬了。。怎么可能？我的继续，那我就要进行破解这两个参数了，那好继续看network，因为要加密，肯定要用js进行加密的看到刚才那个网站的发起者core.js,,然后把它文件下载下来慢慢研究保存后在经过美化，然后进行查找那个encSecKey参数（ps:JSj’e’tong’yang’de美化网址为http://www.css88.com/tool/js_beautify/），然后找到这个看到window.asrsea()方法有四个参数，先不去管这个函数，先看看他的四个参数是什什，这里没必要去研究那四个参数怎样来的，只需要知道他是什么，那么我们可以加点代码上去让他显示出来，从而利用fiddler来进行调试加入代码如下可以分别获取上面的每一个参数，也把那个params获取看看，然后在fiddler上操作如下完成上面的设置后刷新网页就可以在console上面找到参数信息，如果没有的话这是因为你之前浏览该网页的时候它被缓存了下来，所以要清除缓存文件（在清除浏览器记录里面有）那个rid有本歌曲的id，明显是与评论有关的，我试着连翻几页后，发现那个offset就是评论偏移数，offset就是(页数-1)20，total在第一页是true，在其他页是false同样的方法也得到第二个参数为：010001第三个参数为：00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7第四个参数为：0CoJUm6Qyw8W8jud接下来就要看window.asrsea()方法是什么操作的了，还是通过查找js文件可以看到这个通过研究i是随机获取十六个字符而b函数是AES加密，其中偏移量为0102030405060708，模式为CBC，看回d函数，其中params连续两次加密，第一次加密时，文本为第一个参数。密钥为第四个参数，第二次加密时文本为第一次加密的值，密钥为随机数a。而encSeckey是一个RSA加密，他的公钥是第二个参数，模式是第三个参数，文本为那个随机字符串a终于分析完了，接着开始敲代码先来个获取第一页评论的代码这是获取两个参数的类这是解析网易云音乐和获取评论的类然而一点击运行，直接给我报了个错：TypeError: can’t concat str to bytes原来是因为在第二次加密的时候，那个params是个byte类型，所以把他转成字符串类型就可以了再次点击运行，结果还是报错了：json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)这个报错因为我的json解析错了，回头调试一看，网页返回的东西是空的，但他的状态码是200，这是什么鬼？接着我再试了把那两个参数的值直接复制和前面一样看看，结果运行成功，这就说明我的是加密过程错了，然后我就回去看了几篇，看不出什么错误，上网百度找到了这个知乎文章https://zhuanlan.zhihu.com/p/32069543，我把她的代码复制过来运行下，结果是可以的，我就继续看看我和她的区别，原来我在用那个16个随机字符的时候用错了，我在两个参数给了两个不同的，而是需要给共同一个的，看到这里，我就直接回去改了下，果然运行成功，代码我就不贴出来了，效果如下接着是获取每一页的评论，而每一页与第一个参数的offset有关，其中的公式为offse=(页数-1)20，total在第一页是true，在其他页是false而写入数据库我用的是我这篇文章的操作&nbsp;http://mp.weixin.qq.com/s/6sQ_ER39P2NtXaPOnGdQNA，由于篇幅过长，就不贴出来了，感兴趣的可以去看看接下来点运行就可以了，但是运行到第八页的时候出现了这个异常raise errorclass(errno, errval)pymysql.err.InternalError: (1366, “Incorrect string value: ‘\\xF0\\x9F\\x92\\x94’ for column ‘content’ at row 1”)原因是这条评论有个识别不了的表情，之后百度参考这篇文章http://blog.csdn.net/HHTNAN/article/details/76769264?locationNum=9&amp;fps=1修改了数据库的编码方式，注意还要自己修改下创建数据库时的编码方式才可！这是首页数据库效果获取完成（家驹的歌评论这么少吗？不解）终于完成了，虽然辛苦，但是值得，在这个过程中也学会了很多东西，在写这篇文章时参考了两篇文章，一个是知乎首个回答https://www.zhihu.com/question/36081767/answer/140287795，另一个就是解密过程https://github.com/cosven/cosven.github.io/issues/30大家有什么问题的话欢迎去我的公众号日常学python的后台那里问我，我知道的我都一一为你解答，最后，若你也在这篇文章学到了，可以帮我点个赞，转发下吗？谢谢支持哈！ps:若需要完整代码可以在我的公众号日常学python后台回复评论即可获取，还有其他福利以后会一一分析我的公众号的二维码","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"说说数据库","date":"2019-01-14T07:56:44.717Z","path":"2019/01/14/说说数据库/","text":"爬了数据，只能放在记事本上？小的数据还是可以的，但是当你遇到较多的数据，放在记事本上就不是很好了，这时就需要用到数据库来存储了，那我们今天的主题就是入门数据库，在入门数据库之前，我首先介绍下什么是数据库。进入正题1什么是数据库在这里我把百度百科的贴出来给大家哈，我也不好解释，毕竟自己页没有到那种境界数据库(Database)是按照 数据结构来组织、 存储和管理数据的仓库，它产生于距今六十多年前，随着 信息技术和市场的发展，特别是二十世纪九十年代以后， 数据管理不再仅仅是存储和管理数据，而转变成用户所需要的各种数据管理的方式。数据库有很多种 类型，从最简单的存储有各种数据的 表格到能够进行海量 数据存储的大型&nbsp;数据库系统都在各个方面得到了广泛的应用。在信息化社会，充分有效地管理和利用各类信息资源，是进行科学研究和决策管理的前提条件。数据库技术是管理信息系统、办公自动化系统、决策支持系统等各类信息系统的核心部分，是进行科学研究和决策管理的重要技术手段。数据有两种，一种是关系型数据库，另一种是非关系型数据。关系型数据就是以行和列的形式存储数据，以便于用户理解，这一系列的行和列称为表，一组表组成了数据库。表与表之间的数据记录有关系。非关系型数据，也叫NoSQL，就是Not only SQL。其性能是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能会非常高，而可扩展性同样也是基于键值对的，数据之间没有耦合性，所以非常容易水平扩展。比如我们即将学到的mongodb和redis都是这类型的数据库。2学习数据库今天我们先学习下关系型数据库的一种MySQL。那么我们就要先下载好mysql这个工具，网站我直接贴出来了https://dev.mysql.com/downloads/，在上面选择自己的版本进行下载安装即可。或者去下载XAMPP集成包，里面也有MySQL程序。百度即有下载。下载了数据库，还要下载个可视化工具，这样可以清储看到数据内容。可视化工具是Navicat，下载地址为https://www.navicat.com/en/products。注册码的百度找就有了，土豪请无视吧，下载安装后打开是这样的 然后点击连接，先连接一个本地数据库吧，填好上面的数据，注意上面只需填连接名和密码即可，本地连接的ip，端口和名字都是固定的，无需自己写了，然后点击测试看看，连接成功就可以按下确定了，这样就成功连接到你的本地数据库了。注意：密码是自己设置的，可以点击mysql的这个来设置，如下图END最后，你可以先自己在可视化工具尝试建立自己的第一个数据库看看，不会也没关系，我在明天继续更新mysql的相关基础语法。​","tags":[{"name":"mysql","slug":"mysql","permalink":"http://sergiojune.com/tags/mysql/"}]},{"title":"使用requests+BeautifulSoup的简单爬虫练习","date":"2019-01-14T07:56:44.678Z","path":"2019/01/14/使用requests+BeautifulSoup的简单爬虫练习/","text":"这是日常学python的第17篇原创文章上篇文章说了BeautifulSoup库之后，今篇文章就是利用上篇的知识来爬取我们今天的主题网站：猫眼电影top100。这个网站也挺容易的，所以大家可以先自己爬取下，遇到问题再来看下这篇文章哈。这篇文章主要是练习而已，别无用处，大佬请绕道哈！1、本文用到的库及网站requestsBeautifulSoup目标网站：http://maoyan.com/board/42、分析目标网站很容易找到我们想要的信息，上面的5的箭头都是我们想要的信息，分别是电影图片地址、电影名字、主演、上演时间和评分。内容有了，接下来就是获取下一页的链接。这里有两种方法，第一种就是在首页获取所有页的链接，第二种方法就是获取每个页面的下一页的链接。在这里由于只是给了部分页面的链接出来，所以我们获取的是下一页的链接，这样子方便点。好了，分析完毕，接下来代码撸起。3.敲代码什么都不管，立即来个get请求import requestsfrom bs4 import BeautifulSoupurl_start = ‘http://maoyan.com/board/4&#39;response = requests.get(url_start)if response.status_code == 200: &nbsp; &nbsp;soup = BeautifulSoup(response.text, ‘lxml’)print(response.text)输出结果：惊不惊喜，意不意外？如果你经常玩爬虫的，这个就见怪不怪了，我们被反爬了。我们试下加个请求头试试。url_start = ‘http://maoyan.com/board/4&#39;headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36’}response = requests.get(url_start, headers=headers)这样就可以正常返回了，因为一般的网站都会在请求头上加个反爬的，所以遇到了反爬也不要着急，加个请求头试试？接下来用BeautifulSoupL来获取内容imgs = soup.select(‘dd .board-img’) &nbsp;# 这是获取图片链接titles = soup.select(‘dd .board-item-main .name’) &nbsp;# 这是获取电影名字starses = soup.select(‘dd .board-item-main .movie-item-info .star’) &nbsp;# 这是获取电影主演times = soup.select(‘dd .board-item-main .movie-item-info .releasetime’) &nbsp;# 这是获取电影上映时间scores = soup.select(‘dd .board-item-main .score-num’) &nbsp;# 这是获取评分这里每个获取的语句都包含了每个不同电影的信息，这样就不能和正则那样一次把每个电影的信息都在同一个字符里面了。就比如我获取的图片，一个语句获取的是这个页面的所有电影图片的链接，我们存储的时候就要分别取出来了。这里我用到的是for循环0到9把相同的坐标的信息存进同一个字典里面。films = [] &nbsp;# 存储一个页面的所有电影信息 &nbsp; &nbsp;for x in range(0, 10): &nbsp; &nbsp; &nbsp; &nbsp;# 这个是获取属性的链接 &nbsp; &nbsp; &nbsp; &nbsp;img = imgs[x][‘data-src’] &nbsp; &nbsp; &nbsp; &nbsp;# 下面的都是获取标签的内容并去掉两端空格 &nbsp; &nbsp; &nbsp; &nbsp;title = titles[x].get_text().strip() &nbsp; &nbsp; &nbsp; &nbsp;stars = starses[x].get_text().strip()[3:] &nbsp;# 使用切片是去掉主演二字 &nbsp; &nbsp; &nbsp; &nbsp;time = times[x].get_text().strip()[5:] &nbsp;# 使用切片是去掉上映时间二字 &nbsp; &nbsp; &nbsp; &nbsp;score = scores[x].get_text().strip() &nbsp; &nbsp; &nbsp; &nbsp;film = {‘title’: title, ‘img’: img, ‘stars’: stars, ‘time’: time, ‘score’: score} &nbsp; &nbsp; &nbsp; &nbsp;films.append(film)接下来就是获取每一页的链接pages = soup.select(‘.list-pager li a’) &nbsp;# 可以看到下一页的链接在最后一个a标签 &nbsp; &nbsp;page = pages[len(pages)-1][‘href’]后面的就简单了，就是利用循环把所有页面的内容都去取出来就可以了，代码就不贴出来了。写在最后这个就是BeautifulSoup库的小练习，用到昨天的内容不多，只是用到了选择器部分和获取文本内容和属性部分，感觉还是正则比较好用点哈，我一个正则就可以获取每个电影的详细内容了，如下:&lt;dd&gt;.?board-index.?&gt;([\\d]{1,3})&lt;/i&gt;.?title=“(.?)”.?class=”star“&gt;(.?)&lt;/p&gt;.?class=”releasetime“&gt;(.?)&lt;/p&gt;.?class=”integer“&gt;(.?)&lt;/i&gt;.?class=”fraction“&gt;(.?)&lt;/i&gt;还需要用到个匹配模式哈：re.S就可以了。所以本人推荐使用正则表达式哈。需要完整代码的请查看我的github哈！github：https://github.com/SergioJune/gongzhonghao_code/blob/master/python3_spider/index.py如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf◐◑&nbsp;python爬虫常用库之BeautifulSoup详解◐◑&nbsp;老司机带你用python来爬取妹子图日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"入门mysql数据库","date":"2019-01-14T07:56:44.661Z","path":"2019/01/14/入门mysql数据库/","text":"上篇文章简单说了下数据库，并教了怎样安装mysql工具和可视化工具，不知道你现在安装好了没？那今天我们继续说mysql，今天说下mysql语句。进入正题mysql语句分为两种，一种是DDL，就是数据定义语句，比如那些CREATE,ALTER。另一种是DML，就是数据管理语句，比如增删改查，我所说的就是DML，那么开始吧！1创建并使用数据库我们先打开navicat可视化管理工具，连接好我们的本地数据库然后点击查询，进行写我们的mysql语句那么正式开始创建数据库并使用这里我创建了个school的数据库，并使用了他。运行语句可以直接点上面的运行，也可以这样注意：那个数据库的引号是个反引号，在tab键的上方，还有mysql的注释是两个划线再加一个空格。语句结束后最好加个 ; 来结束在这里我们也可以看看我们已有的数据库上面成功创建后你将会在左侧看到这个数据库，这样就成功了。2进行建表上面创建了一个student的表格，并且有id，name，sex，age，address和in_school这几个栏位，其中id是自增的，用了AUTO_INCREMENT来声明的。NOT NULL 就是代表该栏位不能为空，NULL就是相反的。而INT,CHAR这些是数据类型，mysql的数据类型有int,char,varchar(括号里的是自定义长度),datetime等。那么，接下来点击运行，结果报了错，说我没有主键，那我们可以在id栏位加上主键声明再次运行，就成功了，你点开数据库，你会看到这个3进行增删改查操作1).往数据库添加数据格式为：INSERT INTO tablename VALUE(v1,v2,v3,…),如下：这是往students表中添加数据，VALUE后的括号填的值需要对应刚才你创建表时候的栏位，要不会报错。但是如果每个都需要这样岂不是很麻烦？不怕。若是只想填几个栏位，也可以这样填这里是在表名加个你需要填的栏位即可，后面的数据也要记得一 一对应，还有的就是，那些是NOT NULL 的必须要填，否则也会报错。添加了数据之后效果如下图还有一个问题就是，可不可以一次性添加多个数据？答案是肯定可以。想一次性添加多个数据时，只需要把VALUE改成VALUES即可，然后每个数据用逗号隔开，如下：2).查询表格中的数据最简单的格式为：SELECT data FROM tablename查询students表格的所有数据* 是代表所有数据，实际开发中并不会这样查询，因为会影响效率。我们可以指定某一栏位进行查询，如下这样代表只查name和sex这两个栏位的。我们还可以按条件来进行查询，比如只查询性别为女的，嘻嘻这样就查到了性别为女的数据，可以看到，WHERE后面跟的就是条件我们还可以将结果进行排序这里可以看到他是按照id来倒序来排的，ORDER BY 后面跟的是排序条件，而那个DESC是倒序，默认是顺序，为ASC。查询还不止这些，比如还可以指定偏移数来查询多少数据，如下这里是查询年龄大于19岁的学生，并且按照id来降序来排，查询了第一页的3条数据。其中LIMIT就是指定查询多少页的多少数据。上述的 1 代表的是偏移数， 而 3 就是代表数据数，所有表示的是偏移一个数据来查下面的三个数据。其实，查询是有一条很长的表达式的，如下：SELECT data FROM tablename WHERE condition GROUP BY con HAVING condition ORDER BY condition LIMIT offset,num我们在写查询语句时，必须按照上面的顺序，粗体字就是关键字，要不会出错，没有时可以省略。再说说上面还没有说过的几个关键字。GROUP BY:这个是分组。HAVING:这个是按条件来分组。好了，查询的就这么多。3).修改数据格式为：UPDATE tablename SET data [WHERE condition]中括号里的是可以省略，为删除的条件，省略时就为更改所有数据上面的就是修改表格students的表格，其中SET的数据是修改的数据4).删除数据格式为：DELETE FROM tablename [WHERE condition]中括号和修改数据的意思一样，也挺简单的。注意：不写WHERE时是删除该表格的所有数据这是删除students表的性别为男的数据END好了，如果你跟着我一步步做，恭喜你，学会了简单的数据库语句，现在留个练习给你们，可以检验下自己有没有学会操作mysql：1.创建一个数据库,然后设计一个新闻表(数据类型要使用合理)2.使用SQL语句向数据表写入十五条不同的数据3.使用SQL语句查询类别为“百家”的新闻数据4.使用SQL语句删除一条新闻数据5.使用SQL语句查询所有的新闻，以添加时间的倒序进行排列6.使用SQL语句查询第二页数据(每一页5条数据)学会了mysql语句，如果想学更多请去官方文档哈，这是网址https://dev.mysql.com/doc/refman/5.7/en/。在下一篇文章我将会讲述如何用python来连接操作数据库。记得来看看哈！祝大家今天情人节快乐哈，我没什么礼物送给大家，只能送点学习资料，如果需要的话可以在后天回复资源即可获得哈 ！多谢大家捧场。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://sergiojune.com/tags/mysql/"}]},{"title":"如何爬取asp动态网页？搞定可恶的动态参数，这一文告诉你！","date":"2019-01-14T07:56:44.631Z","path":"2019/01/14/如何爬取asp动态网页？搞定可恶的动态参数，这一文告诉你！/","text":"这个asp网站是我的学校的电费查询系统，需要学校的内网才能查询，所以这文说下思路和我遇到的一些坑。我搞这个网站主要是为了方便查电费而已，其实也方便不了多少。而且这个asp网站还不是很容易爬，因为里面有两个可变的参数，会根据页面来变化。好了，先看看页面这个网站需要先登陆进自己的宿舍才能进去，还有很烂的验证码，不过我实现到验证码写入的时候发现这个验证码是可以随便填的，这个就感觉有点垃圾。这个登陆页面有很多坑，下面说下1.上面右边所指的就是两个动态变化的参数，怎么来的呢？是根据上一个页面来的，每个页面都会带有这两个参数，所以我们需要每次访问一次都需要匹配下这两个值就行动态更换，如果不跟换的话，会得不到数据，还会出现下面这个错误。‘236|error|500|回发或回调参数无效。在配置中使用&nbsp;&lt;pages&nbsp;enableEventValidation=”true”/&gt;&nbsp;或在页面中使用&nbsp;&lt;%@&nbsp;Page&nbsp;EnableEventValidation=”true”&nbsp;%&gt;&nbsp;启用了事件验证。出于安全目的，此功能验证回发或回调事件的参数是否来源于最初呈现这些事件的服务器控件。如果数据有效并且是预期的，则使用 ClientScriptManager.RegisterForEventValidation 方法来注册回发或回调数据以进行验证。|’这个就说明你没有更换好上面所说的两个参数注意：第一次访问这个网站是不会有宿舍楼层宿舍号这些数据的，需要进行匹配上面的两个可变参数再进行post才会有数据。2.在你选好你的宿舍楼层号之后表单数据就会出现变化可以看到表单的参数顺序和上面的不一样了，所以在选好宿舍楼层之后我们需要把变单顺序改变后再把参数post出去，要不还会出现上面那个坑，就是回调参数无效第一个箭头所指的参数也需要改变，不过第二个参数是txtname2，也就是每层楼的默认宿舍值，这个固定也没事，不会出错，时间的话还是需要根据自己访问时间来进行变化的，要不也会出现错误，还是同样的错误，也就是下面的这个错误，可想而知asp网站对这些参数是有很挑剔的要求。236|error|500|回发或回调参数无效。在配置中使用 &lt;pages enableEventValidation=”true”/&gt; 或在页面中使用 &lt;%@ Page EnableEventValidation=”true” %&gt; 启用了事件验证。出于安全目的，此功能验证回发或回调事件的参数是否来源于最初呈现这些事件的服务器控件。如果数据有效并且是预期的，则使用 ClientScriptManager.RegisterForEventValidation 方法来注册回发或回调数据以进行验证。|3.这个电费查询按钮，不是ajax，会有新的请求，而且是对同一个网址的不同请求方式，第一次请求时get请求，用于获取asp网页的那两个动态参数，第二次是将动态参数就行post发送出去，这样就会有数据了，如果你是第一次就post的话，会没有数据，网页还是会报错误，同样还是那个错误哈。下面是表单数据self.data = {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTTARGET’:&nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$btnSelect’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTARGUMENT’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘VIEWSTATE’:&nbsp;self.data[‘VIEWSTATE’],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTVALIDATION’:&nbsp;self.data[‘EVENTVALIDATION’],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘hidJZ’:&nbsp;‘jz’+name,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$TextBox1’: (datetime.now()-timedelta(days=30)).strftime(‘%Y-%m-%d’),&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$TextBox2’: datetime.now().strftime(‘%Y-%m-%d’),&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$txtDBBH’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$Toolbar1$ContentPanel1$ddlCZFS’:&nbsp;‘—-全部—-‘,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘RegionPanel2$Region1$toolbarButtom$pagesize’:&nbsp;‘1’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘box_page_state_changed’:&nbsp;‘false’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘2_collapsed’:&nbsp;‘false’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘6_selectedRows’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘box_disabled_control_before_postbac’:&nbsp;‘10’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘box_ajax_mark’:&nbsp;‘true’&nbsp; &nbsp; &nbsp; &nbsp;}坑说完了，说说部分代码的作用吧def&nbsp;get_value(self, html):&nbsp;&nbsp;# 获取表单的两个参数VIEWSTATE和EVENTVALIDATION&nbsp; &nbsp; &nbsp; &nbsp;try:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;soup = BeautifulSoup(html,&nbsp;‘lxml’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value = soup.select(‘input[type=”hidden”]’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values = [v&nbsp;for&nbsp;v&nbsp;in&nbsp;value&nbsp;if&nbsp;‘/w’&nbsp;in&nbsp;str(v)]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;state = values[0][‘value’]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action = values[1][‘value’]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘VIEWSTATE’] = state&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘EVENTVALIDATION’] = action&nbsp; &nbsp; &nbsp; &nbsp;except&nbsp;IndexError&nbsp;as&nbsp;e: &nbsp;# 证明这个不是首页，需要另外的规则&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;match = re.search(‘VIEWSTATE|(.?)|.?EVENTVALIDATION|(.*?)|‘, html)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘VIEWSTATE’] = match.group(1)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data[‘EVENTVALIDATION’] = match.group(2)&nbsp; &nbsp; &nbsp; &nbsp;except&nbsp;Exception&nbsp;as&nbsp;e:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘get_value’, e)这个就是获取两个动态参数的方法，每次根据html元素来获取def&nbsp;get_name(self, jz, html=None): &nbsp;# 输入宿舍号&nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;html:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 表单顺序需要改变&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.data = {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘ScriptManager1’:&nbsp;‘UpdatePanel1|txtjz2’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘hidtime’: datetime.now().strftime(‘%Y-%m-%d %H:%M:%S’),&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘Radio1’:&nbsp;‘1’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtjz2’: jz,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtname2’:&nbsp;‘001001001001001’, &nbsp;# 这个初始化值可以随意，但不能为空&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtpwd2’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘txtyzm2’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTTARGET’:&nbsp;‘txtjz2’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTARGUMENT’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘LASTFOCUS’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘VIEWSTATE’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘EVENTVALIDATION’:&nbsp;‘’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘ASYNCPOST’:&nbsp;‘true’&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.get_value(html) &nbsp;# 换下参数&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;html =&nbsp;self.get_html()&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;html:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;soup = BeautifulSoup(html,&nbsp;‘lxml’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dormitory_num = soup.select(‘select[name=”txtname2”] option’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dormitory_num = [(p.text, p[‘value’])&nbsp;for&nbsp;p&nbsp;in&nbsp;dormitory_num]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for&nbsp;index, p&nbsp;in&nbsp;enumerate(dormitory_num):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(index,&nbsp;‘宿舍号：’, p[0])&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.get_value(html)&nbsp; &nbsp; &nbsp; &nbsp;while&nbsp;True:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num = input(‘请输入你的宿舍，输入左边的编号即可’)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num = re.match(‘\\d+’, num)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if&nbsp;num&nbsp;and&nbsp;int(num.group()) &lt; len(dormitory_num):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num = int(num.group())&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘请输入正确的宿舍编号’)&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;dormitory_num[num][1]这个是获取宿舍号，表单顺序需要改变def get_chapter(self):&nbsp; &nbsp; &nbsp; &nbsp;# 获取验证码&nbsp; &nbsp; &nbsp; &nbsp;url =&nbsp;‘http://172.18.2.42:8000/ValidateCode.aspx&#39;&nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url, headers=self.headers)&nbsp; &nbsp; &nbsp; &nbsp;with&nbsp;open(‘code.jpg’,&nbsp;‘wb’)&nbsp;as&nbsp;f:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(response.content)&nbsp; &nbsp; &nbsp; &nbsp;image = Image.open(‘code.jpg’)&nbsp; &nbsp; &nbsp; &nbsp;image.show()&nbsp; &nbsp; &nbsp; &nbsp;code =&nbsp;input(‘请输入验证码’)&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;code这个是获取验证码的方法，获取验证码是很简单的，就是找到请求的url进行请求就可以了。至于识别，我这里是手动输入，你也可以选择接入打码平台或者用深度学习模型来识别。其他的就不多说了。需要源码的可以在我的GitHub上找：https://github.com/SergioJune/gongzhonghao_code/blob/master/python_play/query.py写在最后如果这篇文章对你用的话，希望不要吝啬你的点赞哈！点赞和转发就是对我的最大支持，这样才有动力输出质量高的原创文章。「点赞是一种态度！」推荐文章：我爬取了37000条球迷评论，知道了这场比赛的重要信息爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"}]},{"title":"爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf","date":"2019-01-14T07:56:44.605Z","path":"2019/01/14/爬取《The Hitchhiker’s Guide to Python!》python进阶书并制成pdf/","text":"这是日常学python的第15篇原创文章前几篇文章我们学习了requests库和正则，还有个urllib库，我上篇文章也用了requests库来教大家去爬那些返回json格式的网页，挺好玩的。有读者让我来个正则的，所以我今天就来个正则+requests来进行爬取。今天原来是想爬小说的，但想到我不怎么看小说，读者也是都喜欢学习的，对吧？嘻嘻！所以我来爬个与python相关的内容，恰好前几天我又看到别人推荐的一本python进阶书，这本书的作者是我们的python大神kennethreitz征集各路爱好python的人所写的，下面是地址：中文版:http://pythonguidecn.readthedocs.io/zh/latest/英文版：http://docs.python-guide.org/en/latest/这本书适合于一切有python的学习者，不管你是初入python的小白，还是熟练使用python的老手，都适用。但是不看也没有影响你学习爬虫哈，这个只是说些python的奇淫技巧。由于这本书在网上只有英语的电子版，可我英语渣渣，所以爬个中文版的网页然后把他弄成电子版。若想直接获取该书电子版，可以在公众号「日常学python」后台回复『进阶』直接获取。本篇文章用到的工具如下：requests库正则表达式Sigil：将html网页转成epub电子书epub转pdf:http://cn.epubee.com/epub转pdf.html好了，下面详细分析：1分析网站内容可以看到首页中有整本书的内容链接，所以可以直接爬首页获取整本书的链接。熟练地按下f12查看网页请求，非常容易找到这个请求网站为：http://pythonguidecn.readthedocs.io/zh/latest/请求方式为get，状态码为200，而且返回的是html元素，所以我们可以用正则来匹配所需要的内容。那看看我们的匹配内容所在的地方可以看到这个内容的地址和内容标题都在这个a标签上，所以正则很容易，如下：toctree-l1.?reference internal“ href=”([^“]?)”&gt;(.?)&lt;/a&gt;不知道你的正则学得怎样了，这里还是简单说下：.：这个是概括字符集，为匹配除换行符以外的任意字符：这个是数量词，匹配的次数为0次以上?：加了这个问号表示非贪婪，一般默认为贪婪[^”]：这个表示不匹配双引号，挺好用的实在不记得的可以看看我这篇文章，这里不详细说了,不记得就点开爬虫必学知识之正则表达式下篇看看这里需要注意的是：在这里获取的网址列表里面有个内容的导航，如下：所有我们在匹配完之后还需要再将这些带#号的网址给过滤掉。接下来的就是获取每个网页的内容可以看到内容都在这个div标签内，所以和上面一样，用正则就可以获取了。ps: 其实这里用BeautifulSoup更好用，我会在后面文章中讲到哈！匹配内容的正则为：section“.?(&lt;h1&gt;.?)&lt;div class=”sphinxsidebar因为我的那个工具是把这些内容的html下载下来就可以了，所以接下来不需要清洗里面的html元素。内容分析完毕，接下来的就容易了，就是用个循环把遍历所有文章，然后就利用正则把他爬下来就可以了。2实操部分import re, requestsclass Spider(object): &nbsp; &nbsp;def init(self, headers, url): &nbsp; &nbsp; &nbsp; &nbsp;self.headers = headers &nbsp; &nbsp; &nbsp; &nbsp;self.url = url &nbsp; &nbsp;def get_hrefs(self): &nbsp; &nbsp; &nbsp; &nbsp;‘’’获取书本的所有链接’’’ &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(self.url, self.headers) &nbsp; &nbsp; &nbsp; &nbsp;if response.status_code == 200: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response.encoding = ‘utf-8’ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hrefs = re.findall(‘toctree-l1.?reference internal” href=”([^”]?)”&gt;(.*?)&lt;/a&gt;’, response.text, re.S) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return hrefs &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘访问书本内容失败，状态码为’, response.status_code) &nbsp; &nbsp;def get_page(self, url): &nbsp; &nbsp; &nbsp; &nbsp;‘’’获取首页’’’ &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(url, self.headers) &nbsp; &nbsp; &nbsp; &nbsp;response.encoding = ‘utf-8’ &nbsp; &nbsp; &nbsp; &nbsp;content = re.findall(‘section”.?(&lt;h1&gt;.?)&lt;div class=”sphinxsidebar’, response.text, re.S) &nbsp; &nbsp; &nbsp; &nbsp;return content[0] &nbsp; &nbsp;def get_content(self, href): &nbsp; &nbsp; &nbsp; &nbsp;‘’’获取每个页面的内容’’’ &nbsp; &nbsp; &nbsp; &nbsp;if href: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;href = self.url + href &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = requests.get(href, self.headers) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response.encoding = ‘utf-8’ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;content = re.findall(‘section”.?(&lt;h1&gt;.?)&lt;div class=”sphinxsidebar’, response.text, re.S) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if content: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return content[0] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘正则获取失败’) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘获取内容失败’) &nbsp; &nbsp;def run(self): &nbsp; &nbsp; &nbsp; &nbsp;‘’’循环获取整本书内容’’’ &nbsp; &nbsp; &nbsp; &nbsp;self.num = 0 &nbsp; &nbsp; &nbsp; &nbsp;hrefs = self.get_hrefs() &nbsp; &nbsp; &nbsp; &nbsp;content = self.get_page(self.url) &nbsp; &nbsp; &nbsp; &nbsp;with open(str(self.num)+‘Python最佳实践指南.html’, ‘w’, encoding=‘utf-8’) as f: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(content) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘写入目录成功’) &nbsp; &nbsp; &nbsp; &nbsp;for href, title in hrefs: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if “#” in href: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;continue &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.num += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;content = self.get_content(href) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with open(str(self.num)+title+‘.html’, ‘w’, encoding=‘utf-8’) as f: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f.write(content) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载第’+str(self.num)+‘章成功’) &nbsp; &nbsp; &nbsp; &nbsp;print(‘下载完毕’)def main(): &nbsp; &nbsp;url = ‘http://pythonguidecn.readthedocs.io/zh/latest/&#39; &nbsp; &nbsp;headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36’} &nbsp; &nbsp;spider = Spider(headers, url) &nbsp; &nbsp;spider.run()if name == ‘main‘: &nbsp; &nbsp;main()点击运行，感觉美滋滋，可惜啊，代码总是爱玩弄你，赶紧报了个错： &nbsp;File “E:/anaconda/python_project/newspaper/spider.py”, line 52, in run &nbsp; &nbsp;with open(str(self.num)+title+‘.html’, ‘w’, encoding=‘utf-8’) as f:FileNotFoundError: [Errno 2] No such file or directory: ‘38与C/C++库交互.html’一眼看下去，还挺郁闷的，我没有打开文件的，都是在写文件，为什么报了这个错？仔细一看报错内容，这个名字有问题啊，你看38与C/C++库交互.html这个在window系统是以为你在&nbsp;38与C&nbsp;的&nbsp;C++库交互.html&nbsp;下的，怪不得会报错，所以，我在这里加了这个代码把/给替换掉3把内容整成pdf点击Sigil 的&nbsp;+&nbsp;号把刚才下载的内容导入生成目录添加书名作者添加封面：点击左上角的&nbsp;工具&nbsp;-&gt;&nbsp;添加封面&nbsp;即可点击保存即可完成转pdf：http://cn.epubee.com/epub转pdf.html这个很容易就不说了。结语好了，文章内容就这么多，下个文章就是学习新内容了。期待ing。上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑老司机带你用python来爬取妹子图◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;爬虫必学知识之正则表达式下篇日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"requestst","slug":"requestst","permalink":"http://sergiojune.com/tags/requestst/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://sergiojune.com/tags/正则表达式/"}]},{"title":"爬虫必学知识之正则表达式下篇","date":"2019-01-14T07:56:44.578Z","path":"2019/01/14/爬虫必学知识之正则表达式下篇/","text":"这是日常学python的第13篇原创文章继上篇文章说了正则表达式的简单用法，那今天我们就继续说一下正则表达式的复杂的用法。好了，废话不多说，直接进入正题。正则表达式情景：当你想要匹配一个qq号，qq号码长度为5-10位，那根据上篇文章的说法，很容易就可以想到该正则：[0-9]{5,10}这样是可以的，但是当你匹配一个长度大于10的号码时就会出错，这时就会去该字符串的前10个数字出来，如下：import rea=‘221753259265’r=re.findall(‘[0-9]{5,10}’,a)#明显当查找的字符串长度大于8位时就会出错，只会截取前一部分长度print(r)# 结果[‘2217532592’]这样的话你就会得到一个错误的qq号码。这时就需要引入边界匹配了：^：这个是从左边开始匹配，规定左边的首个字符$：这个是从右边开始匹配，规定右边的首个字母现在再写个匹配qq号码的正则r=re.findall(‘^[0-9]{5,10}$’,a)#这个表示从左边起为5-10的数字长度，右边也是一样print(‘第一个匹配结果：’,r)a = ‘2217532592’r=re.findall(‘^[0-9]{5,10}$’,a)print(‘第二个匹配结果：’,r)# 结果第一个匹配结果： []第二个匹配结果： [‘2217532592’]这样就可以匹配到了，是不是很神奇？组：前面我们有用 [ ] 来匹配，中括号里面表示的是或关系，而这里的组表示的是并关系，并且用小括号括起来 ( )。比如：重复 python 字样三次import rea=‘pythonpythonpythonjakjpythonpythonsdjjpythonpythonpythonsd’r=re.findall(‘(python){3}’,a)print(r)# 结果[‘python’, ‘python’]这里的结果不是返回三个python，而是返回这个组，当符合一次就会将此组添加到返回列表中一次。这个组还挺好用的，再看下这个需求：获取下列英文中的life和python之间的内容。a=‘life is short,i use python’r=re.findall(‘life(.)python’,a,re.S)print(r) # 这样获取的就是组内的内容# 结果[‘ is short,i use ‘]这个组还常用，因为在我们经常在用正则来解析html元素时，经常需要获取两个标签之间的内容，标签是确定的，标签内容不确定，就可以用这个了。如下这个html元素：&lt;strong&gt;&lt;a href=“#py2”&gt;python进阶 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“#python3”&gt;python入门 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“#vce”&gt;vce解决方法 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“demo06.html#new” target=“_blank”&gt;百度 &lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“mailto: 2217532592@qq.com“&gt;反馈意见&lt;/a&gt; &nbsp; &nbsp; &nbsp; &nbsp;&lt;a href=“img/1.jpg”&gt;下载图片 &lt;/a&gt; &nbsp; &nbsp;&lt;/strong&gt;这样就可以用组来获取a标签的内容了：&lt;a .?&gt;(.?)&lt;/a&gt;。？表示非贪婪哦！re.findall(pattern,string,flags)：这个方法的前两个参数对你们来说都很熟悉了，第一个参数为正则表达式，第二个参数为要进行匹配的字符串，而第三个可选参数为匹配模式，有如下几种匹配模式：re.I(re.IGNORECASE) ：使匹配对大小写不敏感re.L(re.LOCAL)：做本地化识别（locale-aware）匹配re.M(re.MULTILINE)：多行匹配，影响 ^ 和 $re.S(re.DOTALL)：使 . 匹配包括换行在内的所有字符(这个常用)re.U(re.UNICODE)：根据Unicode字符集解析字符。这个标志影响 \\w, \\W, \\b, \\B.re.X(re.VERBOSE)：该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解当需要写多个匹配模式时，可以用 | 分隔每个模式代码如下：a=‘Java12Python89’r=re.findall(‘python’,a,re.I)print(r)a=‘hsjhj h123jfkksf hajkGH\\nkj fjfk’r=re.findall(‘.’,a,re.I|re.S)print(r)# 结果[‘Python’][‘h’, ‘s’, ‘j’, ‘h’, ‘j’, ‘ ‘, ‘h’, ‘1’, ‘2’, ‘3’, ‘j’, ‘f’, ‘k’, ‘k’, ‘s’, ‘f’, ‘ ‘, ‘h’, ‘a’, ‘j’, ‘k’, ‘G’, ‘H’, ‘\\n’, ‘k’, ‘j’, ‘ ‘, ‘f’, ‘j’, ‘f’, ‘k’]正则除了可以用来检索字符串，还可以用来替换字符串，常见的可以用来替换那些文本中的空格，制表符和回车等，这些都是用一个正则就可以搞定的了。python中用这个方法来进行正则替换re.sub(pattern,&nbsp;repl,&nbsp;string,&nbsp;count=0,&nbsp;flags=0)&nbsp;pattern ：正则表达式repl ：替换后的字符串，可为函数string ：要进行替换的字符串count ：替换的次数，顺序为 从左往右，默认值为0，表示无限次。falgs ： 匹配模式，和findall()差不多代码如下：import rea=‘skjC#ksjfc#jkdsc#’r=re.sub(‘c#’,‘gg’,a)#返回值是替换后的字符串print(r)print(a)r=re.sub(‘c#’,‘gg’,a,1) # 这个加了替换次数print(r)r=re.sub(‘c#’,‘gg’,a,1,re.I) # 加了匹配模式，忽视大小写print(r)# 结果skjC#ksjfggjkdsggskjC#ksjfggjkdsc#skjggksjfc#jkdsc#我们试试第二个参数为函数的情况def convert(value):#他是把对象传进去这个参数 &nbsp; &nbsp;print(value) &nbsp; &nbsp;#可以通过group()方法来获取内容 &nbsp; &nbsp;return ‘!!’+value.group()+“!!”r=re.sub(‘c#’,convert,a,flags=re.I)#接收个参数后，更改后的内容为他的返回值print(r)# 结果&lt;_sre.SRE_Match object; span=(3, 5), match=‘C#’&gt;&lt;_sre.SRE_Match object; span=(9, 11), match=‘c#’&gt;&lt;_sre.SRE_Match object; span=(15, 17), match=‘c#’&gt;skj!!C#!!ksjf!!c#!!jkds!!c#!!这个第二个参数为convert函数，里面的.group() 方法是获取匹配后的字符串的值，所以我们就可以根据匹配后的字符串来进行相对应的替换内容，比如这个简单的小需求：把字符串中的数字大于50的改为99，小于的就改为11。a=‘ds+45sd78asd12568asd45asd74ew+9ddf12sd45’def func(value): &nbsp; &nbsp;if int(value.group())&gt;50: &nbsp; &nbsp; &nbsp; &nbsp;return ‘99’ &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp;return ‘11’r=re.sub(‘\\d{1,2}’,func,a)print(r)# 结果ds+11sd99asd119911asd11asd99ew+11ddf11sd11另谈两个函数re.match(pattern,string,flags) ：这个是从字符串的首个字母开始匹配，若首个字母不符合，就会返回None, 反之返回一个 Match对象。而他只会匹配第一个结果，不会返回所有符合结果的内容。参数内容与findall()方法一样。re.search(pattern,string,flags) ：这个与match方法差不多，不过不是从首字符开始匹配，也是只返回一个正确的匹配内容。代码：import rea=‘pythonphpjavacphp’r=re.match(‘php’,a)#这个一开始没有就返回Noneprint(r)r=re.search(‘php’,a)#这个搜索到之后就返回一个对象#返回的对象可以通过group()方法来获取他的内容print(r)# 获取匹配内容print(r.group())# 结果None&lt;_sre.SRE_Match object; span=(6, 9), match=‘php’&gt;php这两个函数返回的内容的几个属性：group() ：获取匹配的内容statr() ：获取到匹配字符的起始位置end() ：获取匹配到字符的结束位置span() ：获取匹配到字符的起始和结束位置，元组形式返回。前面提到组的概念，试下这两个方法的组的用法：import re#获取life和python之间的内容a=‘life is short,i use python’r=re.search(‘life(.)python’,a,re.S)#用小括号的就是一组print(r.group(1))#这个下标1就是对应的中间部分#也可以获取中间的两部分a=‘javawoshipythonjunephp’r=re.search(‘java(.)python(.)php’,a)#两个小括号就是分成了两组print(r.group(1),r.group(2))#分别打印第一第二组print(r.groups())#这个获取所有分组信息# 结果 is short,i use woshi june(‘woshi’, ‘june’)上面的代码注释已经很清楚了，还有个group()方法是获取整个正则匹配的内容，不按分组。match()方法也一样，就不演示了。最后一个问题：怎样拆分含有多种分隔符的字符串？比如：kfs;hsji’fhsikfbhsfk=jsf/shj。要将不属于字母的都去掉，你是不是会想到用字符串的循环，然后再一个一个分割出来？我告诉你，学了正则之后，再也不用这么麻烦了。re库里面有个split()方法，如下：re.split(pattern, string, maxsplit=0)，参数看名字应该就能知道。直接一行代码进行分割：a = ‘kfs;hsjifhsikfbhsfk=jsf/shj’r = re.split(‘[;*=/]’, a)print(r)# 结果[‘kfs’, ‘hsjifhsikf’, ‘bhsfk’, ‘jsf’, ‘shj’]是不是很完美？所以说正则必须得学！END这个正则复杂点的已经说完了，还有些进阶的，不过暂时没有用到，就不打算说了，需要的可以去百度看看哈！留个小练习证明自己正则学得好怎么样：1.kevintian126@126.com&nbsp;2. 1136667341@qq.com&nbsp;3. meiya@cn-meiya.com&nbsp;4. wq901200@hotmail.com&nbsp;5. meiyahr@163.com6.&nbsp;meiyuan@0757info.com&nbsp;7. chingpeplo@sina.com&nbsp;8. tony@erene.com.com9. melodylu@buynow.com用正则把上面的@与com之间的内容匹配出来，可以把你的答案写在留言区上，过两天在留言区公布答案哈！上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑爬虫必学知识之正则表达式上篇◐◑&nbsp;python爬虫常用库之requests详解◐◑&nbsp;python使用requests+re简单入门爬虫日常学python代码不止bug，还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://sergiojune.com/tags/正则表达式/"}]},{"title":"爬虫必学知识之正则表达式上篇","date":"2019-01-14T07:56:44.561Z","path":"2019/01/14/爬虫必学知识之正则表达式上篇/","text":"这是日常学python的第12篇文章在向网页进行了提交请求之类的之后，我们可以得到了网页的返回内容，里面自然而然会有我们想要的数据，但是html元素文本这么多，我们不可能一 一去找我们需要的数据，这时就需要用到正则表达式了，正则表达式是学爬虫必须学的内容，而且不止python可以用，java等其他语言都可以用，所以学了好处大大。什么是正则表达式？正则表达式就是一个特殊的字符序列，可以用于检测一个字符串是否与我们的所设定的字符串相匹配。功能有快速检索文本和快速替换一些文本的操作。python里面有个处理正则表达式的库 re。有个方法findall(pattern,string,flags)用来匹配正则达式，我们就先用这个方法处理下。参数如下：pattern：正则表达式string：要进行匹配的字符串flags：匹配的模式结果是一个匹配内容的列表‘\\d’ : 这个表示匹配单个0-9数字‘\\D’ : 与\\d相反，匹配数字以外的内容用代码来感受下：import re#这个库是用来使用正则表达式的a=‘sg+sga5g45gaae3f23hgt13’r=re.findall(‘\\d’,a)#这个就是根据\\d的正则表达式来查找对应字符，其中\\d是对应0-9的数字#查找非数字用\\Dr1=re.findall(‘\\D’,a)print(r)print(r1)#结果[‘5’, ‘4’, ‘5’, ‘3’, ‘2’, ‘3’, ‘1’, ‘3’][‘s’, ‘g’, ‘+’, ‘s’, ‘g’, ‘a’, ‘g’, ‘g’, ‘a’, ‘a’, ‘e’, ‘f’, ‘h’, ‘g’, ‘t’]可以看到找出了字符串中的数字和非数字如果我们在匹配一个字符串时，中间内容是有多个变化的，我们需要应变多种不同的字符，如这个字符串a=‘abc,acc,agc,anc,afc,adc,aec’如果需要匹配这个字符串时，我们就需要用到&nbsp;[ ]&nbsp;,用中括号括起来的字符，里面的内容表示或关系，那来看看代码import rea=‘abc,acc,agc,anc,afc,adc,aec’#现在查找上面的中间字符为c或者f的字符串#这个要求可以用到字符集来实现r=re.findall(‘a[cf]c’,a)#[]表示字符集，里面的内容是或关系# 结果[‘acc’, ‘afc’]上面匹配了中间字符是c或者是f的字符串，匹配中间字符非c和非f,可以在前面加个 ^ 符号import rer=re.findall(‘a[……cf]c’,a)#[c-f]就是表示从c到fprint(r)# 结果[‘abc’, ‘agc’, ‘anc’, ‘adc’, ‘aec’]上面只处理了中间字符为chu者f的字符串，但是没有匹配全部的，若要匹配全部，可以加个 - 符号，表示范围，如下import rer=re.findall(‘a[b-n]c’,a)#[c-f]就是表示从c到fprint(r)# 结果[‘abc’, ‘acc’, ‘agc’, ‘anc’, ‘afc’, ‘adc’, ‘aec’][b-n]:这个就是表示b到n的字符匹配汉字：[\\u4E00-\\u9FA5]概括字符集：用一个 \\ + 字母&nbsp;表示一系列的字符的元字符，只能匹配单个字符，常用的如下\\w：匹配数字和字符「不包括&amp;符号」只匹配单词，数字和下划线\\W：与\\w相反，这个包括空格和回车\\s：匹配空格字符，如空格，回车和制表符\\S：与\\s相反.：匹配除换行符之外的其他字符还有前面的\\d和\\D也是有个小技巧：如果想要匹配所有字符，就可以把上面的两个相反的合并起来就可以了。代码如下：#概括字符集,就是用一个\\加个字母来表示一类字符，比如刚开始的\\d,\\Dimport rea=‘hdsk\\n122\\rs3$ dkl%df36\\t5&amp;’r=re.findall(‘\\w’,a)#这个是匹配数字和单词print(r)#也可以匹配非数字非单词r=re.findall(‘\\W’,a)print(r)#匹配空格字符和制表符等其他字符r=re.findall(‘\\s’,a)print(r)#匹配除换行符之外的其他字符r=re.findall(‘.’,a)print(r)# 结果[‘h’, ‘d’, ‘s’, ‘k’, ‘1’, ‘2’, ‘2’, ‘s’, ‘3’, ‘d’, ‘k’, ‘l’, ‘d’, ‘f’, ‘3’, ‘6’, ‘5’][‘\\n’, ‘\\r’, ‘$’, ‘ ‘, ‘%’, ‘\\t’, ‘&amp;’][‘\\n’, ‘\\r’, ‘ ‘, ‘\\t’][‘h’, ‘d’, ‘s’, ‘k’, ‘1’, ‘2’, ‘2’, ‘\\r’, ‘s’, ‘3’, ‘$’, ‘ ‘, ‘d’, ‘k’, ‘l’, ‘%’, ‘d’, ‘f’, ‘3’, ‘6’, ‘\\t’, ‘5’, ‘&amp;’]数量词：当一个字符需要连续重复匹配多次时，就要用到这个。如匹配三个字符组成的字符串：[a-zA-z]{3}&nbsp;，大括号里面的表示重复次数。若要匹配三到六个字符，大括号的就需要这样写：{3,6}.代码如下;#数量词，当一个字符需要多次重复匹配时就需要用到import rea=‘python java111php23 html’r=re.findall(‘[a-z]{3}’,a)#重复多次就用大括号，括号内的数表示重复的次数print(r)#也可以重复一个范围，表示匹配3到6个字符r=re.findall(‘[a-z]{3,6}’,a)print(r)#这样就可以把单词都找出来了# 结果[‘pyt’, ‘hon’, ‘jav’, ‘php’, ‘htm’][‘python’, ‘java’, ‘php’, ‘html’]其他数量词表示： ：匹配零次或无限多次+：匹配一次或以上?：匹配零次或者一次a=‘pytho243python34pythonn’#表示匹配对应内容0次或者无限次r=re.findall(‘python*’,a)#这个就是代表对n字符的数量词匹配print(r)#+表示匹配内容1次或者无限次r=re.findall(‘python+’,a)print(r)#?表示可以匹配0次或者1次,注意这个？和上面的非贪婪代表的意思不一样r=re.findall(‘python?’,a)print(r)# 结果[‘pytho’, ‘python’, ‘pythonn’][‘python’, ‘pythonn’][‘pytho’, ‘python’, ‘python’]贪婪匹配：正则表达式默认为贪婪匹配，即匹配符合字符串的最大长度，如上面的[a-zA-z]{3,6}，他会趋于匹配长度为6的字符串，匹配到条件不满足时才停止匹配。非贪婪匹配：就是趋于匹配长度最小的字符串，匹配满足第一个条件就会停止匹配r=re.findall(‘[a-z]{3,6}’,a)# 贪婪匹配print(r)r=re.findall(‘[a-z]{3,6}?’,a)print(r)#由于是非贪婪，所以匹配当第一个条件满足时就停止匹配# 结果[‘python’, ‘java’, ‘php’, ‘html’][‘pyt’, ‘hon’, ‘jav’, ‘php’, ‘htm’]END这篇文章只是介绍了下正则表达式的简单用法，可以用来入门正则，下一篇文章讲正则表达式高级点的用法。留个小练习：写一个正则来匹配生日，字符串为：2005-06-092005-6-92005 6 92005，06，09可以把答案写在留言区哈！上述文章如有错误欢迎在留言区指出，如果这篇文章对你有用，点个赞，转个发如何？MORE延伸阅读◐◑python爬虫常用库之requests详解◐◑&nbsp;python使用requests+re简单入门爬虫◐◑python爬虫常用库之urllib详解日常学python代码不止bug,还有美和乐趣","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://sergiojune.com/tags/正则表达式/"}]},{"title":"随便说点事情","date":"2019-01-14T07:43:14.176Z","path":"2019/01/14/随便说点事情/","text":"随便说点事情吧。最近在学习c语言，算是对以前知识的复习，但是学习过程中并不是很想学，总是想着python，有时真的很怀疑自己是不是真的选择错了，好好的python转去学习c/c++。可能因为我自己有个gh吧，毕竟涉及到钱的问题就是很困扰，还是解脱不了对钱的束缚。 不过我是真的想学好底层基础，就是看着太无聊了，不肯花时间去慢慢学习，想把这些写下来，看看能不能让自己静下心来。 现在算是体会到了什么是孤独吧，一个人学习，一个人敲代码，很多人都没有共同语言，迷茫了又不敢去打扰别人，只好跑来自己的博客去倾诉。对啊，我还有自己的博客，你想说什么都可以，他都可以倾听！所以以后有事没事都来这里发发牢骚了，毕竟这是我的自留地。 今天终于把游戏给卸载了，自从我开了个gh之后为了钱好久没有认真学习了，我现在还在打算是否要继续发下去，转载文章也是需要精力，也不想随便转载而浪费了读者的阅读时间，这样子对不起读者，更对不起我自己。是时候做个权衡了。 以后一周发两篇即可，其他的不管了，随意而为，发技术或者自己感想都可以吧。 我这个博客需要更新了，好久之前就想更新了，一直没更新，还是自己的行动力不足啊，或者也是对自己的恐惧感不足。想起辉哥的一句话：任何行动力不足的都源自于你不够恐惧。足够恐惧，你就会行动。现在意识到自己处于一个很危险的处境也是很难的，很多时候自己都太过于骄傲了，但是自己其实很渣，这个坏毛病总是改不了。希望以后能改掉吧。 还有自己当初在家里想的那些目标，好像都没有实现到，一个月还剩两天就过了，目标一个都没有实现，不知道问题出在哪里了。自己买的书总是在第一周保持新鲜感，再过几天的话就越来越不想看了，这也是个大问题，得解决！可是自己没有办法，想了想，可能是自己没有那个「延迟满足感」吧，总想着做一件事，今天做完明天就能有效果，殊不知，效果这个东西只会在你不经意间才能体现出来，它并不会特意表现出来给你看的。还是马云说得好「今天很残酷，明天更残酷，后天会很美好，但大多数人倒在了明天晚上」。可能我就是那个大多数人吧，但我不甘心，我是要成为那小部分人的，我道理都挺那么多了，那我就得过好这一生，管理好自己。 这个博客从明天开始更新了，需要把我的笔记的都放上去了。 但是都是随意写的，自己看得懂就好，尽量也让大家都看得懂。 以上内容发自一个对自己游戏感到失望，对人生感到迷茫的渣渣，希望说出来能过得好点。","tags":[{"name":"胡思乱想","slug":"胡思乱想","permalink":"http://sergiojune.com/tags/胡思乱想/"}]},{"title":"数据结构的时间复杂度分析","date":"2019-01-14T07:43:14.172Z","path":"2019/01/14/数据结构的时间复杂度分析/","text":"1.算法定义算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作。 通俗来说，数据结构是指一组数据的存储结构，而算法就是操作数据的一组方法。就好比图书馆藏书籍，为了方便查找，通常都会分门别类地整理放在特定的地方，这些书籍的结构就是存储结构。当我们需要查找书的时候，我们是要按类别来查找呢，还是一本一本地找？或者按书本编号来查找？这个就是对数据存储的操作。所以说这些查找的方法都是算法。 算法的五个基本特性： 输入：每个算法都有零个或多个输入 输出：算法一定会有输出，要不这个算法毫无意义 有穷性：指算法在执行有限的步骤后能自动结束，不会陷入死循环。并且每个步骤都需要在可接受的时间内完成 确定性：算法的每个步骤都需要有明确的意义 可行性：算法的每一步都能够通过执行有限次数完成对算法设计的五个要求 正确性：指算法至少应该具有输入、输出和加工处理无歧义性、能正确反映问题的需求、能够得到问题的正确答案 可读性：设计的算法便于阅读、理解和交流 健壮性：当输入非法数据时，算法也能做出相应的处理，而不是产生异常或者有莫名其妙的结果 时间效率高：也就是等等下面详细说的时间复杂度要小。 存储量低：就是空间复杂度，追求时间少的同时，也尽可能追求需要消耗的内存少点。 2.度量算法的执行时间 事后统计方法就是使用大量的测试数据进行对算法的测试，进而对不同的算法运行时间进行比较来确定算法效率的高低。 这样子并不好，一是测试结果非常依赖于测试环境，二是测试结果需要大规模的数据进行测试，非常浪费人力物力。非常不推荐使用这种方法来进行度量。 大 O 复杂度的表示法通过对算法步骤实现代码的运行次数来度量算法的执行时间，随着代码的运行次数越少，其执行时间必定会越少，就比如下面这个例子： 123456789101112131415# include&lt;stdio.h&gt;# include&lt;stdlib.h&gt;int main(void)&#123; int sum = 0; int i; for(i = 0; i &lt;= 100; i++) &#123; sum = sum + i; &#125; printf(&quot;sum=%d\\n&quot;, sum); system(&quot;pause&quot;); return 0;&#125; 如上图所示，这段代码的时间复杂度就是 2n+ 5，当 n 很大时，这个公式中的低阶、常量和系数都是可以忽略不计的，也就是说上面程序的复杂度就是 n，记作 O(n)。 3.时间复杂度的分析只关注循环执行次数最多的一段代码因为大 O 这种复杂度分析只关注最大阶的量级，会忽略掉低阶、常量和系数，所以在对时间复杂度的分析时，可以直接对循环次数最多的代码进行分析即可。 1234567int i = 1;while(i &lt; 100)&#123; i = i * 2;&#125;system(&quot;pause&quot;);return 0; 根据方法我们只看循环部分即可，这个的分析方法可以先设 x 为代码执行步数，当 i 大于等于 100 时会退出循环，就是有 2 的 x 次方 等于 n(100)，可以推得 x = log2n ,最后去掉常量，一般写成 O(logn). 加法法则就是总复杂度等于量级最大得那段代码的复杂度 123456789101112131415161718192021int i, j;int sum = 0;for(i = 0; i &lt; 100; i++)&#123; sum = sum + i;&#125;for(j = 0; j &lt; 1000; j++)&#123; sum = sum + j;&#125;for(i = 0; i &lt; 1000; i++)&#123; for(j = 0; j &lt; 1000; j++) &#123; sum = sum + i; &#125;&#125;system(&quot;pause&quot;);return 0; 这个前面的两个循环都不如第三个嵌套循环量级大，就可以直接分析第三个循环的时间复杂度即可，所以该时间复杂度就是 O(n2) 3.乘法法则嵌套代码的时间复杂度等于嵌套内外代码复杂度的乘积。 嵌套可以是循环嵌套，也可以是函数内部嵌套另一个函数。 他们的时间复杂度都是外部嵌套的时间复杂度 乘于 内部嵌套代码的时间复杂度。 12345678910111213141516171819int f1(int n)&#123; int i = 0; int sum = 0; for(; i &lt; n; i ++) &#123; sum = sum + f2(i); &#125; return sum;&#125;int f2(int n)&#123; int i = 0; int sum = 0; while(i &lt; n); sum = sum * 2; return sum;&#125; 上面的 f1 函数的时间复杂度就是等于 f1的循环代码的时间复杂度 乘于 f2 的时间复杂度， 也就是 O(n*logn) 4.常见的时间复杂度 常数阶O(1), 对数阶O(log n), 3线性阶O(n), 线性对数阶O(n log n), 平方阶O(n^2)， 立方阶O(n^3) k次方阶O(n^K), 指数阶O(2^n)。消耗时间从小到大排序为： O(1) &lt; (log n) &lt; (n) &lt; O(n log n) &lt; (n^2) &lt; O(n^3) &lt; O(n^3) &lt; O(n^K) &lt; O(2^n)","tags":[{"name":"时间复杂度","slug":"时间复杂度","permalink":"http://sergiojune.com/tags/时间复杂度/"}]},{"title":"数据结构之栈的理解及实现","date":"2019-01-14T07:43:14.168Z","path":"2019/01/14/数据结构之线性栈的理解及实现/","text":"1.栈的理解栈是一种只允许从一端插入或者删除数据的结构，就比如你向弹匣装子弹，只能在头部装，在打枪的时候，也是在头部的子弹先发射出去，简单地说就是后进先出。 栈的插入操作叫做进栈，也称压栈、入栈。栈的删除操作叫做出栈，也叫弹栈。 2.栈的应用我们在浏览网页的时候，都会有个后退的按钮，这个就是用栈来实现的，还有就是大部分软件的可撤销操作，也都是用栈实现的。 3.栈的结构需要实现的方法结构： 需要声明栈的容量以及当前栈的长度（就是下一个元素插入的位置或者当前栈顶的元素位置） 方法： InitStack(*s): 初始化操作，建立一个空栈。 DestoryStack(*s): 若栈存在，就销毁栈 ClearStaack(*s): 清空栈 StackEmpty(*s): 判断栈是否为空，若为空，返回true GetTop(*s, e): 若栈顶存在，就用 e 来返回栈定元素 Push(*s， e): 向栈顶插入元素，并用 e 返回栈顶元素 Pop(*s, e): 若站栈顶存在，就删除栈顶元素并赋值给 e 返回 StackLength(): 获取栈的长度 4.用 c 语言实现栈有空补上 5. 用 Java 语言实现栈顺序存储结构实现栈123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class ArrayStack implements Stack &#123; int length; int[] arr; public ArrayStack(int capacity)&#123; arr = new int[capacity]; length = 0; &#125; // 默认创建长度为10的数组 public ArrayStack()&#123; this(10); &#125; @Override public boolean isEmpty() &#123; return length == 0; &#125; @Override public int length() &#123; return length; &#125; @Override public int peek() &#123; if(length == 0) throw new IllegalArgumentException(&quot;不存在栈顶元素&quot;); return arr[length - 1]; &#125; @Override public int pop() &#123; if(length == 0) throw new IllegalArgumentException(&quot;出栈失败，栈顶为空&quot;); int e = arr[length - 1]; length --; // 当栈长度小于当前容量的4倍时就进行数据搬移，防止占用大量空间 // 这个时候也要判断 arr.length / 2 != 0，因为不能创建长度为0的数组 // 4倍才进行缩小是因为防止时间复杂度进行了震荡 if(length == arr.length / 4 &amp;&amp; arr.length / 2 != 0) resize(arr.length / 2); return e; &#125; @Override public void push(int e) &#123; if(length &gt;= arr.length) resize(length * 2); // 扩容为当前两倍 arr[length] = e; length ++; &#125; // 动态扩容栈 public void resize(int capacity)&#123; int [] newArr = new int[capacity]; for(int i = 0; i &lt; length; i ++)&#123; newArr[i] = arr[i]; &#125; arr = newArr; &#125; @Override public String toString() &#123; StringBuilder res = new StringBuilder(); res.append(String.format(&quot;capacoty:%d, length:%d\\n&quot;, arr.length, length)); res.append(&quot;ArrayStack: tail [&quot;); for(int i = 0; i &lt; length; i++)&#123; res.append(arr[i]); if(i != length - 1) res.append(&quot;, &quot;); &#125; res.append(&quot;] top&quot;); return res.toString(); &#125; 栈的链式存储结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class LinkedListStack implements Stack &#123; class Node&#123; int data; Node next; public Node(int data, Node next)&#123; this.data = data; this.next = next; &#125; public Node(int data)&#123; this(data, null); &#125; &#125; int length; Node head; public LinkedListStack()&#123; length = 0; // 虚拟头节点 head = new Node(0); &#125; @Override public int length() &#123; return length; &#125; @Override public boolean isEmpty() &#123; return length == 0; &#125; @Override public void push(int e) &#123; // 直接插入链表的头部 Node node = new Node(e); node.next = head.next; head.next = node; length++; &#125; @Override public int pop() &#123; if(length == 0) throw new IllegalArgumentException(&quot;出栈失败，当前栈为空&quot;); Node node = head.next; head.next = node.next; length --; return node.data; &#125; @Override public int peek() &#123; if(length == 0) throw new IllegalArgumentException(&quot;出栈失败，当前栈为空&quot;); return head.next.data; &#125; @Override public String toString() &#123; StringBuilder res = new StringBuilder(); Node node = head.next; res.append(String.format(&quot;length: %d\\n&quot;, length)); res.append(&quot;LinkedList: &quot;); for(int i = 0; i &lt; length; i++)&#123; res.append(node.data); res.append(&quot; --&gt; &quot;); node = node.next; &#125; res.append(&quot;NULL&quot;); return res.toString(); &#125;","tags":[{"name":"栈","slug":"栈","permalink":"http://sergiojune.com/tags/栈/"}]},{"title":"数据结构之数组的基本方法实现","date":"2019-01-14T07:43:14.166Z","path":"2019/01/14/数据结构之数组的基本方法实现/","text":"1.数组介绍数组就是内存中一段连续的存储空间，并且存储的内容必须数据类型相同。数组坐标从 0 开始，因为是在 c 语言中的计算数组的地址位置时，这样计算的： arr[i]_address = base_addrress + i * sizeof(data) 如果从 1 开始，cpu 就会多执行了一次减法计算，对人类也不友好 arr[i]_address = base_addrress + (i-1) * sizeof(data) 其他语言也是和向 c 学习的，因为这样可以减轻大家的学习成本（我猜的） 2.什么时候用数组当一组数据需要经常对数据进行查询和改变元素的值，这个时候就可以使用数组，因为数组是一段连续的存储空间，这就有利于根据地址来查找某一个位置的元素或者进行改变，这两个操作都是 O(1)。 比如英雄联盟中的商店，那个装备位置都是固定的，你就可以根据地址快速找出装备来进行查看，这个就可以用数组来实现。 但是你的装备栏就不可以了，因为需要经常回家更新装备，需要进行大量的插入和删除，这个时候再用数组来存储的话，就显得效率很低了，因为这两个的时间复杂度都是 O(n)。但是另一种数据结构就非常合适，就是链表。这个以后再说。 数组的插入和删除操作也是可以通过适当改变也可以让其时间复杂度变小点的。比如在对数组进行删除时，我们可以先把需要删除的元素记录下来暂不处理，当数组的容量装不下来的时候，这个时候再触发真正的删除操作，这样子就可以大大地减少对数据的迁移以及申请内存空间的操作。 还有插入的时候我们也可以先把需要插入位置的元素(初始元素)先把其放进数组尾部，然后把元素插入该位置，然后记录下这些元素的位置，当数组容量满的时候，再进行一次数据迁移，这样也可大大减少数据迁移工作。 这两个可以说是通过空间换时间吧。 3. 基本方法数组结构：1.数组声明 2.数组当前长度 方法： boolean isEmpty()：判断数组是否为空，时间复杂度为 O(1) int get(int index)：根据 index 获取元素，时间复杂度为 O(1) set(int index, int e)：改变 index 位置的元素，时间复杂度为 O(1) int find(int e)：查找该元素是否存在在数组，存在就返回该元素的位置，反之返回 -1，时间复杂度为 O(1) void add(int index, int e)：添加元素，时间复杂度为 O(n) int remove(int index)：删除元素，时间复杂度为 O(n) 4.方法的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import java.util.Random;public class ArrayList &#123; int length; // 当前数组的长度 int[] array; // 暂时存储int类型 // 初始化数组 public ArrayList(int capacity)&#123; this.array = new int[capacity]; &#125; //空参数的就默认容量为10 public ArrayList()&#123; this(10); &#125; // 获取当前数组长度 public int getLength()&#123; return this.length; &#125; // 判断数组是否为空 public boolean isEmpty()&#123; return this.length == 0; &#125; // 根据index获取线性表的值 public int get(int index)&#123; if(index &lt; 0 || index &gt;= length)&#123; throw new IllegalArgumentException(&quot;index 不存在&quot;); &#125; return array[index]; &#125; // 改变 index 位置的元素 public int set(int index, int e)&#123; if(index &lt; 0 || index &gt;= length)&#123; throw new IllegalArgumentException(&quot;改变元素失败，index 不存在&quot;); &#125; int k = array[index]; array[index] = e; return k; &#125; // 根据所给的值查找该元素在数组的位置，不存在就返回 -1 public int find(int e)&#123;// for(int i = 0; i &lt; length; i++)&#123;// if(array[i] == e)// return i;// &#125;// return -1; // 需要先判断数组是否为空 if(length == 0) return -1; if(array[length - 1] == e) return length -1; int key = array[length - 1]; // 记录最后一个元素 array[length - 1] = e; // 当哨兵 int i = 0; while(array[i] != e) // 这个比上面的方法少了个判断，当数组过大时，效率就会有差别 i++; // 再把最后一个元素复原 array[length - 1] = key; if(i == length - 1) return -1; return i; &#125; // 向index位置插入元素 public void add(int index, int e)&#123; if(index &gt; length || index &lt; 0) throw new IllegalArgumentException(&quot;添加元素失败，index非法&quot;); if(length &gt;= array.length) throw new IllegalArgumentException(&quot;添加元素失败，容量已达到最大&quot;); for(int i = length; i &gt; index; i--)&#123; array[i] = array[i - 1]; &#125; array[index] = e; length ++; &#125; // 删除当前数组的index位置的元素 public int remove(int index)&#123; if(index &gt; length || index &lt; 0) throw new IllegalArgumentException(&quot;删除元素失败，index非法&quot;); int e = array[index]; for(int i = index; i &lt; length - 1; i++)&#123; array[i] = array[i + 1]; &#125; length --; return e; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format(&quot;capacity: %d, length: %d\\n&quot;, array.length, length)); res.append(&quot;Array: [ &quot;); for (int i = 0; i &lt; length; i++) &#123; res.append(array[i]); if(i == length - 1) res.append(&apos;]&apos;); else res.append(&quot;, &quot;); &#125; return res.toString(); &#125; 5. 以后再补充别的扩展","tags":[{"name":"数组","slug":"数组","permalink":"http://sergiojune.com/tags/数组/"}]},{"title":"数据结构之单链表的基本方法实现","date":"2019-01-14T07:43:14.165Z","path":"2019/01/14/数据结构之单链表的基本方法实现/","text":"1.链表的定义与数组有很大的区别，链表不用去申请一段连续的空间，而且他的存储是不连续的，可以真正地做到动态长度。在链表中，不但需要记录该元素的值，还需要记录下一个元素的地址。 数组在使用时需要申请一段连续空间，当内存不足时就会申请失败，使用链表就可以避免这个问题，不需要进行申请一段连续的内存空间，可以进行动态申请，用到的时候才需要申请。 2.什么时候用链表当需要对数据进行大量的删除插入操作的时候，就可以选择使用链表，这两个操作对于链表来说他的时间复杂度可以说是 O(1)。严格意义来说还是 O(n)。因为进行插入数据的时候，还是需要通过遍历来找到插入的位置，但是这里不需要进行数据迁移，将数据插入即可。如下图所示： 可以看到，找到位置后，修改其下一个指向元素指向即可，但是需要注意的是，两个指向的改变顺序不能变 12n.next = node.next;node.next = n; 变了之后就会使新插入的元素指向自己，这就会产生引用混乱了。 删除元素也是同理。 3.加入虚拟头结点的作用在对链表进行插入和删除等操作时，都必须考虑头结点是否为空的情况，因为不为空才会有下一个节点的存在。 在加上了虚拟头结点，就可以节省这一步的操作，无论链表是否为空，都必定存在链表，只不过这个链表只有虚拟头结点的时候长度还是为 0， 这个时候就可以判断该节点的下一个节点是否为空即可。 注意：虚拟头结点不算入长度 4.需要的结构以及实现的基本方法结构: 需要一个内部类，里面存储元素的值以及下一个元素的引用 同时在初始化链表的时候需要声明长度为 0 ，并且实现虚拟头结点 方法： int getLength()：获取链表长度 boolean isEmpty()：判断链表是否为空 void add(int index, int data)：向链表的 index 位置加入元素 int get(int index)：获取 index 位置的元素 void set(int index, int data)：修改 index 位置的元素 boolean contains(int data)：判断该元素是否在链表中 int remove(int index)：移除链表中 index 位置的元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171import java.util.Random;// 单链表public class SinglyLinkedlist &#123; // 节点结构 class Node&#123; int data; Node next; // 记录下一节点 public Node(Node node, int data)&#123; this.next = node; this.data = data; &#125; public Node(int data)&#123; this(null, data); &#125; public String toString()&#123; return data + &quot;&quot;; &#125; &#125; int length; Node head; // 虚拟头结点 public SinglyLinkedlist()&#123; this.length = 0; // 初始化头结点 head = new Node(null, 0); &#125; // 获取长度 public int getLength()&#123; return length; &#125; // 判断链表是否为空 public boolean isEmpty()&#123; return length == 0; &#125; // 往链表头添加元素 public void addFirst(int data)&#123; // 因为第一个节点是不存储数据的，所以不需要判断第一个节点是否为空// Node node = new Node(data);// node.next = head.next;// head.next = node;// length ++; add(0, data); &#125; // 往链尾添加元素 public void addLast(int data)&#123;// Node node = head;// for (;node.next != null; node = node.next) &#123;//// &#125;// node.next = new Node(data);// length ++; add(length, data); // 直接调用自己的函数 &#125; // 往链表任意位置插入元素 public void add(int index, int data)&#123; if(index &lt; 0 || index &gt;length) throw new IllegalArgumentException(&quot;插入元素失败，index 非法&quot;); Node node = head; int i = 0; // 找到插入元素位置的前一个节点即可 while(i &lt; index)&#123; node = node.next; i ++; &#125; Node n = new Node(data); n.next = node.next; node.next = n; length ++; &#125; // 获取链表index的位置 public int get(int index)&#123; if(index &lt; 0 || index &gt;= length) throw new IllegalArgumentException(&quot;获取元素失败，index 非法&quot;); Node node = head; for(int i = 0; i &lt; index; i++)&#123; node = node.next; &#125; return node.next.data; &#125; // 获取首位元素 public int getFirst()&#123; return get(0); &#125; // 获取尾部元素 public int getLast()&#123; return get(length - 1); &#125; // 修改链表中的 index 位置的元素 public void set(int index, int data)&#123; if(index &lt; 0 || index &gt;= length) throw new IllegalArgumentException(&quot;插入元素失败，index 非法&quot;); Node node = head.next; int i = 0; while(i &lt; index)&#123; node = node.next; i++; &#125; node.data = data; &#125; // 查询链表是否包含这个元素 public boolean contains(int data)&#123; Node node = head.next; while(node != null)&#123; if(node.data == data) return true; node = node.next; &#125; return false; &#125; // 删除指定节点 public int remove(int index)&#123; if(index &lt; 0 || index &gt;= length) throw new IllegalArgumentException(&quot;删除元素失败，index 非法&quot;); Node node = head; for (int i = 0; i &lt; index; i++) &#123; node = node.next; &#125; Node n = node.next; // 准备被删除的元素 node.next = n.next; int data = n.data; n.next = null; // 释放 length--; // 必须记得这个 return data; &#125; // 删除头结点 public int removeFirst()&#123; return remove(0); &#125; // 删除尾节点 public int removeLast()&#123; return remove(length - 1); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); Node node = head.next; res.append(String.format(&quot;length: %d\\n&quot;, length)); res.append(&quot;LinkedList: &quot;); for(int i = 0; i &lt; length; i++)&#123; res.append(node.data); res.append(&quot; --&gt; &quot;); node = node.next; &#125; res.append(&quot;NULL&quot;); return res.toString(); &#125; 5.以后补充","tags":[{"name":"单链表","slug":"单链表","permalink":"http://sergiojune.com/tags/单链表/"}]},{"title":"抓包软件 Fiddler 了解一下？","date":"2019-01-14T07:43:14.141Z","path":"2019/01/14/抓包软件 Fiddler 了解一下？/","text":"学会如何抓包，是爬虫的必备技能，甚至可以说，不会抓包就等同于不会爬虫。 那我们怎样抓包呢？如果直接抓取浏览器上的内容，可以直接使用开发者工具进行抓包，但有个局限，只能抓浏览器的，功能也没有多少。还可以使用别的工具，比如 mitmproxy、charles，当然还有今天所说的 Fiddler 。今天要分享的就是如何使用fiddler进行抓包以及它的功能讲解。 1. 下载并安装fiddler 下载地址：https://www.telerik.com/download/fiddler 这里填写下你的下载目的和国家就可以下载了，安装的话这里就不多说了。 2. Fiddler工作原理以及优缺点 图片来自https://blog.csdn.net/DreamTL/article/details/70405368 如上图，Fiddler 作为一个代理，先是捕捉到客户端的 request 请求，然后再自己转发到服务器端，服务器接收到请求时，会返回一个响应 response ，Fiddler 还是会继续捕捉到服务器的响应请求，再来转发给客户端，简单来说， Fiddler 就是作为一个中间人。 优缺点： 只支持 http、https、ftp、webscoket 数据流等相关协议的捕捉，无法监测或修改其他数据流，如 SMTP、POP3 协议(邮箱相关的协议)，无法处理请求和响应超过 2GB 的数据，还有就是只支持 windows 平台，如果想要支持 mac 的话，建议下载 charles ，不过很吃性能，在我机子上运行下那风扇就想起来了。 3. Fiddler 功能详解 3.1 如何进行抓包 1）先勾选允许抓取 https 流量 这个就是允许抓取 https 的流量。如果第一次使用，勾选之后会提示你安装一个证书，这个证书就是用来做中间人进行抓包的， Fiddler使用此证书来解密所捕捉到的包，再加密转发到相对应的服务器端或者客户端。 如果你还没有安装就点击 Yes 安装即可，如果没有弹出此窗口的也可以点击右上角的 actio 按钮的第一个选项也是可以弹出此个窗口的 上面圈出来的是表示抓取哪一部分的流量。 from all processes：抓取所有进程的流量 from browsers only：只抓取浏览器的流量 from non-browsers only：不抓取浏览器的流量 from remote clients only：抓取远程的客户端，当需要抓取的是手机端的流量，就需要用到这个 2)接下来设置端口号 一般默认即可，这里我设置成了8889。 下面的那个 Allow remote computers to connect 是允许远程的客户端进行连接，如果抓取手机端的也需要勾选。 3)浏览器设置代理服务器 Fiddler 的设置完了，这时候还需要在你的浏览器上设置代理服务器才能进行抓取。 使用 Chrome 浏览器的可以直接使用 SwitchyOmega 插件进行修改即可，操作简单。 点击新增情景模式 选择代理服务器，随后填写任意名字，这里我填写的是 Fiddler,点击创建即可。 然后填写以上内容，代理服务器为本机，端口号为上面在 Fiddler 设置的端口号，填写完在左下角点应用选项才算设置完毕。 设置完之后再在浏览器插件处点击该插件，然后选择该模式即可开始抓包 设置完之后第一次打开可能是这样的 关闭 Fiddler 再重新打开就可以了 3.2 进行手机端抓包 手机端的抓包也是很容易的，先是设置好 Fiddler 的允许抓取远程客户端。 这时用手机连接wifi，然后长按修改网络(不同安卓手机不一样) 点击代理，然后点击手动 主机名就写你电脑上的 ip 地址，查看 ip 地址可以在控制台上输入 ipconfig 即可 由于我的电脑也是连接 wifi 的，所以 ip 地址对应的是 无线网络的那个，如果你的电脑是使用网线来进行上网的，那 ip 地址就是 以太网的那个。 填完之后还不能抓取，如果直接抓取会显示证书有问题。 我们也是需要安装证书才可以正常抓取的。那接下来安装证书 手机浏览器输入 你的ip地址:端口号 进入网址下载证书，如 192.168.1.2:8888， 端口号还是之前在 Fiddler设置的那个 点击上图箭头的网址进行下载即可，下载完毕之后点击安装即可。 如果你是 miui 系统的机子，就需要进入wifi 设置的界面进行安装 点击 高级设置 –&gt; 安装证书 即可，期间需要密码验证或者设置密码之类的设置即可。 还有一个大坑，就是如果你的机子 是安卓 9 而是 miui 系统(其他系统没测试过)的话，安装了证书也是没用的，在进行抓包的时候还是会提示证书有问题。安卓 8 版本的我没有测试过，不过安卓 7 版本以下的估计都可以。 弄完了以上的东西就可以抓包了，如果设置完了，网络没了，还是那样子，第一次设置完需要重启下 Fiddler 软件就可以抓包了。 3.3 抓包内容的介绍 很明显左边的就是捕捉的请求和响应，右边的就是对应请求的详细信息，比如请求头，表单信息，比如上图下面箭头所指的就是表单信息。，如果这些信息看到的内容很少的话，可以直接点击下面的 View in Notepad 按钮就可以在笔记本中显示出来，非常方便。 左边每列代表的含义为： 左边第一列中每个图片代表的含义为： 3.4 再说几个常用的功能 查找：抓包时，经常会抓到一堆不重要的包，而需要找的包夹杂在里面非常难找，所以就可以用关键字来查找，入口为： 也可以直接点击这个 或者直接按快捷键 Ctrl + F 即可 这里的功能很强大，可以只查找请求或者响应或者两个都查，还可以用正则表达式来查找，就不一 一说了。 映射：也就是重定向，将服务器端的响应内容可以更改为客户端上的文件，功能也是很强大，之前我在爬取网易云评论时也是弄过的，有兴趣的可以看看利用python爬取网易云音乐，并把数据存入mysql。 在这里填写对应的规则和文件即可 还有一个类似于 postman的功能，就是下面这个 就是在这里模拟请求，有什么需要模拟的话可以先在这里模拟一次，成功之后再用编程去敲出来也是不错的，非常强大。 由于篇幅问题，还有一些功能就暂时不介绍了，等以后用到的时候会说哈，比如断点调试之类的，到时记得时刻关注哦！","tags":[{"name":"抓包","slug":"抓包","permalink":"http://sergiojune.com/tags/抓包/"}]},{"title":"使用单链表进行冒泡排序","date":"2019-01-14T07:43:14.140Z","path":"2019/01/14/使用单链表进行冒泡排序/","text":"1.什么是冒泡排序下面这个是百度百科的解释： 冒泡排序（Bubble Sort），是一种计算机科学领域的较简单的排序算法。它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果他们的顺序（如从大到小、首字母从A到Z）错误就把他们交换过来。走访元素的工作是重复地进行直到没有相邻元素需要交换，也就是说该元素已经排序完成。这个算法的名字由来是因为越大的元素会经由交换慢慢“浮”到数列的顶端（升序或降序排列），就如同碳酸饮料中二氧化碳的气泡最终会上浮到顶端一样，故名“冒泡排序”。 按我的理解就是将相邻的两个元素进行比较，不符合要求的就进行交换位置，交换之后就继续进行下一个的元素的比较，比较一圈之后，最大或者最小的元素就会出现在末端。 由上图就可以看出，每每相邻就比较，就像一个个泡泡冒起来，我就是这样理解冒泡排序的。 2.链表的冒泡排序思路 1)先讨论下不加虚拟头的情况在不加虚拟头时候，我们就需要单独讨论当前节点是不是在链表头部，如果是头部的时候，就需要单独创建一个节点来暂时保存头节点，然后就进行交换 在交换之后，这时就可以创建两个引用，分别指向头节点和头节点的下一个节点。 然后通过使用该两个节点的下一个节点就行比较，上面的这两个引用只是为了方便比较和记录。 前面的两个节点在第一次进行比较时，符合位置要求，所以向下移一个位置，第二次比较时就需要进行交换位置了，如上图所示，步骤就不多说了，上面都有。 交换位置记得顺序不能改变，变了就会造成引用混乱。 在交换后这时两个节点也可以在上图看出，所以我们只需要移动当前节点即可(currentNode)，然后进行下一次比较。 2）接着说带虚拟头的如果带了虚拟头了，就比上面少了一步，因为不需要进行判断头节点了，这个时候头节点变成了虚拟的，不需要管的了，步骤就和上面的第二步一样。 比较完了，那什么时候停止呢?因为有时候无法轻易得出链表的长度，这时我引进了一个整型变量，负责记录一次冒泡排序后的交换次数，如果交换次数大于 0，说明链表还需要进行比较，反之就可以直接跳出循环了。 3.代码展示1234567891011121314151617181920212223242526272829303132333435363738394041// 链表的冒泡排序public Node sort(Node head)&#123; // 链表没有元素或者只有一个元素 if(head == null || head.next == null) return head; // 创建一个虚拟头结点，便于节点交换，这样子就不需要单独考虑头结点 Node dummyHead = new Node(head, -1); int num = 0; // 记录交换次数 while( true)&#123; // 用两个变量分别记录当前节点和下一节点 // 然后再用他们的下一节点进行比较 Node currentNode = dummyHead; Node nextNode = dummyHead.next; // 跳出条件为下一个节点不存在 while(nextNode.next != null)&#123; // 需要进行交换 if(currentNode.next.data &gt; nextNode.next.data)&#123; // 这个有点难说，需要自己画图 currentNode.next = nextNode.next; nextNode.next = currentNode.next.next; currentNode.next.next = nextNode; currentNode = currentNode.next; num ++; &#125; else&#123; currentNode = currentNode.next; nextNode = nextNode.next; &#125; &#125; // 判断是否进行交换 if(num == 0) // 等于 0 说明没有进行交换 break; else num = 0; // 归 0 下次再进行交换 &#125; return dummyHead.next;&#125; 4. 复杂度分析 时间复杂度 O(n^2)：假设最坏情况，比较到最后两个元素还需要进行交换位置，这时外圈循环就是 n - 1 次了，内层永远都是比较到最后一个元素，就是 n。所以时间复杂度为 O(n^2)。就是 n 的平方 空间复杂度 O(1)。","tags":[{"name":"单链表 冒泡排序","slug":"单链表-冒泡排序","permalink":"http://sergiojune.com/tags/单链表-冒泡排序/"}]},{"title":"两栈共享空间的实现","date":"2019-01-14T07:43:14.137Z","path":"2019/01/14/两栈共享空间的实现/","text":"1. 两栈共享存储空间定义使用两个栈来对相同类型的数据来进行存储，就是有个顶部和尾部。 若只是使用一个栈来使用的话，事先确定栈的容量是非常重要的，因为当容量小了，就需要不断扩大容量，或者容量大了，就会浪费一段存储空间，这都是很麻烦的。但是使用两个栈就可以做到最大限度地利用事先开辟的存储空间。 当两个栈需要操作相反的关系的时候就可以考虑这个，就比如总有人赚钱和赔钱，那就一边负责增加元素，另一边负责删除元素。 实现两栈可以理解为一个长度为 n 的线性表从任意位置切开成两段空间，这样子就会有数组的首尾端都是两个栈的栈底。 左边的栈为空的时候，就是栈顶为 -1(栈顶的元素位置)，右边的栈为空的时候就是栈顶元素为 n 的时候。 所以当左边栈满的时候，就是栈顶为 n-1，而右边栈为空就是栈顶为 n，而右边栈满的时候就是栈顶为 0，左边栈为空就是栈顶为 -1。所以当两个碰面的时候，也就是两个栈满的时候，就是两个栈顶的位置相差一的时候，即 leftTop + 1 = rightTop。 理解上面这个就好实现了，栈需要记录左右两边的栈顶的位置，所以栈的结构需要改变，需要有栈初始化两个栈的容量的总量，还必须声明左右两边栈顶的位置。 两个方法实现 push()：这个需要加入一个栈号参数来判断需要插入那个栈，同时插入时需要判断栈是否满 pop()：也需要加一个栈号参数来判断删除哪边栈的元素，删除时需要判断栈是否为空。 2.代码部分12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// 两栈共享空间public class DoubleStack &#123; int[] arr; // 左右分别记录两个栈的的下一个元素需要插入的位置 int left; int right; int length; public DoubleStack(int capacity)&#123; arr = new int[capacity]; left = 0; right = capacity - 1; length = 0; &#125; public DoubleStack()&#123; this(10); &#125; public boolean isEmpty()&#123; return length == 0; &#125; public int length()&#123; return length; &#125; public void push(int e, int stackSide)&#123; if(left == right + 1) throw new IllegalArgumentException(&quot;栈已满，入栈失败&quot;); // 1 代表左边的栈， 2 代表右边的栈 if(stackSide == 1)&#123; arr[left] = e; left ++; &#125; else if (stackSide == 2)&#123; arr[right] = e; right --; &#125; else throw new IllegalArgumentException(&quot;stackSide 错误&quot;); length ++; &#125; public int pop(int stackSide)&#123; if(stackSide == 1)&#123; if(left == 0) throw new IllegalArgumentException(&quot;出栈失败，左栈为空&quot;); int e = arr[left]; length --; left --; return e; &#125; else if(stackSide == 2)&#123; if(right == arr.length - 1) throw new IllegalArgumentException(&quot;出栈失败，右栈为空&quot;); int e = arr[right]; length --; right ++; return e; &#125; else throw new IllegalArgumentException(&quot;出栈失败，栈不存在&quot;); &#125; public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format(&quot;capacith: %d, length: %d\\n&quot;, arr.length, length())); res.append(&quot;DoubleStack: left tail [&quot;); for(int i = 0; i &lt; left; i++)&#123; res.append(arr[i]); if(i != left - 1) res.append(&quot;, &quot;); &#125; res.append(&quot;] top &quot;); res.append(&quot;right top [&quot;); for(int i = right + 1; i &lt; arr.length; i++)&#123; res.append(arr[i]); if(i != arr.length - 1) res.append(&quot;, &quot;); &#125; res.append(&quot;] tail&quot;); return res.toString(); &#125;","tags":[{"name":"栈","slug":"栈","permalink":"http://sergiojune.com/tags/栈/"}]},{"title":"hexo搭建的博客提交文章本地预览成功但github没有显示解决方法","date":"2019-01-14T07:43:14.135Z","path":"2019/01/14/hexo搭建的博客提交文章本地预览成功但github没有显示解决方法/","text":"记录自己博客的一个坑！前几天我写了文章，想发布在博客上，谁知道在提交上去，只可以在本地预览，却不可以在github上看到，而且github上也没有提交文章的记录。这个坑我百度了很久也没有找到，直到问到人，说需要升级git，然后升级了。果然，成功提交了！！！升级git是直接在重新下载个git把原来的覆盖掉就可以了。升级完成之后记得再提交一次，比如：hexo cleanhexo ghexo d这样就可以了，如果还是不可以，可以选择清除下cookie及历史记录。我完成了上面的之后，文章算是提交上去了，但是，我的主题没有了，很悲催，没想到一坑还比一坑高，所以还是接着百度，看到了可能是我的github的仓库名字和我的hexo博客里的配置_config.yml​文件中的url名字不一样导致的试了一下，果真是可以了。这个坑是前面那个坑乱改东西留下的，不过解决了，就好！！生活不止眼前的bug，还有乐趣和斗志！","tags":[{"name":"坑","slug":"坑","permalink":"http://sergiojune.com/tags/坑/"}]},{"title":"c语言提高第一天总结","date":"2019-01-14T07:43:14.131Z","path":"2019/01/14/c语言提高第一天总结/","text":"c语言中的数组作为形参时编译器都会把它当成指针处理，所以在对数组进行操作的时候，通常还需要再加个一个形参，那就是数组长度，因为数组的结束没有标志，需要程序员自己进行判断。 2.引入数据类型是为了更容易地表达现实生活中的人和事，数据类型本质就是固定大小的一段内存空间，常见的数据类型的所占字节大小如下： int: 4 字节 char: 1 字节 float: 4 字节 double: 8字节 long: 4 字节 指针: 4 字节 当有一个数组 b，打印 b+1 和 &amp;b+1 是不同的，这是因为他们的数据类型不一样，前者代表的是这个数组的首个元素的地址，后者表达的是数组的最后一个元素的地址，所以 &amp;b 表达的是整块数组的地址 1234567891011121314# include &lt;stdio.h&gt;# include &lt;stdlib.h&gt;int main(void)&#123; int b[] = &#123;1,2,5,3,8,9,4&#125;; printf(&quot;b = %d, b + 1 = %d, &amp;b = %d, &amp; b + 1 = %d\\n&quot;, b, b+1, &amp;b, &amp;b+1); system(&quot;pause&quot;); return 0;&#125;# 结果是：b = 7338172, b + 1 = 7338176, &amp;b = 7338172, &amp; b + 1 = 7338200 return 不是把内存块的字节 return 出来，而是把内存快的首地址返回。 判断栈是开口向上还是向下，利用栈后进先出的特性可以通过先后定义两个变量继而打印他们的地址来知道开口的方向，先存的变量的地址小就说明开口是向上的，反之就是向下的，有一个需要注意的 是，存放数组时，他的每一个元素的地址都是递增，也就是说都是开口向上的，这个说明不了栈的开口！123456789101112131415# include&lt;stdio.h&gt;# include&lt;stdlib.h&gt;int main(void)&#123; // 两个变量先后被压进栈，通过判断地址可以判断栈的开口 int a; int b; printf(&quot;&amp;a=%d, &amp;b=%d\\n&quot;, &amp;a, &amp;b); // 通过debug方式编译是开口向下的，通过release编译则是向下的 system(&quot;pause&quot;); return 0;&#125;结果：&amp;a=13630336, &amp;b=13630340 // 可以看到开口是向上的 函数调用模型： 一般都是先将该函数的地址压入栈，然后就是将其参数压入栈，接着才是将其代码压入栈，如果该函数调用了别的函数，执行到了，就会将该函数的运行状态压入栈，然后再将被调用的函数地址压入栈，接着重复上述过程。 c语言中，内存区一般分为四个区，分别是： 栈：存放的是局部变量，程序运行结束之后就会被操作系统自动释放 堆： 存放的是动态创建的变量，不能自动释放，需要程序员手动释放 全局区： 又名静态区，存放的是全局变量以及静态变量，还存放一些常量，比如字符串常量和其他常量，该区域还是等待程序结束后操作系统释放内存，但和栈不同的是，他里的所被分配的内存可以被之前的函数给调用。 代码区： 存放我们所写的代码。 第一天就这么多啦，每天进步一点点就足够！今天还学到一个鼓励自己的方法： 每当自己学不进去的时候，就去知乎看看，你会发现，原来你自己什么都不会！毕竟知乎里的都是大牛。然后你就会马上滚回来学习了","tags":[{"name":"c","slug":"c","permalink":"http://sergiojune.com/tags/c/"}]},{"title":"谈谈今晚以及在跑步时的一些随想","date":"2018-05-24T14:11:38.000Z","path":"2018/05/24/谈谈今晚以及在跑步时的一些随想/","text":"今天可以算是我的第一次在讲台上分享我的技术，准备了两天左右吧，其实也没那么长，就是做了个ppt而已，由于第一次做，有很多地方都没有做好，比如字体太小了，背景和字体颜色混合了导致看不清字等等此类的问题。如果下次再做的话一定要做得好一点。还有就是我的分享有点垃圾哈，讲的内容自我感觉不是很好，也不知道需要讲什么地方，以后还是多观察下别人怎么分享吧。这次需要表扬自己的是上讲台讲台没有预料中的那么紧张，虽然讲话不是很自如，不过也比以前好多了，希望下次能更进一步吧！加油！ 说说跑步的感想！这两星期我经常跑步，跑完步还会做50个俯卧撑，虽然不太标准，不过也好过没有做吧。在跑步时会跑多几圈就会很累，就想放弃，不过我最后还是咬牙就挺着过去了。这两天跑步呢，我就想，追逐梦想就和跑步一样吧。就比如我把梦想都分成很多个小目标，跑步的圈数或者步数就是这些小小的目标。在刚开始的时候，你跑步会很顺利，小目标也是这样子，刚开始的小目标也是很容易就可以实现的。可是当你跑步的步数多了，也就是小目标实现多了，之后的每一步或每一个小目标都会显得举步维艰，这时候就需要你自己的意志力去支持着自己继续跑下去。所以说梦想也需要用自己的一生去实现，后面可能会越来越难，可你每实现的一个小目标都会把很多人甩在身后面，这些人就相当于你在跑步过程中的路人。 人生是一场永无止尽的跑步！还有这几天会经常不知道干嘛，然而还有很多事情没做，先把这些事列下来吧！线性代数课程补上 离散数学课程及作业补上 高等数学课程补上 scrapy的学习 马蜂窝网站的全站抓取 淘宝商品抓取 亚马逊模拟登陆 爬虫面试题 看 图解http 看 网络是怎样连接 了解下各种排序 不列出来都不知道有这么多事情没做，以后觉得没事的时候可以找这些事情做了。 写完，睡觉！晚安！","tags":[]},{"title":"python爬虫常用库之urllib详解","date":"2018-03-11T16:00:19.000Z","path":"2018/03/12/python爬虫常用库之urllib详解/","text":"以下为个人在学习过程中做的笔记总结之爬虫常用库urlliburlib库为python3的HTTP内置请求库urilib的四个模块：urllib.request:用于获取网页的响应内容urllib.error:异常处理模块，用于处理异常的模块urllib.parse:用于解析urlurllib.robotparse:用于解析robots.txt，主要用于看哪些网站不能进行爬取，不过少用1urllib.requesturllib.request.urlopen(url,data=None,[timeout,],cafile=None,cadefault=False,context=None)url:为请求网址data:请求时需要发送的参数timeout:超时设置，在该时间范围内返回请求内容就不会报错示例代码： 1from urllib import request 2 3# 请求获取网页返回内容 4response = request.urlopen(‘https://movie.douban.com/&#39;) 5# 获取网页返回内容 6print(response.read().decode(‘utf-8’)) 7# 获取状态码 8print(response.status) 9# 获取请求头10print(response.getheaders())1# 对请求头进行遍历2for k, v in response.getheaders():3 print(k, ‘=’, v)可以使用上面的代码对一些网站进行请求了，但是当需要一些反爬网站时，这就不行了，这时我们需要适当地增加请求头进行请求，这时就需要使用复杂一点的代码了，这时我们需要用到Request对象代码示例：1# 请求头2headers = {‘User-Agent’:‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0’}3requests = request.Request(‘https://movie.douban.com/&#39;, headers=headers) # 加入自己的请求头更加接近浏览器4# 进行请求,把Request对象传入urlopen参数中5response = request.urlopen(requests)6print(response.read().decode(‘utf-8’))这个我添加了请求头进行请求，使我发送的请求更加接近浏览器的行为。可以对应一些反爬网站了如果网站需要进行登陆，这时需要用到post方法，用上面的也是可以的。代码如下： 1from urllib import request, parse 2# 使用post方法来进行模拟登陆豆瓣 3data = {‘source’: ‘None’, 4 ‘redir’: ‘https://www.douban.com/&#39;, 5 ‘form_email’: ‘user’, 6 ‘form_password’: ‘passwd’, 7 ‘remember’: ‘on’, 8 ‘login’: ‘登录’} 9# 将data的字典类型转换为get请求方式10data = bytes(parse.urlencode(data), encoding=‘utf-8’)11requests = request.Request(‘https://accounts.douban.com/login&#39;, headers=headers, data=data, method=‘POST’)12response = request.urlopen(requests)13print(response.read().decode(‘utf-8’))这里我用到了data的参数把登陆需要的参数传进去，还加了个请求方法Methodparse.urlencode()后面有讲这里还有另外一种添加请求头的方法Request.add_header(): 参数有两个，分别为请求头对应的键和值，这种方法一次只能添加一个请求头，添加多个需要用到循环或者直接用前面的方法添加多个请求头在登陆了网站之后，我们需要用到cookie来保存登陆信息，这时就需要获取cookie了。urllib获取cookie比较麻烦。代码示例如下： 1from http import cookiejar 2# 获取cookie 3cookie = cookiejar.CookieJar() 4# 获取助手把cookie传进去 5handler = request.HTTPCookieProcessor(cookie) 6# 获取opener进行请求网站 7opener = request.build_opener(handler) 8# 请求网页 9response = opener.open(‘https://movie.douban.com/&#39;)10# 打印cookie11for c in cookie:12 print(c.name, ‘=’, c.value)单纯地打印没什么用，我们需要把他存入文件来保存，下次使用时再次加载cookie来登陆保存cookie为文件：1from http import cookiejar2# 将cookie保存在文件中3filename = ‘cookie.txt’4cookie = cookiejar.MozillaCookieJar(filename) # 表示使用Mozilla的cookie方式存储和读取5handler = request.HTTPCookieProcessor(cookie)6opener = request.build_opener(handler)7opener.open(‘https://movie.douban.com/&#39;)8# 保存文件9cookie.save(ignore_discard=True, ignore_expires=True)另一种保存方法：1from http import cookiejar2cookie = cookiejar.LWPCookieJar(filename) # 表示 Set-Cookie3 文件格式存储和读取3handler = request.HTTPCookieProcessor(cookie)4opener = request.build_opener(handler)5opener.open(‘https://movie.douban.com/&#39;)6# 保存文件7cookie.save(ignore_discard=True, ignore_expires=True)这两种保存格式都是不一样的，需要保存的内容一样。保存可以了，这时就需要用到加载了，当然也可以。代码如下：1from http import cookiejar2# 从cookie文件加载到网页上实现记住登陆3cookie = cookiejar.LWPCookieJar()4# 加载文件5cookie.load(filename, ignore_discard=True, ignore_expires=True)6handler = request.HTTPCookieProcessor(cookie)7opener = request.build_opener(handler)8opener.open(‘https://movie.douban.com/&#39;)这样就可以实现不用密码进行登陆了。cookie小总结：在操作cookie时，都是分五步，如下：进行导包，至关重要的一步，不导包直接出错。获取cookie处理对象，使用cookiejar包创建cookie处理器，使用request.HTTPCookieJarProcessor()利用cookie处理器构建opener，使用request.build_opener()进行请求网站，用opener.open(),这个不能用request.urlopen()如果有时你在同一ip连续多次发送请求，会有被封ip的可能，这时我们还需要用到代理ip进行爬取，代码如下：1proxy = request.ProxyHandler({2 ‘https’: ‘https://106.60.34.111:80&#39;3})4opener = request.build_opener(proxy)5opener.open(‘https://movie.douban.com/&#39;, timeout=1)可以看到越复杂的请求都需要用到request.build_opener(),这个方法有点重要，请记住哈2urllib.error将上面的使用代理ip的请求进行异常处理，如下： 1from urllib import request, error 2try: 3 proxy = request.ProxyHandler({ 4 ‘https’: ‘https://106.60.34.111:80&#39; 5 }) 6 opener = request.build_opener(proxy) 7 opener.open(‘https://movie.douban.com/&#39;, timeout=1) 8except error.HTTPError as e: 9 print(e.reason(), e.code(), e.headers())10except error.URLError as e:11 print(e.reason)因为有时这个ip或许也被封了，有可能会抛出异常，所以我们为了让程序运行下去进而进行捕捉程序error.URLError: 这个是url的一些问题，这个异常只有一个reason属性error.HTTPError:这个是error.URLError的子类，所以在与上面的混合使用时需要将这个异常放到前面，这个异常是一些请求错误，有三个方法，.reason(), .code(), .headers(),所以在捕捉异常时通常先使用这个3urllib.parse解析url:urllib.parse.urlparse(url, scheme=’’, allow_fragments=True)简单的使用：1from urllib import request, parse2# 解析url3print(parse.urlparse(‘https://movie.douban.com/&#39;))4print(parse.urlparse(‘https://movie.douban.com/&#39;, scheme=‘http’))5print(parse.urlparse(‘movie.douban.com/‘, scheme=‘http’))6# 下面是结果7ParseResult(scheme=‘https’, netloc=‘movie.douban.com’, path=‘/‘, params=‘’, query=‘’, fragment=‘’)8ParseResult(scheme=‘https’, netloc=‘movie.douban.com’, path=‘/‘, params=‘’, query=‘’, fragment=‘’)9ParseResult(scheme=‘http’, netloc=‘’, path=‘movie.douban.com/‘, params=‘’, query=‘’, fragment=‘’)可以看出加了scheme参数和没加的返回结果是有区别的。而当scheme协议加了，而前面的url也包含协议，一般会忽略后面的scheme参数既然后解析url，那当然也有反解析url，就是把元素串连成一个url1from urllib import parse2# 将列表元素拼接成url3url = [‘http’, ‘www’, ‘baidu’, ‘com’, ‘dfdf’, ‘eddffa’] # 这里至少需要6个元素（我乱写的，请忽视）4print(parse.urlunparse(url))5# 下面是结果6http://www/baidu;com?dfdf#eddffaurlparse()接收一个列表的参数，而且列表的长度是有要求的，是必须六个参数以上，要不会抛出异常1Traceback (most recent call last):2 File “E:/anaconda/python_project/python3_spider/urllib_test.py”, line 107, in &lt;module&gt;3 print(parse.urlunparse(url))4 File “E:\\anaconda\\lib\\urllib\\parse.py”, line 454, in urlunparse5 _coerce_args(components))6ValueError: not enough values to unpack (expected 7, got 6)urllib.parse.urljoin():这个是将第二个参数的url缺少的部分用第一个参数的url补齐1# 连接两个参数的url, 将第二个参数中缺的部分用第一个参数的补齐2print(parse.urljoin(‘https://movie.douban.com/&#39;, ‘index’))3print(parse.urljoin(‘https://movie.douban.com/&#39;, ‘https://accounts.douban.com/login&#39;))4# 下面是结果5https://movie.douban.com/index6https://accounts.douban.com/loginurllib.parse.urlencode():这个方法是将字典类型的参数转为请求为get方式的字符串1data = {‘name’: ‘sergiojuue’, ‘sex’: ‘boy’}2data = parse.urlencode(data)3print(‘https://accounts.douban.com/login&#39;+data)4# 下面是结果5https://accounts.douban.com/loginname=sergiojuue&amp;sex=boy4结语还有个urllib.robotparse库少用，就不说了，留给以后需要再去查文档吧。上面的只是我在学习过程中的总结，如果有什么错误的话，欢迎在留言区指出，还有就是需要查看更多用法的请查看文档https://docs.python.org/3/library/urllib.html需要代码的可以去我的github上面fork，给个star也行！github:https://github.com/SergioJune/gongzhonghao_code/blob/master/python3_spider/urllib_test.py 学习过程中看的大多是崔庆才大佬的视屏：https://edu.hellobi.com/course/157日常学python一个专注于python的公众号","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://sergiojune.com/tags/爬虫/"},{"name":"urllib","slug":"urllib","permalink":"http://sergiojune.com/tags/urllib/"}]},{"title":"2017年终总结","date":"2017-12-31T14:11:38.000Z","path":"2017/12/31/2017年终总结/","text":"今天是2017的最后一天了，对于2017，是我至今为止最重要的一年，在这一年中经历了人生的第一场高考，很高兴，这也是最后一场。而在下半年，我也进入了大学，虽然大学不是理想中的大学，但是我也很高兴，因为终于可以不再像高中那样了，在这里我可以学习我想要学的东西！ 上半年那时我还在准备高考，每天沉迷于题海之中，那段在一中的时间，是我高中过得最充实的时间，每天早上六点起床，晚上奋斗到差不多一点才睡。但是高三一年的勤奋拉近不了我在高中挥霍了两年的时间所带来的与别人的距离，所以在每次模拟试下来之后都在想，为啥前两年不学多一点呢？这样子在高三就不会这么辛苦，也就不会有这么大的坑让给高三的一年来填。但是我也不会后悔，因为在那两年里，我学会了踢球，懂得了人生有太多的意外，比如我在高二那年的足球班赛，我也认识了一大堆志同道合的人。所以说，对自己走过的路不曾后悔。在这半年，我度过了第一个百日誓师，还记得那天康亚华校长的谈话，还记得我们跑步的激情，还记得那天之后对未来的憧憬。。。 百日誓师，壮观！！！ 下面是我们班在百日誓师的照片 最后放张我们学校的照片吧，离开后才发现学校是真的美！ 最后六月份，走上了考场，离开了学校，最终的结果也对得起我在高三奋斗的一年！ 下半年下半年我走进了大学，大学虽然没有想象的大和好，但这始终会是我在余下四年继续坚持我奋斗的地方，所以我也爱上了这个地方。 看！ 我的大学还是很美的。还有我的班，每个都很帅很漂亮，有木有！！！在这大学半年里，我学了很多编程知识，比如c语言，python，html+css+javascript和java，学了这么多，都是因为去尝试吧，因为我暂时不知道我要往哪个方向发展，现在我知道了，我准备往爬虫，分析大数据方向，或许还有人工智能，还有就是以python为主的前端在学了这么多，自己模仿了两个网页，也做了一个简单的查看新闻app，总之还没有深入，准备下一年深入学习。不过在学习学校的课程是失败的，很多课都没有认真学习，英语也没有学好，觉得蛮遗憾的，浪费了这么多时间。还有，我也根据了网上的教程自己搭建了一个属于自己的博客，也挺不错的。而在自己博客定的小目标也没有完成多少，可能要完成的东西太多了，所以也没有完成，下一次就知道应该定多少了，嘻嘻！我在最后的一个月还参加了一个打卡活动，里面的早起和坚持锻炼我基本上算是坚持下来了，算是比较欣慰了。 引用知识星球人里一段话 人生那么长，做着枯燥而无意义的工作，每时每刻都是煎熬。 人生那么短，如果每一步成长都是令人激动的，那你一定不会说:”后悔”。 尝试，不断试错，不断碰壁，不断重来，你会发现人生有那么多的可能。会发现世界上有那么美丽的事情让我们满心欢喜。 献给现在迷茫，未来成功的你。 不忘初心，方得始终。2018，我来了！！！","tags":[{"name":"hard","slug":"hard","permalink":"http://sergiojune.com/tags/hard/"}]}]